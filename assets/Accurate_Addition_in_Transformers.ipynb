{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxBe5uWwKtXo"
      },
      "source": [
        "# Accurate Integer Addition in Transformers\n",
        "\n",
        "This CoLab defines, trains and analyses a Transformer model that performs integer addition e.g. 33357+82243=115600. Each digit is a separate token. For 5 digit addition, the model is given 12 \"question\" (input) tokens, and must then predict the corresponding 6 \"answer\" (output) tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCeIsnGSJaze"
      },
      "source": [
        "![ProblemShape.svg](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg 
   width="540.95996pt"
   height="48pt"
   viewBox="0 0 540.95996 48"
   version="1.1"
   id="svg250"
   xmlns:xlink="http://www.w3.org/1999/xlink"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg" content="%3Cmxfile%20host%3D%22app.diagrams.net%22%20modified%3D%222023-10-02T20%3A49%3A06.431Z%22%20agent%3D%22Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20Win64%3B%20x64)%20AppleWebKit%2F537.36%20(KHTML%2C%20like%20Gecko)%20Chrome%2F117.0.0.0%20Safari%2F537.36%20Edg%2F117.0.2045.47%22%20version%3D%2221.8.2%22%20etag%3D%22Sr2zJPmldHJdmaQhiDSy%22%20type%3D%22google%22%3E%20%20%20%3Cdiagram%20name%3D%22Page-1%22%20id%3D%22UnXlO1nxWK-ck5HUXBPX%22%3E%20%20%20%20%20%3CmxGraphModel%20dx%3D%221386%22%20dy%3D%22831%22%20grid%3D%221%22%20gridSize%3D%2210%22%20guides%3D%221%22%20tooltips%3D%221%22%20connect%3D%221%22%20arrows%3D%221%22%20fold%3D%221%22%20page%3D%221%22%20pageScale%3D%221%22%20pageWidth%3D%22850%22%20pageHeight%3D%221100%22%20math%3D%220%22%20shadow%3D%220%22%3E%20%20%20%20%20%20%20%3Croot%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%220%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%221%22%20parent%3D%220%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%222%22%20value%3D%223%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%2240%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%223%22%20value%3D%223%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22120%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%224%22%20value%3D%223%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontColor%3D%23000000%3BfontStyle%3D1%3BfontSize%3D14%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%2280%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%225%22%20value%3D%227%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22200%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%226%22%20value%3D%225%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22160%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%227%22%20value%3D%22%2B%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22240%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%228%22%20value%3D%228%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22280%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%229%22%20value%3D%222%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22360%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2210%22%20value%3D%222%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontColor%3D%23000000%3BfontStyle%3D1%3BfontSize%3D14%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22320%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2211%22%20value%3D%223%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22440%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2212%22%20value%3D%224%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22400%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2213%22%20value%3D%22%3D%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22480%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2214%22%20value%3D%221%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22560%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2215%22%20value%3D%221%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22520%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2216%22%20value%3D%226%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22640%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2217%22%20value%3D%225%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontColor%3D%23000000%3BfontStyle%3D1%3BfontSize%3D14%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22600%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2218%22%20value%3D%220%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22680%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2219%22%20value%3D%220%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D14%3BfontStyle%3D1%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22720%22%20y%3D%2250%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2220%22%20value%3D%22D4%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%2240%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2221%22%20value%3D%22D2%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22120%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2222%22%20value%3D%22D3%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontColor%3D%23000000%3BfontStyle%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%2280%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2223%22%20value%3D%22D0%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22200%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2224%22%20value%3D%22D1%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22160%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2225%22%20value%3D%22D4%26%2339%3B%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22280%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2226%22%20value%3D%22D2%26%2339%3B%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22360%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2227%22%20value%3D%22D3%26%2339%3B%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontColor%3D%23000000%3BfontStyle%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22320%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2228%22%20value%3D%22D0%26%2339%3B%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22440%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2229%22%20value%3D%22D1%26%2339%3B%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22400%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2230%22%20value%3D%22A4%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22560%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2231%22%20value%3D%22A5%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22520%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2232%22%20value%3D%22A2%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22640%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2233%22%20value%3D%22A3%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontColor%3D%23000000%3BfontStyle%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22600%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2234%22%20value%3D%22A1%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22680%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%20%20%3CmxCell%20id%3D%2235%22%20value%3D%22A0%22%20style%3D%22text%3BstrokeColor%3Dnone%3Balign%3Dcenter%3BfillColor%3Dnone%3Bhtml%3D1%3BverticalAlign%3Dmiddle%3BwhiteSpace%3Dwrap%3Brounded%3D0%3BfontSize%3D12%3B%22%20vertex%3D%221%22%20parent%3D%221%22%3E%20%20%20%20%20%20%20%20%20%20%20%3CmxGeometry%20x%3D%22720%22%20y%3D%2281%22%20width%3D%2240%22%20height%3D%2230%22%20as%3D%22geometry%22%20%2F%3E%20%20%20%20%20%20%20%20%20%3C%2FmxCell%3E%20%20%20%20%20%20%20%3C%2Froot%3E%20%20%20%20%20%3C%2FmxGraphModel%3E%20%20%20%3C%2Fdiagram%3E%20%3C%2Fmxfile%3E%20">
  <defs
     id="defs70">
    <g
       id="g68">
      <g
         id="glyph-0-0">
        <path
           d="M 1.3125 0 L 1.3125 -6.5625 L 6.5625 -6.5625 L 6.5625 0 Z M 1.484375 -0.171875 L 6.40625 -0.171875 L 6.40625 -6.40625 L 1.484375 -6.40625 Z M 1.484375 -0.171875 "
           id="path2" />
      </g>
      <g
         id="glyph-0-1">
        <path
           d="M 0.390625 -2 L 1.796875 -2.15625 C 1.835938 -1.800781 1.953125 -1.53125 2.140625 -1.34375 C 2.335938 -1.15625 2.578125 -1.0625 2.859375 -1.0625 C 3.148438 -1.0625 3.394531 -1.175781 3.59375 -1.40625 C 3.800781 -1.632813 3.90625 -1.9375 3.90625 -2.3125 C 3.90625 -2.675781 3.804688 -2.960938 3.609375 -3.171875 C 3.421875 -3.390625 3.1875 -3.5 2.90625 -3.5 C 2.71875 -3.5 2.5 -3.460938 2.25 -3.390625 L 2.40625 -4.5625 C 2.789063 -4.550781 3.085938 -4.632813 3.296875 -4.8125 C 3.503906 -5 3.609375 -5.238281 3.609375 -5.53125 C 3.609375 -5.78125 3.53125 -5.976563 3.375 -6.125 C 3.226563 -6.28125 3.03125 -6.359375 2.78125 -6.359375 C 2.53125 -6.359375 2.316406 -6.269531 2.140625 -6.09375 C 1.972656 -5.925781 1.867188 -5.679688 1.828125 -5.359375 L 0.5 -5.578125 C 0.59375 -6.035156 0.734375 -6.398438 0.921875 -6.671875 C 1.109375 -6.941406 1.367188 -7.15625 1.703125 -7.3125 C 2.035156 -7.46875 2.40625 -7.546875 2.8125 -7.546875 C 3.519531 -7.546875 4.085938 -7.320313 4.515625 -6.875 C 4.867188 -6.5 5.046875 -6.082031 5.046875 -5.625 C 5.046875 -4.957031 4.6875 -4.429688 3.96875 -4.046875 C 4.394531 -3.953125 4.738281 -3.742188 5 -3.421875 C 5.257813 -3.097656 5.390625 -2.707031 5.390625 -2.25 C 5.390625 -1.59375 5.144531 -1.03125 4.65625 -0.5625 C 4.175781 -0.101563 3.578125 0.125 2.859375 0.125 C 2.179688 0.125 1.617188 -0.0664063 1.171875 -0.453125 C 0.722656 -0.847656 0.460938 -1.363281 0.390625 -2 Z M 0.390625 -2 "
           id="path5" />
      </g>
      <g
         id="glyph-0-2">
        <path
           d="M 0.453125 -6.078125 L 0.453125 -7.40625 L 5.375 -7.40625 L 5.375 -6.375 C 4.96875 -5.96875 4.550781 -5.390625 4.125 -4.640625 C 3.707031 -3.890625 3.390625 -3.09375 3.171875 -2.25 C 2.953125 -1.414063 2.84375 -0.664063 2.84375 0 L 1.453125 0 C 1.472656 -1.039063 1.6875 -2.097656 2.09375 -3.171875 C 2.5 -4.253906 3.039063 -5.222656 3.71875 -6.078125 Z M 0.453125 -6.078125 "
           id="path8" />
      </g>
      <g
         id="glyph-0-3">
        <path
           d="M 0.46875 -1.9375 L 1.90625 -2.078125 C 1.945313 -1.753906 2.066406 -1.5 2.265625 -1.3125 C 2.460938 -1.125 2.695313 -1.03125 2.96875 -1.03125 C 3.269531 -1.03125 3.523438 -1.148438 3.734375 -1.390625 C 3.941406 -1.640625 4.046875 -2.007813 4.046875 -2.5 C 4.046875 -2.957031 3.941406 -3.300781 3.734375 -3.53125 C 3.523438 -3.757813 3.253906 -3.875 2.921875 -3.875 C 2.503906 -3.875 2.132813 -3.691406 1.8125 -3.328125 L 0.640625 -3.5 L 1.375 -7.40625 L 5.1875 -7.40625 L 5.1875 -6.0625 L 2.46875 -6.0625 L 2.25 -4.78125 C 2.570313 -4.945313 2.898438 -5.03125 3.234375 -5.03125 C 3.867188 -5.03125 4.40625 -4.796875 4.84375 -4.328125 C 5.289063 -3.867188 5.515625 -3.265625 5.515625 -2.515625 C 5.515625 -1.898438 5.335938 -1.347656 4.984375 -0.859375 C 4.492188 -0.203125 3.816406 0.125 2.953125 0.125 C 2.253906 0.125 1.6875 -0.0546875 1.25 -0.421875 C 0.8125 -0.796875 0.550781 -1.300781 0.46875 -1.9375 Z M 0.46875 -1.9375 "
           id="path11" />
      </g>
      <g
         id="glyph-0-4">
        <path
           d="M 2.40625 -1.078125 L 2.40625 -3.03125 L 0.4375 -3.03125 L 0.4375 -4.390625 L 2.40625 -4.390625 L 2.40625 -6.34375 L 3.71875 -6.34375 L 3.71875 -4.390625 L 5.6875 -4.390625 L 5.6875 -3.03125 L 3.71875 -3.03125 L 3.71875 -1.078125 Z M 2.40625 -1.078125 "
           id="path14" />
      </g>
      <g
         id="glyph-0-5">
        <path
           d="M 1.6875 -4.0625 C 1.3125 -4.21875 1.035156 -4.429688 0.859375 -4.703125 C 0.691406 -4.972656 0.609375 -5.273438 0.609375 -5.609375 C 0.609375 -6.171875 0.804688 -6.632813 1.203125 -7 C 1.597656 -7.363281 2.15625 -7.546875 2.875 -7.546875 C 3.582031 -7.546875 4.132813 -7.363281 4.53125 -7 C 4.925781 -6.632813 5.125 -6.171875 5.125 -5.609375 C 5.125 -5.265625 5.03125 -4.953125 4.84375 -4.671875 C 4.664063 -4.398438 4.414063 -4.195313 4.09375 -4.0625 C 4.507813 -3.882813 4.828125 -3.632813 5.046875 -3.3125 C 5.265625 -3 5.375 -2.628906 5.375 -2.203125 C 5.375 -1.515625 5.148438 -0.953125 4.703125 -0.515625 C 4.265625 -0.078125 3.675781 0.140625 2.9375 0.140625 C 2.25 0.140625 1.679688 -0.0390625 1.234375 -0.40625 C 0.691406 -0.832031 0.421875 -1.410156 0.421875 -2.140625 C 0.421875 -2.554688 0.519531 -2.929688 0.71875 -3.265625 C 0.925781 -3.609375 1.25 -3.875 1.6875 -4.0625 Z M 1.984375 -5.5 C 1.984375 -5.21875 2.0625 -4.992188 2.21875 -4.828125 C 2.382813 -4.671875 2.601563 -4.59375 2.875 -4.59375 C 3.144531 -4.59375 3.363281 -4.671875 3.53125 -4.828125 C 3.695313 -4.992188 3.78125 -5.222656 3.78125 -5.515625 C 3.78125 -5.785156 3.695313 -6 3.53125 -6.15625 C 3.363281 -6.320313 3.148438 -6.40625 2.890625 -6.40625 C 2.609375 -6.40625 2.382813 -6.320313 2.21875 -6.15625 C 2.0625 -5.988281 1.984375 -5.769531 1.984375 -5.5 Z M 1.84375 -2.28125 C 1.84375 -1.882813 1.941406 -1.578125 2.140625 -1.359375 C 2.347656 -1.140625 2.601563 -1.03125 2.90625 -1.03125 C 3.207031 -1.03125 3.453125 -1.132813 3.640625 -1.34375 C 3.835938 -1.5625 3.9375 -1.867188 3.9375 -2.265625 C 3.9375 -2.617188 3.835938 -2.898438 3.640625 -3.109375 C 3.441406 -3.328125 3.191406 -3.4375 2.890625 -3.4375 C 2.535156 -3.4375 2.269531 -3.316406 2.09375 -3.078125 C 1.925781 -2.835938 1.84375 -2.570313 1.84375 -2.28125 Z M 1.84375 -2.28125 "
           id="path17" />
      </g>
      <g
         id="glyph-0-6">
        <path
           d="M 5.3125 -1.34375 L 5.3125 0 L 0.265625 0 C 0.316406 -0.507813 0.476563 -0.988281 0.75 -1.4375 C 1.019531 -1.894531 1.5625 -2.492188 2.375 -3.234375 C 3.019531 -3.835938 3.414063 -4.25 3.5625 -4.46875 C 3.757813 -4.769531 3.859375 -5.066406 3.859375 -5.359375 C 3.859375 -5.671875 3.769531 -5.914063 3.59375 -6.09375 C 3.425781 -6.269531 3.191406 -6.359375 2.890625 -6.359375 C 2.585938 -6.359375 2.347656 -6.265625 2.171875 -6.078125 C 1.992188 -5.898438 1.890625 -5.597656 1.859375 -5.171875 L 0.4375 -5.328125 C 0.519531 -6.117188 0.785156 -6.6875 1.234375 -7.03125 C 1.679688 -7.375 2.242188 -7.546875 2.921875 -7.546875 C 3.671875 -7.546875 4.253906 -7.34375 4.671875 -6.9375 C 5.097656 -6.539063 5.3125 -6.046875 5.3125 -5.453125 C 5.3125 -5.117188 5.25 -4.796875 5.125 -4.484375 C 5.007813 -4.179688 4.816406 -3.863281 4.546875 -3.53125 C 4.378906 -3.300781 4.066406 -2.976563 3.609375 -2.5625 C 3.148438 -2.144531 2.859375 -1.867188 2.734375 -1.734375 C 2.617188 -1.597656 2.523438 -1.46875 2.453125 -1.34375 Z M 5.3125 -1.34375 "
           id="path20" />
      </g>
      <g
         id="glyph-0-7">
        <path
           d="M 3.265625 0 L 3.265625 -1.515625 L 0.1875 -1.515625 L 0.1875 -2.78125 L 3.453125 -7.546875 L 4.671875 -7.546875 L 4.671875 -2.78125 L 5.59375 -2.78125 L 5.59375 -1.515625 L 4.671875 -1.515625 L 4.671875 0 Z M 3.265625 -2.78125 L 3.265625 -5.34375 L 1.546875 -2.78125 Z M 3.265625 -2.78125 "
           id="path23" />
      </g>
      <g
         id="glyph-0-8">
        <path
           d="M 0.4375 -4.1875 L 0.4375 -5.5 L 5.6875 -5.5 L 5.6875 -4.1875 Z M 0.4375 -1.90625 L 0.4375 -3.234375 L 5.6875 -3.234375 L 5.6875 -1.90625 Z M 0.4375 -1.90625 "
           id="path26" />
      </g>
      <g
         id="glyph-0-9">
        <path
           d="M 4.125 0 L 2.6875 0 L 2.6875 -5.421875 C 2.164063 -4.929688 1.546875 -4.570313 0.828125 -4.34375 L 0.828125 -5.640625 C 1.203125 -5.765625 1.609375 -6 2.046875 -6.34375 C 2.492188 -6.6875 2.800781 -7.085938 2.96875 -7.546875 L 4.125 -7.546875 Z M 4.125 0 "
           id="path29" />
      </g>
      <g
         id="glyph-0-10">
        <path
           d="M 5.328125 -5.671875 L 3.9375 -5.515625 C 3.894531 -5.804688 3.800781 -6.019531 3.65625 -6.15625 C 3.519531 -6.289063 3.335938 -6.359375 3.109375 -6.359375 C 2.804688 -6.359375 2.546875 -6.222656 2.328125 -5.953125 C 2.117188 -5.679688 1.988281 -5.113281 1.9375 -4.25 C 2.289063 -4.664063 2.738281 -4.875 3.28125 -4.875 C 3.875 -4.875 4.382813 -4.644531 4.8125 -4.1875 C 5.25 -3.726563 5.46875 -3.140625 5.46875 -2.421875 C 5.46875 -1.648438 5.238281 -1.03125 4.78125 -0.5625 C 4.332031 -0.101563 3.753906 0.125 3.046875 0.125 C 2.285156 0.125 1.660156 -0.164063 1.171875 -0.75 C 0.691406 -1.34375 0.453125 -2.3125 0.453125 -3.65625 C 0.453125 -5.03125 0.703125 -6.019531 1.203125 -6.625 C 1.710938 -7.238281 2.375 -7.546875 3.1875 -7.546875 C 3.757813 -7.546875 4.234375 -7.382813 4.609375 -7.0625 C 4.984375 -6.75 5.222656 -6.285156 5.328125 -5.671875 Z M 2.0625 -2.53125 C 2.0625 -2.0625 2.164063 -1.695313 2.375 -1.4375 C 2.59375 -1.1875 2.84375 -1.0625 3.125 -1.0625 C 3.382813 -1.0625 3.601563 -1.164063 3.78125 -1.375 C 3.96875 -1.582031 4.0625 -1.925781 4.0625 -2.40625 C 4.0625 -2.894531 3.960938 -3.25 3.765625 -3.46875 C 3.578125 -3.695313 3.335938 -3.8125 3.046875 -3.8125 C 2.773438 -3.8125 2.539063 -3.703125 2.34375 -3.484375 C 2.15625 -3.273438 2.0625 -2.957031 2.0625 -2.53125 Z M 2.0625 -2.53125 "
           id="path32" />
      </g>
      <g
         id="glyph-0-11">
        <path
           d="M 2.875 -7.546875 C 3.601563 -7.546875 4.175781 -7.285156 4.59375 -6.765625 C 5.082031 -6.148438 5.328125 -5.128906 5.328125 -3.703125 C 5.328125 -2.285156 5.078125 -1.265625 4.578125 -0.640625 C 4.171875 -0.128906 3.601563 0.125 2.875 0.125 C 2.144531 0.125 1.554688 -0.15625 1.109375 -0.71875 C 0.660156 -1.28125 0.4375 -2.28125 0.4375 -3.71875 C 0.4375 -5.132813 0.679688 -6.15625 1.171875 -6.78125 C 1.585938 -7.289063 2.15625 -7.546875 2.875 -7.546875 Z M 2.875 -6.359375 C 2.707031 -6.359375 2.550781 -6.300781 2.40625 -6.1875 C 2.269531 -6.070313 2.164063 -5.875 2.09375 -5.59375 C 2 -5.21875 1.953125 -4.585938 1.953125 -3.703125 C 1.953125 -2.828125 1.992188 -2.222656 2.078125 -1.890625 C 2.171875 -1.554688 2.285156 -1.332031 2.421875 -1.21875 C 2.554688 -1.113281 2.707031 -1.0625 2.875 -1.0625 C 3.050781 -1.0625 3.207031 -1.117188 3.34375 -1.234375 C 3.476563 -1.347656 3.585938 -1.546875 3.671875 -1.828125 C 3.765625 -2.203125 3.8125 -2.828125 3.8125 -3.703125 C 3.8125 -4.585938 3.765625 -5.191406 3.671875 -5.515625 C 3.585938 -5.847656 3.476563 -6.070313 3.34375 -6.1875 C 3.207031 -6.300781 3.050781 -6.359375 2.875 -6.359375 Z M 2.875 -6.359375 "
           id="path35" />
      </g>
      <g
         id="glyph-1-0">
        <path
           d="M 1.125 0 L 1.125 -5.625 L 5.625 -5.625 L 5.625 0 Z M 1.265625 -0.140625 L 5.484375 -0.140625 L 5.484375 -5.484375 L 1.265625 -5.484375 Z M 1.265625 -0.140625 "
           id="path38" />
      </g>
      <g
         id="glyph-1-1">
        <path
           d="M 0.6875 0 L 0.6875 -6.4375 L 2.90625 -6.4375 C 3.414063 -6.4375 3.800781 -6.40625 4.0625 -6.34375 C 4.425781 -6.257813 4.738281 -6.109375 5 -5.890625 C 5.34375 -5.597656 5.597656 -5.226563 5.765625 -4.78125 C 5.929688 -4.34375 6.015625 -3.832031 6.015625 -3.25 C 6.015625 -2.757813 5.957031 -2.328125 5.84375 -1.953125 C 5.726563 -1.578125 5.582031 -1.265625 5.40625 -1.015625 C 5.226563 -0.765625 5.03125 -0.566406 4.8125 -0.421875 C 4.601563 -0.285156 4.347656 -0.179688 4.046875 -0.109375 C 3.753906 -0.0351563 3.410156 0 3.015625 0 Z M 1.546875 -0.765625 L 2.921875 -0.765625 C 3.347656 -0.765625 3.679688 -0.800781 3.921875 -0.875 C 4.160156 -0.957031 4.351563 -1.070313 4.5 -1.21875 C 4.695313 -1.414063 4.851563 -1.6875 4.96875 -2.03125 C 5.082031 -2.375 5.140625 -2.785156 5.140625 -3.265625 C 5.140625 -3.941406 5.03125 -4.457031 4.8125 -4.8125 C 4.59375 -5.175781 4.320313 -5.421875 4 -5.546875 C 3.769531 -5.640625 3.40625 -5.6875 2.90625 -5.6875 L 1.546875 -5.6875 Z M 1.546875 -0.765625 "
           id="path41" />
      </g>
      <g
         id="glyph-1-2">
        <path
           d="M 2.90625 0 L 2.90625 -1.546875 L 0.109375 -1.546875 L 0.109375 -2.265625 L 3.046875 -6.4375 L 3.703125 -6.4375 L 3.703125 -2.265625 L 4.578125 -2.265625 L 4.578125 -1.546875 L 3.703125 -1.546875 L 3.703125 0 Z M 2.90625 -2.265625 L 2.90625 -5.171875 L 0.890625 -2.265625 Z M 2.90625 -2.265625 "
           id="path44" />
      </g>
      <g
         id="glyph-1-3">
        <path
           d="M 4.53125 -0.765625 L 4.53125 0 L 0.265625 0 C 0.265625 -0.1875 0.296875 -0.367188 0.359375 -0.546875 C 0.472656 -0.835938 0.648438 -1.125 0.890625 -1.40625 C 1.128906 -1.6875 1.472656 -2.007813 1.921875 -2.375 C 2.617188 -2.957031 3.085938 -3.414063 3.328125 -3.75 C 3.578125 -4.082031 3.703125 -4.398438 3.703125 -4.703125 C 3.703125 -5.015625 3.585938 -5.273438 3.359375 -5.484375 C 3.140625 -5.703125 2.851563 -5.8125 2.5 -5.8125 C 2.113281 -5.8125 1.804688 -5.695313 1.578125 -5.46875 C 1.347656 -5.238281 1.234375 -4.921875 1.234375 -4.515625 L 0.421875 -4.609375 C 0.472656 -5.210938 0.679688 -5.671875 1.046875 -5.984375 C 1.410156 -6.304688 1.898438 -6.46875 2.515625 -6.46875 C 3.128906 -6.46875 3.613281 -6.296875 3.96875 -5.953125 C 4.332031 -5.609375 4.515625 -5.1875 4.515625 -4.6875 C 4.515625 -4.425781 4.460938 -4.171875 4.359375 -3.921875 C 4.253906 -3.671875 4.078125 -3.40625 3.828125 -3.125 C 3.585938 -2.851563 3.1875 -2.476563 2.625 -2 C 2.144531 -1.601563 1.835938 -1.332031 1.703125 -1.1875 C 1.566406 -1.039063 1.457031 -0.898438 1.375 -0.765625 Z M 4.53125 -0.765625 "
           id="path47" />
      </g>
      <g
         id="glyph-1-4">
        <path
           d="M 0.375 -1.703125 L 1.171875 -1.8125 C 1.265625 -1.363281 1.414063 -1.039063 1.625 -0.84375 C 1.84375 -0.644531 2.113281 -0.546875 2.4375 -0.546875 C 2.800781 -0.546875 3.109375 -0.671875 3.359375 -0.921875 C 3.617188 -1.179688 3.75 -1.503906 3.75 -1.890625 C 3.75 -2.253906 3.628906 -2.550781 3.390625 -2.78125 C 3.160156 -3.019531 2.863281 -3.140625 2.5 -3.140625 C 2.34375 -3.140625 2.15625 -3.109375 1.9375 -3.046875 L 2.03125 -3.75 C 2.082031 -3.738281 2.125 -3.734375 2.15625 -3.734375 C 2.488281 -3.734375 2.789063 -3.820313 3.0625 -4 C 3.332031 -4.175781 3.46875 -4.445313 3.46875 -4.8125 C 3.46875 -5.101563 3.367188 -5.34375 3.171875 -5.53125 C 2.972656 -5.71875 2.71875 -5.8125 2.40625 -5.8125 C 2.101563 -5.8125 1.847656 -5.710938 1.640625 -5.515625 C 1.441406 -5.328125 1.3125 -5.039063 1.25 -4.65625 L 0.453125 -4.796875 C 0.554688 -5.328125 0.773438 -5.738281 1.109375 -6.03125 C 1.453125 -6.320313 1.878906 -6.46875 2.390625 -6.46875 C 2.742188 -6.46875 3.066406 -6.390625 3.359375 -6.234375 C 3.660156 -6.085938 3.890625 -5.882813 4.046875 -5.625 C 4.203125 -5.363281 4.28125 -5.085938 4.28125 -4.796875 C 4.28125 -4.515625 4.203125 -4.257813 4.046875 -4.03125 C 3.898438 -3.800781 3.679688 -3.617188 3.390625 -3.484375 C 3.773438 -3.398438 4.070313 -3.21875 4.28125 -2.9375 C 4.488281 -2.664063 4.59375 -2.320313 4.59375 -1.90625 C 4.59375 -1.34375 4.382813 -0.863281 3.96875 -0.46875 C 3.5625 -0.0820313 3.046875 0.109375 2.421875 0.109375 C 1.859375 0.109375 1.390625 -0.0546875 1.015625 -0.390625 C 0.640625 -0.722656 0.425781 -1.160156 0.375 -1.703125 Z M 0.375 -1.703125 "
           id="path50" />
      </g>
      <g
         id="glyph-1-5">
        <path
           d="M 0.375 -3.171875 C 0.375 -3.929688 0.453125 -4.546875 0.609375 -5.015625 C 0.765625 -5.484375 0.992188 -5.84375 1.296875 -6.09375 C 1.609375 -6.34375 2 -6.46875 2.46875 -6.46875 C 2.820313 -6.46875 3.128906 -6.394531 3.390625 -6.25 C 3.648438 -6.113281 3.863281 -5.914063 4.03125 -5.65625 C 4.195313 -5.394531 4.328125 -5.078125 4.421875 -4.703125 C 4.523438 -4.328125 4.578125 -3.816406 4.578125 -3.171875 C 4.578125 -2.421875 4.5 -1.8125 4.34375 -1.34375 C 4.1875 -0.882813 3.953125 -0.523438 3.640625 -0.265625 C 3.335938 -0.015625 2.945313 0.109375 2.46875 0.109375 C 1.851563 0.109375 1.367188 -0.113281 1.015625 -0.5625 C 0.585938 -1.09375 0.375 -1.960938 0.375 -3.171875 Z M 1.1875 -3.171875 C 1.1875 -2.117188 1.304688 -1.414063 1.546875 -1.0625 C 1.796875 -0.71875 2.101563 -0.546875 2.46875 -0.546875 C 2.832031 -0.546875 3.140625 -0.71875 3.390625 -1.0625 C 3.640625 -1.414063 3.765625 -2.117188 3.765625 -3.171875 C 3.765625 -4.234375 3.640625 -4.9375 3.390625 -5.28125 C 3.140625 -5.632813 2.832031 -5.8125 2.46875 -5.8125 C 2.101563 -5.8125 1.8125 -5.660156 1.59375 -5.359375 C 1.320313 -4.960938 1.1875 -4.234375 1.1875 -3.171875 Z M 1.1875 -3.171875 "
           id="path53" />
      </g>
      <g
         id="glyph-1-6">
        <path
           d="M 3.359375 0 L 2.5625 0 L 2.5625 -5.046875 C 2.375 -4.859375 2.125 -4.671875 1.8125 -4.484375 C 1.5 -4.304688 1.222656 -4.175781 0.984375 -4.09375 L 0.984375 -4.859375 C 1.421875 -5.054688 1.804688 -5.300781 2.140625 -5.59375 C 2.472656 -5.894531 2.707031 -6.1875 2.84375 -6.46875 L 3.359375 -6.46875 Z M 3.359375 0 "
           id="path56" />
      </g>
      <g
         id="glyph-1-7">
        <path
           d="M 0.59375 -4.15625 L 0.390625 -5.359375 L 0.390625 -6.4375 L 1.296875 -6.4375 L 1.296875 -5.359375 L 1.078125 -4.15625 Z M 0.59375 -4.15625 "
           id="path59" />
      </g>
      <g
         id="glyph-1-8">
        <path
           d="M -0.015625 0 L 2.46875 -6.4375 L 3.375 -6.4375 L 6.015625 0 L 5.046875 0 L 4.296875 -1.953125 L 1.59375 -1.953125 L 0.890625 0 Z M 1.84375 -2.640625 L 4.03125 -2.640625 L 3.359375 -4.4375 C 3.148438 -4.976563 3 -5.421875 2.90625 -5.765625 C 2.820313 -5.347656 2.703125 -4.9375 2.546875 -4.53125 Z M 1.84375 -2.640625 "
           id="path62" />
      </g>
      <g
         id="glyph-1-9">
        <path
           d="M 0.375 -1.6875 L 1.203125 -1.765625 C 1.265625 -1.359375 1.40625 -1.050781 1.625 -0.84375 C 1.851563 -0.644531 2.125 -0.546875 2.4375 -0.546875 C 2.820313 -0.546875 3.144531 -0.6875 3.40625 -0.96875 C 3.675781 -1.257813 3.8125 -1.640625 3.8125 -2.109375 C 3.8125 -2.566406 3.679688 -2.925781 3.421875 -3.1875 C 3.171875 -3.445313 2.84375 -3.578125 2.4375 -3.578125 C 2.175781 -3.578125 1.941406 -3.515625 1.734375 -3.390625 C 1.535156 -3.273438 1.375 -3.128906 1.25 -2.953125 L 0.515625 -3.046875 L 1.140625 -6.359375 L 4.34375 -6.359375 L 4.34375 -5.59375 L 1.765625 -5.59375 L 1.421875 -3.875 C 1.804688 -4.132813 2.210938 -4.265625 2.640625 -4.265625 C 3.203125 -4.265625 3.675781 -4.070313 4.0625 -3.6875 C 4.445313 -3.300781 4.640625 -2.800781 4.640625 -2.1875 C 4.640625 -1.601563 4.472656 -1.097656 4.140625 -0.671875 C 3.722656 -0.148438 3.15625 0.109375 2.4375 0.109375 C 1.851563 0.109375 1.375 -0.0507813 1 -0.375 C 0.632813 -0.707031 0.425781 -1.144531 0.375 -1.6875 Z M 0.375 -1.6875 "
           id="path65" />
      </g>
    </g>
  </defs>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g74">
    <use
       xlink:href="#glyph-0-1"
       x="12.075892"
       y="15.006933"
       id="use72" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g78">
    <use
       xlink:href="#glyph-0-1"
       x="72.103622"
       y="15.006933"
       id="use76" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g82">
    <use
       xlink:href="#glyph-0-1"
       x="42.08976"
       y="15.006933"
       id="use80" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g86">
    <use
       xlink:href="#glyph-0-2"
       x="132.13136"
       y="15.006933"
       id="use84" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g90">
    <use
       xlink:href="#glyph-0-3"
       x="102.11749"
       y="15.006933"
       id="use88" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g94">
    <use
       xlink:href="#glyph-0-4"
       x="162.00453"
       y="15.006933"
       id="use92" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g98">
    <use
       xlink:href="#glyph-0-5"
       x="192.15909"
       y="15.006933"
       id="use96" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g102">
    <use
       xlink:href="#glyph-0-6"
       x="252.18683"
       y="15.006933"
       id="use100" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g106">
    <use
       xlink:href="#glyph-0-6"
       x="222.17296"
       y="15.006933"
       id="use104" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g110">
    <use
       xlink:href="#glyph-0-1"
       x="312.21457"
       y="15.006933"
       id="use108" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g114">
    <use
       xlink:href="#glyph-0-7"
       x="282.20068"
       y="15.006933"
       id="use112" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g118">
    <use
       xlink:href="#glyph-0-8"
       x="342.08774"
       y="15.006933"
       id="use116" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g122">
    <use
       xlink:href="#glyph-0-9"
       x="402.25616"
       y="15.006933"
       id="use120" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g126">
    <use
       xlink:href="#glyph-0-9"
       x="372.24231"
       y="15.006933"
       id="use124" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g130">
    <use
       xlink:href="#glyph-0-10"
       x="462.28391"
       y="15.006933"
       id="use128" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g134">
    <use
       xlink:href="#glyph-0-3"
       x="432.27002"
       y="15.006933"
       id="use132" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g138">
    <use
       xlink:href="#glyph-0-11"
       x="492.29776"
       y="15.006933"
       id="use136" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g142">
    <use
       xlink:href="#glyph-0-11"
       x="522.31165"
       y="15.006933"
       id="use140" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g148">
    <use
       xlink:href="#glyph-1-1"
       x="9.2503681"
       y="37.517334"
       id="use144" />
    <use
       xlink:href="#glyph-1-2"
       x="15.752884"
       y="37.517334"
       id="use146" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g154">
    <use
       xlink:href="#glyph-1-1"
       x="69.278099"
       y="37.517334"
       id="use150" />
    <use
       xlink:href="#glyph-1-3"
       x="75.780617"
       y="37.517334"
       id="use152" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g160">
    <use
       xlink:href="#glyph-1-1"
       x="39.264233"
       y="37.517334"
       id="use156" />
    <use
       xlink:href="#glyph-1-4"
       x="45.76675"
       y="37.517334"
       id="use158" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g166">
    <use
       xlink:href="#glyph-1-1"
       x="129.30583"
       y="37.517334"
       id="use162" />
    <use
       xlink:href="#glyph-1-5"
       x="135.80835"
       y="37.517334"
       id="use164" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g172">
    <use
       xlink:href="#glyph-1-1"
       x="99.291969"
       y="37.517334"
       id="use168" />
    <use
       xlink:href="#glyph-1-6"
       x="105.79449"
       y="37.517334"
       id="use170" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g180">
    <use
       xlink:href="#glyph-1-1"
       x="188.46599"
       y="37.517334"
       id="use174" />
    <use
       xlink:href="#glyph-1-2"
       x="194.96851"
       y="37.517334"
       id="use176" />
    <use
       xlink:href="#glyph-1-7"
       x="199.97618"
       y="37.517334"
       id="use178" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g188">
    <use
       xlink:href="#glyph-1-1"
       x="248.49371"
       y="37.517334"
       id="use182" />
    <use
       xlink:href="#glyph-1-3"
       x="254.99623"
       y="37.517334"
       id="use184" />
    <use
       xlink:href="#glyph-1-7"
       x="260.00391"
       y="37.517334"
       id="use186" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g196">
    <use
       xlink:href="#glyph-1-1"
       x="218.47986"
       y="37.517334"
       id="use190" />
    <use
       xlink:href="#glyph-1-4"
       x="224.98236"
       y="37.517334"
       id="use192" />
    <use
       xlink:href="#glyph-1-7"
       x="229.99005"
       y="37.517334"
       id="use194" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g204">
    <use
       xlink:href="#glyph-1-1"
       x="308.52145"
       y="37.517334"
       id="use198" />
    <use
       xlink:href="#glyph-1-5"
       x="315.02396"
       y="37.517334"
       id="use200" />
    <use
       xlink:href="#glyph-1-7"
       x="320.03165"
       y="37.517334"
       id="use202" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g212">
    <use
       xlink:href="#glyph-1-1"
       x="278.5076"
       y="37.517334"
       id="use206" />
    <use
       xlink:href="#glyph-1-6"
       x="285.0101"
       y="37.517334"
       id="use208" />
    <use
       xlink:href="#glyph-1-7"
       x="290.01779"
       y="37.517334"
       id="use210" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g218">
    <use
       xlink:href="#glyph-1-8"
       x="399.67685"
       y="37.517334"
       id="use214" />
    <use
       xlink:href="#glyph-1-2"
       x="405.68256"
       y="37.517334"
       id="use216" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g224">
    <use
       xlink:href="#glyph-1-8"
       x="369.66296"
       y="37.517334"
       id="use220" />
    <use
       xlink:href="#glyph-1-9"
       x="375.66867"
       y="37.517334"
       id="use222" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g230">
    <use
       xlink:href="#glyph-1-8"
       x="459.70459"
       y="37.517334"
       id="use226" />
    <use
       xlink:href="#glyph-1-3"
       x="465.7103"
       y="37.517334"
       id="use228" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g236">
    <use
       xlink:href="#glyph-1-8"
       x="429.6907"
       y="37.517334"
       id="use232" />
    <use
       xlink:href="#glyph-1-4"
       x="435.69641"
       y="37.517334"
       id="use234" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g242">
    <use
       xlink:href="#glyph-1-8"
       x="489.71844"
       y="37.517334"
       id="use238" />
    <use
       xlink:href="#glyph-1-6"
       x="495.72415"
       y="37.517334"
       id="use240" />
  </g>
  <g
     fill="#000000"
     fill-opacity="1"
     id="g248">
    <use
       xlink:href="#glyph-1-8"
       x="519.7323"
       y="37.517334"
       id="use244" />
    <use
       xlink:href="#glyph-1-5"
       x="525.73804"
       y="37.517334"
       id="use246" />
  </g>
</svg>
)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIGRg-UH-bwa"
      },
      "source": [
        "The CoLab follows on from the [Understanding Addition in Transformers](https://colab.research.google.com/drive/1p71dC3LCPJIfFKqr2rhmBZBOyyMpovyn) which explains integer addition and documents a rare high-loss use case called \"Use Sum 9 Cascade\". This CoLab seeks to eliminate this high-loss use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVxj2am_VxJo"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPLArf0NMGay"
      },
      "source": [
        "# Part 1: Setup\n",
        "Imports standard libraries. Don't bother reading. Skip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oMYSAsqK4HT",
        "outputId": "f1baab88-09cc-4e6d-b9e0-1b740d6bc358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    %pip install kaleido\n",
        "    #%pip install git+https://github.com/neelnanda-io/TransformerLens.git@new-demo\n",
        "    %pip install transformer_lens\n",
        "    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "    #%pip install circuitsvis\n",
        "    %pip install jaxtyping\n",
        "    %pip install einops\n",
        "    %pip install fancy_einsum\n",
        "    %pip install torchtyping\n",
        "    %pip install transformers\n",
        "    %pip install datasets\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeSetoptLHyY"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOa8dMM_LJfm"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvBTz_dPLLak"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from jaxtyping import Float, Int\n",
        "from torchtyping import TensorType as TT\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "\n",
        "import circuitsvis as cv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVQnhp21LOSa"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jizlxUZLUnN"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Ui9q02MMIx"
      },
      "source": [
        "# Part 2: Configuration\n",
        "This section defines the token embedding / unembedding and creates the model.\n",
        "\n",
        "The model has been successfully trained to do 2, 5, 10, 15 digit integer addition. The default is n_digits = 5.\n",
        "\n",
        "The model has been successfully trained with 2, 3 or 4 attention heads. The  default is n_heads = 3. More heads do not increase accuracy\n",
        "\n",
        "The model has been successfully trained with 1 or 2 layers. The default is n_layers = 2. Two layers increases accuracy in Cascading UseSum9 cases over 1 layer.\n",
        "\n",
        "long_equals changes the question format \"12345+11111=023465\" to \"12345+11111equals023465\". This does not increase accuracy.\n",
        "\n",
        "Setting more_ms9_cases to true (to increase the percentage of UseSum9 cases in the training data) speeds up training, but doesnt increase trained accuracy.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjwlUfwgXVOF"
      },
      "outputs": [],
      "source": [
        "more_ms9_cases = True # When doing addition, increase frequency of Make Sum 9 cases in training data\n",
        "long_equals = False # If true, in question formats use \"equals\" instead of \"=\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsOoUjh0MwtP"
      },
      "outputs": [],
      "source": [
        "# Tokens used in vocab. (Token indexes 0 to 9 represent digits 0 to 9)\n",
        "PLUS_INDEX = 10\n",
        "MINUS_INDEX = 11\n",
        "EQUALS_INDEX = 12\n",
        "E_INDEX = 13\n",
        "Q_INDEX = 14\n",
        "U_INDEX = 14\n",
        "A_INDEX = 16\n",
        "L_INDEX = 17\n",
        "S_INDEX = 18\n",
        "\n",
        "#@markdown Model\n",
        "n_layers = 2 #@param\n",
        "d_vocab = (S_INDEX+1 if long_equals == True else EQUALS_INDEX+1)\n",
        "n_heads = 3 #@param\n",
        "d_model = ( 512 // n_heads ) * n_heads    # About 512, and divisible by n_heads\n",
        "d_head = d_model // n_heads               # About 170 when n_heads == 3\n",
        "d_mlp = 4 * d_model\n",
        "seed = 129000 #@param\n",
        "\n",
        "#@markdown Data\n",
        "n_digits = 5 #@param\n",
        "n_ctx = 3 * n_digits + (8 if long_equals == True else 3)\n",
        "act_fn = 'relu'\n",
        "batch_size = 64 #@param\n",
        "\n",
        "#@markdown Optimizer\n",
        "lr = 0.00008 #@param\n",
        "weight_decay = 0.1 #@param\n",
        "n_training_steps = 20000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJZoF1bpLvcX"
      },
      "outputs": [],
      "source": [
        "# Train or load model? Training saves the model weights in a temporary CoLab file\n",
        "train_model = True #@param\n",
        "\n",
        "# The name of the temporary CoLab file to save model to\n",
        "pth_location = \"model.pth\"\n",
        "\n",
        "# Save graphs to files as PDF and HTML\n",
        "save_graph_to_file = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXO_lNJqL1LQ"
      },
      "outputs": [],
      "source": [
        "# Embedding / Unembedding\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    tokens = utils.to_numpy(tokens)\n",
        "    x = \"\".join([str(i) for i in tokens[:n_digits]])\n",
        "    y = \"\".join([str(i) for i in tokens[n_digits+1:n_digits*2+1]])\n",
        "    z = \"\".join([str(i) for i in tokens[n_ctx-n_digits-1:]])\n",
        "    equals = \"equals\" if long_equals == True else \"=\"\n",
        "    operator = \"+\"\n",
        "    return f\"{x}{operator}{y}{equals}{z}\"\n",
        "\n",
        "def string_to_tokens(string, batch=False):\n",
        "    lookup = {str(i):i for i in range(10)}\n",
        "    lookup['+']=PLUS_INDEX\n",
        "    lookup['-']=MINUS_INDEX\n",
        "    lookup['=']=EQUALS_INDEX\n",
        "\n",
        "    if long_equals == True:\n",
        "      lookup['e']=E_INDEX\n",
        "      lookup['q']=Q_INDEX\n",
        "      lookup['u']=U_INDEX\n",
        "      lookup['a']=A_INDEX\n",
        "      lookup['l']=L_INDEX\n",
        "      lookup['s']=S_INDEX\n",
        "\n",
        "    tokens = [lookup[i] for i in string if i not in '\\n ']\n",
        "    if batch:\n",
        "        return torch.tensor(tokens)[None, :]\n",
        "    else:\n",
        "        return torch.tensor(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVwGVXCAL4Zx"
      },
      "outputs": [],
      "source": [
        "# Transformer creation\n",
        "\n",
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = n_layers,\n",
        "    n_heads = n_heads,\n",
        "    d_model = d_model,\n",
        "    d_head = d_head,\n",
        "    d_mlp = d_mlp,\n",
        "    act_fn = act_fn,\n",
        "    normalization_type = 'LN',\n",
        "    d_vocab=d_vocab,\n",
        "    d_vocab_out=d_vocab,\n",
        "    n_ctx=n_ctx,\n",
        "    init_weights = True,\n",
        "    device=\"cuda\",\n",
        "    seed = seed,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(cfg)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(),\n",
        "                        lr=lr,\n",
        "                        weight_decay=weight_decay,\n",
        "                        betas=(0.9, 0.98))\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sil5uniOECa"
      },
      "source": [
        "# Part 3: Data Generator. Addition sub-task categorisation\n",
        "This section defines the loss function and the training/tesing data generator.\n",
        "\n",
        "It also defines functions to categorise the training data by the addition sub-task defined in the paper. The addition sub tasks are abbreviated as:\n",
        "- BA is Base Add. Calculates the sum of two digits Dn and Dn' modulo 10, ignoring any carry over from previous columns.\n",
        "- MC1 is Make Carry 1. Evaluates to true if adding digits Dn and Dn' results in a carry over of 1 to the next column.\n",
        "- MS9 is Make Sum 9. Evaluates to true if adding digits Dn and Dn' gives exactly 9.\n",
        "- UC1 is Use Carry 1. Takes the previous column's carry output and adds it to the sum of the current digit pair.\n",
        "- US9 is Use Sum 9. Propagates (aka cascades) a carry over of 1 to the next column if the current column sums to 9 and the previous column generated a carry over. US9 is the most complex task as it spans three digits. For some rare questions (e.g. 00555 + 00445 = 01000) US9 applies to up to four sequential digits, causing a chain effect, with the MC1 cascading through multiple digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxz7eNnwwzjg"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "\n",
        "# Calculate the per-token probability by comparing a batch of prediction \"logits\" to answer \"tokens\"\n",
        "def logits_to_tokens_loss(logits, tokens):\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the addition answer probabilities\n",
        "  ans_logits = logits[:, -(n_digits+2):-1]\n",
        "\n",
        "  # Convert raw score (logits) vector into a probability distribution.\n",
        "  # Emphasize the largest scores and suppress the smaller ones, to make them more distinguishable.\n",
        "  ans_probs = F.log_softmax(ans_logits.to(torch.float64), dim=-1)\n",
        "\n",
        "  max_indices = torch.argmax(ans_probs, dim=-1)\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the models answer.\n",
        "  ans_tokens = tokens[:, -(n_digits+1):]\n",
        "\n",
        "  # Extract values from the ans_probs tensor, based on indices from the ans_tokens tensor\n",
        "  ans_loss = torch.gather(ans_probs, -1, ans_tokens[:, :, None])[..., 0]\n",
        "\n",
        "  return ans_loss, max_indices\n",
        "\n",
        "# Calculate loss as negative of average per-token mean probability\n",
        "def loss_fn(ans_loss):\n",
        "  return -ans_loss.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3brLMbIOEw3"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" data generator function. Invoked using next().\n",
        "# \"Addition\" batch entries are formated XXXXX+YYYYY=ZZZZZZ e.g. 55003+80002=135005\n",
        "# \"Subtraction\" batch entries are formated XXXXX-YYYYY=ZZZZZZ e.g. 55003-80002=-24999, 80002-55003=024999\n",
        "# Note that answer has one more digit than the question\n",
        "# Returns characteristics of each batch entry to aid later analysis\n",
        "def data_generator(batch_size, n_digits, seed):\n",
        "    torch.manual_seed(seed)\n",
        "    while True:\n",
        "        #generate a batch of questions (answers calculated below)\n",
        "        batch = torch.zeros((batch_size, n_ctx)).to(torch.int64)\n",
        "        x = torch.randint(0, 10, (batch_size, n_digits))\n",
        "        y = torch.randint(0, 10, (batch_size, n_digits))\n",
        "\n",
        "        if more_ms9_cases == True:\n",
        "          # The UseSum9 task is compound and rare and so hard to learn.\n",
        "          # For some of batches, we increase the MakeSum9 case frequency\n",
        "          # UseSum9 also relies on MakeCarry1 (50%) from previous column.\n",
        "          # So UseSum9 frequency is increased by 60% * 40% * 50% = 12%\n",
        "          if random.randint(1, 5) < 3: # 60%\n",
        "            # Flatten x and y to 1D tensors\n",
        "            x_flat = x.view(-1)\n",
        "            y_flat = y.view(-1)\n",
        "\n",
        "            num_elements_to_modify = int(0.40 * x.numel()) # 40%\n",
        "            indices_to_modify = torch.randperm(x_flat.numel())[:num_elements_to_modify]\n",
        "            if random.randint(1, 2) == 1:\n",
        "              x_flat[indices_to_modify] = 9 - y_flat[indices_to_modify]\n",
        "            else:\n",
        "              y_flat[indices_to_modify] = 9 - x_flat[indices_to_modify]\n",
        "\n",
        "            # Reshape x and y back to its original shape\n",
        "            x = x_flat.view(x.shape)\n",
        "            y = y_flat.view(x.shape)\n",
        "\n",
        "        batch[:, :n_digits] = x\n",
        "        batch[:, n_digits] = PLUS_INDEX\n",
        "        batch[:, 1+n_digits:1+n_digits*2] = y\n",
        "        if long_equals == True:\n",
        "          batch[:, 1+n_digits*2] = E_INDEX\n",
        "          batch[:, 2+n_digits*2] = Q_INDEX\n",
        "          batch[:, 3+n_digits*2] = U_INDEX\n",
        "          batch[:, 4+n_digits*2] = A_INDEX\n",
        "          batch[:, 5+n_digits*2] = L_INDEX\n",
        "          batch[:, 6+n_digits*2] = S_INDEX\n",
        "        else:\n",
        "          batch[:, 1+n_digits*2] = EQUALS_INDEX\n",
        "\n",
        "        # These attributes are used for testing addition\n",
        "        base_adds = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
        "        make_carry1s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
        "        sum9s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
        "        use_carry1s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
        "        use_sum9s = torch.zeros((batch_size,n_digits)).to(torch.int64)\n",
        "\n",
        "        # generate the addition question answers & other info for testing\n",
        "        for i in range(n_digits):\n",
        "            # the column in the test attributes being updated\n",
        "            test_col = n_digits-1-i\n",
        "\n",
        "            base_add = batch[:, n_digits-1-i] + batch[:, 2*n_digits-i]\n",
        "            base_adds[:, test_col] = base_add % 10\n",
        "\n",
        "            sum9 = (base_add == 9)\n",
        "            sum9s[:, test_col] = sum9\n",
        "\n",
        "            if i>0:\n",
        "              use_carry1s[:, test_col] = make_carry1s[:, test_col+1]\n",
        "            use_carry = use_carry1s[:, test_col]\n",
        "\n",
        "            use_sum9s[:, test_col] = sum9 & use_carry;\n",
        "\n",
        "            digit_sum = base_add + use_carry1s[:, test_col]\n",
        "\n",
        "            make_carry = (digit_sum >= 10)\n",
        "            make_carry1s[:, test_col] = make_carry\n",
        "\n",
        "            batch[:, -1-i] = (digit_sum % 10)\n",
        "\n",
        "        # Final (possible) carry to highest digit of the sum\n",
        "        batch[:, -1-n_digits] = make_carry1s[:, 0]\n",
        "\n",
        "        yield batch.cuda(), base_adds.cuda(), make_carry1s.cuda(), sum9s.cuda(), use_carry1s.cuda(), use_sum9s.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6KEz_pnB-YE"
      },
      "outputs": [],
      "source": [
        "ds = data_generator(batch_size, n_digits, seed)\n",
        "\n",
        "tokens, base_adds, make_carry1s, sum9s, use_carry1s, use_sum9s = next(ds)\n",
        "\n",
        "print(tokens[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0jBxPgNOJNB"
      },
      "outputs": [],
      "source": [
        "# Data generator unit test (optional)\n",
        "# This unit test checks that the above data_generator function is sensible\n",
        "def unit_test_data_generator(train_tokens, train_use_carry1s, train_make_carry1s):\n",
        "  test_token = train_tokens[0]\n",
        "  test_use_carry = train_use_carry1s[0]\n",
        "  test_make_carry = train_make_carry1s[0]\n",
        "\n",
        "  if n_digits == 5:\n",
        "    digits = test_token.cpu().numpy()\n",
        "    use = test_use_carry.cpu().numpy()\n",
        "    force = test_make_carry.cpu().numpy()\n",
        "\n",
        "    num1 = digits[0]*10000 + digits[1]*1000 + digits[2]*100 + digits[3]*10 + digits[4];\n",
        "    num2 = digits[6]*10000 + digits[7]*1000 + digits[8]*100 + digits[9]*10 + digits[10];\n",
        "    if long_equals == True:\n",
        "      sum = digits[17]*100000 + digits[18]*10000 + digits[19]*1000 + digits[20]*100 + digits[21]*10 + digits[22];\n",
        "    else:\n",
        "      sum = digits[12]*100000 + digits[13]*10000 + digits[14]*1000 + digits[15]*100 + digits[16]*10 + digits[17];\n",
        "\n",
        "    assert num1 + num2 == sum, \"Unit test failed: Data generator: Bad sum\"\n",
        "    assert (digits[4]+digits[10]+use[4]>=10) == force[4], \"Unit test failed: Data generator: Bad carry 0\"\n",
        "    assert (digits[3]+digits[9]+use[3]>=10) == force[3], \"Unit test failed: Data generator: Bad carry 1\"\n",
        "    assert (digits[2]+digits[8]+use[2]>=10) == force[2], \"Unit test failed: Data generator: Bad carry 2\"\n",
        "    assert (digits[1]+digits[7]+use[1]>=10) == force[1], \"Unit test failed: Data generator: Bad carry 3\"\n",
        "    assert (digits[0]+digits[6]+use[0]>=10) == force[0], \"Unit test failed: Data generator: Bad carry 4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaTSmnUBOLfP"
      },
      "outputs": [],
      "source": [
        "# Base-Add-only loss\n",
        "# Identify the subset of (simple) tokens that only require BA (not UC1 or US9) to get the correct answer\n",
        "# Array index 0 is the 'Units' digit. Array index 3 is the 'Thousands' digit.\n",
        "ba_alldigits_loss = []\n",
        "ba_alldigits_oneloss = 0\n",
        "\n",
        "ba_perdigit_loss = []\n",
        "ba_perdigit_cases = 0\n",
        "ba_total_cases = 0\n",
        "\n",
        "\n",
        "# Base Add AllDigits\n",
        "# Identity the tokens in the batch where UC1 is false for all columns simultaneously, so only BA is required on all digits\n",
        "def calculate_ba_oneloss(tokens, per_token_losses, base_adds, use_carry1s):\n",
        "  global ba_alldigits_oneloss\n",
        "\n",
        "  answer = 0\n",
        "  any_use_carry1s = torch.any(use_carry1s.bool(), dim=1)\n",
        "  no_use_carry1s = ~ any_use_carry1s\n",
        "  num_cases = utils.to_numpy(torch.sum(no_use_carry1s))\n",
        "  if num_cases > 0 :\n",
        "    filtered_loss = per_token_losses[:, -n_digits:] * no_use_carry1s[:, None]\n",
        "    sum_loss = torch.sum(filtered_loss)\n",
        "    answer = - utils.to_numpy(sum_loss) / num_cases\n",
        "    answer = answer / n_digits  # Approximately align the scale of ba_alldigits_loss to ba_perdigit_loss\n",
        "  ba_alldigits_oneloss = answer\n",
        "\n",
        "\n",
        "def calculate_ba_loss(tokens, per_token_losses, base_adds, use_carry1s):\n",
        "  global ba_perdigit_cases\n",
        "  global ba_total_cases\n",
        "\n",
        "  # Base Add All Digits\n",
        "  # Identity the tokens in the batch where UC1 is false for all columns simultaneously, so only BA is required on all digits\n",
        "  calculate_ba_oneloss(tokens, per_token_losses, base_adds, use_carry1s)\n",
        "  ba_alldigits_loss.append(ba_alldigits_oneloss)\n",
        "\n",
        "\n",
        "  # Base Add Per Digit\n",
        "  # For each token in the batch, identity the digit columns (e.g. 3) where use_carry is false, so only BA is required on that digit\n",
        "  ba_perdigit_cases = 0;\n",
        "  for digit_num in range(n_digits):\n",
        "    answer = 0\n",
        "    no_use_carry = 1 - use_carry1s[:, -1-digit_num]\n",
        "    num_cases = utils.to_numpy(torch.sum(no_use_carry))\n",
        "    ba_perdigit_cases += num_cases\n",
        "    ba_total_cases += num_cases\n",
        "    if num_cases > 0 :\n",
        "      filtered_loss = per_token_losses[:, -1-digit_num] * no_use_carry\n",
        "      sum_loss = torch.sum(filtered_loss)\n",
        "      answer = - utils.to_numpy(sum_loss) / num_cases\n",
        "    if len(ba_perdigit_loss)<=digit_num:\n",
        "      ba_perdigit_loss.append([])\n",
        "    if (num_cases == 0) & (len(ba_perdigit_loss[digit_num]) > 0) :\n",
        "      answer = ba_perdigit_loss[digit_num][-1] # Use the previous step's loss. Improves graph\n",
        "    ba_perdigit_loss[digit_num].append(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq8Lyu7WOP7q"
      },
      "outputs": [],
      "source": [
        "# Use Carry 1 loss\n",
        "# Identify the subset of tokens that require UC1 (but not US9) to get the correct answer\n",
        "# Array index 0 is the 'Units' digit. Array index 3 is the 'Thousands' digit.\n",
        "uc1_anydigits_loss = []\n",
        "uc1_anydigits_oneloss = 0\n",
        "\n",
        "uc1_perdigit_loss = []\n",
        "uc1_perdigit_cases = 0\n",
        "uc1_total_cases = 0\n",
        "\n",
        "\n",
        "# UC1 AnyDigits (exclude Sum9)\n",
        "# Identity the tokens in the batch where UC1 is used at least once over the columns & Sum9 is never used\n",
        "def calculate_uc1_loss_any(tokens, per_token_losses, use_carry1s, sum9s):\n",
        "  global uc1_anydigits_oneloss\n",
        "\n",
        "  num_use_carry1s = torch.sum(use_carry1s, dim=1)\n",
        "  any_use_carry1s = torch.where( num_use_carry1s != 0, 1, 0 ) # At least one digit uses UC1\n",
        "  num_sum9s = torch.sum(use_sum9s, dim=1)\n",
        "  no_sum9s = torch.where( num_sum9s == 0, 1, 0 ) # No digits have Sum9 true\n",
        "  filtered_cases = any_use_carry1s & no_sum9s\n",
        "  num_cases = utils.to_numpy(torch.sum(filtered_cases))\n",
        "  filtered_indices = torch.nonzero(filtered_cases).squeeze()\n",
        "  filtered_token_losses = per_token_losses[filtered_indices]\n",
        "  answer = - filtered_token_losses.mean()\n",
        "  uc1_anydigits_oneloss = utils.to_numpy(answer)\n",
        "\n",
        "\n",
        "def calculate_uc1_loss(tokens, per_token_losses, use_carry1s, sum9s):\n",
        "  global uc1_perdigit_cases\n",
        "  global uc1_total_cases\n",
        "\n",
        "  # UC1 AnyDigits (exclude Sum9)\n",
        "  # Identity the tokens in the batch where UC1 is used at least once over the columns & Sum9 is never used\n",
        "  calculate_uc1_loss_any(tokens, per_token_losses, use_carry1s, sum9s)\n",
        "  uc1_anydigits_loss.append(uc1_anydigits_oneloss)\n",
        "\n",
        "  # UC1 PerDigit (exclude Sum9)\n",
        "  # For each token in the batch, identity the digit columns (e.g. 3) where UC1 is used on the columns & Sum9 is not true\n",
        "  uc1_perdigit_cases = 0\n",
        "  for digit_num in range(n_digits):\n",
        "    answer = 0\n",
        "    use_carry = use_carry1s[:, -1-digit_num]\n",
        "    no_sum9 = 1 - sum9s[:, -1-digit_num]\n",
        "    filtered_cases = use_carry & no_sum9\n",
        "    num_cases = utils.to_numpy(torch.sum(filtered_cases))\n",
        "    uc1_perdigit_cases += num_cases\n",
        "    uc1_total_cases += num_cases\n",
        "    if num_cases > 0 :\n",
        "      filtered_loss = per_token_losses[:, -1-digit_num] * filtered_cases\n",
        "      sum_loss = torch.sum(filtered_loss)\n",
        "      answer = - utils.to_numpy(sum_loss) / num_cases\n",
        "    if len(uc1_perdigit_loss)<=digit_num:\n",
        "      uc1_perdigit_loss.append([])\n",
        "    if (num_cases==0) & (len(uc1_perdigit_loss[digit_num]) > 0) :\n",
        "      answer = uc1_perdigit_loss[digit_num][-1] # Use the previous step's loss. Improves graph\n",
        "    uc1_perdigit_loss[digit_num].append(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMbbGIdnOS6o"
      },
      "outputs": [],
      "source": [
        "# Use Sum 9 loss\n",
        "# Identify the subset of tokens that require US9 (being Sum9 and Carry1 from prev column) to get the correct answer\n",
        "# Array index 0 is the 'Units' digit. Array index 3 is the 'Thousands' digit.\n",
        "us9_anydigits_loss = []\n",
        "us9_anydigits_oneloss = 0\n",
        "\n",
        "us9_perdigit_loss = []\n",
        "us9_perdigit_cases = 0\n",
        "us9_total_cases = 0\n",
        "\n",
        "\n",
        "# US9 OneDigit\n",
        "# Identity the tokens in the batch where US9 is used at least once over the columns\n",
        "def calculate_us9_oneloss(tokens, per_token_losses, use_sum9s):\n",
        "  global us9_anydigits_oneloss\n",
        "\n",
        "  num_use_sum9s = torch.sum(use_sum9s, dim=1)\n",
        "  filtered_num_use_sum9s = torch.where( num_use_sum9s != 0, 1, 0 ) # At least OneDigit uses US9\n",
        "  num_cases = utils.to_numpy(torch.sum(filtered_num_use_sum9s))\n",
        "  filtered_indices = torch.nonzero(filtered_num_use_sum9s).squeeze()\n",
        "  filtered_token_losses = per_token_losses[filtered_indices]\n",
        "  answer = - filtered_token_losses.mean()\n",
        "  us9_anydigits_oneloss = utils.to_numpy(answer);\n",
        "\n",
        "\n",
        "def calculate_us9_loss(tokens, per_token_losses, use_sum9s):\n",
        "  global us9_perdigit_cases\n",
        "  global us9_total_cases\n",
        "\n",
        "  # US9 OneDigit\n",
        "  # Identity the tokens in the batch where US9 is used at least once over the columns\n",
        "  calculate_us9_oneloss(tokens, per_token_losses, use_sum9s)\n",
        "  us9_anydigits_loss.append(us9_anydigits_oneloss)\n",
        "\n",
        "  # For each token in the batch, identity the digit columns (e.g. 3) where US9 is used\n",
        "  us9_perdigit_cases = 0\n",
        "  for digit_num in range(n_digits):\n",
        "    answer = 0\n",
        "    use_carry = use_carry1s[:, -1-digit_num]\n",
        "    use_sum9 = sum9s[:, -1-digit_num]\n",
        "    filtered_cases = use_carry & use_sum9\n",
        "    num_cases = utils.to_numpy(torch.sum(filtered_cases))\n",
        "    us9_perdigit_cases += num_cases\n",
        "    us9_total_cases += num_cases\n",
        "    if num_cases > 0 :\n",
        "      filtered_loss = per_token_losses[:, -1-digit_num] * filtered_cases\n",
        "      sum_loss = torch.sum(filtered_loss)\n",
        "      answer = - utils.to_numpy(sum_loss) / num_cases\n",
        "    if len(us9_perdigit_loss)<=digit_num:\n",
        "      us9_perdigit_loss.append([])\n",
        "    if (num_cases==0) & (len(us9_perdigit_loss[digit_num]) > 0) :\n",
        "      answer = us9_perdigit_loss[digit_num][-1] # Use the previous step's loss. Improves graph\n",
        "    us9_perdigit_loss[digit_num].append(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E2V3nZEOUCa"
      },
      "outputs": [],
      "source": [
        "# Check that us9_perdigit_loss, uc1_perdigit_loss and ba_perdigit_loss do NOT overlap\n",
        "# This ensures the graphs of each are non-overlapping\n",
        "def unit_test_nonoverlapping():\n",
        "  global ba_perdigit_cases\n",
        "  global ba_total_cases\n",
        "  global uc1_perdigit_cases\n",
        "  global uc1_total_cases\n",
        "  global us9_perdigit_cases\n",
        "  global us9_total_cases\n",
        "\n",
        "  perdigit_numcases = us9_perdigit_cases + uc1_perdigit_cases + ba_perdigit_cases\n",
        "  assert (perdigit_numcases == batch_size * n_digits), \"Cases overlap: \" + str(perdigit_numcases) + \" != \" + str(batch_size*n_digits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEI4jjJmOevS"
      },
      "source": [
        "# Part 4: Train model with Infinite Data\n",
        "Train model for n_training_steps, storing train_losses per epoch.\n",
        "\n",
        "Each training step (of n_training_steps) new training data (a batch of batch_size tokens) is generated and the model is trained and loss calculated on it. No separate \"testing\" data is needed, as the training data is unique each step. Memorisation of past training data by the model (if any) is minimally beneficial. For 5 digit addition there are 10 billion possible questions, and model training is on ~2 million questions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_config():\n",
        "  print(\"n_digits=\", n_digits, \"n_heads=\", n_heads, \"n_layers=\", n_layers, \"n_ctx=\", n_ctx)\n",
        "  print(\"seed=\", seed, \"long_equals=\", long_equals, \"n_training_steps=\", n_training_steps)"
      ],
      "metadata": {
        "id": "RMU5Sb40N4CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29fo9Gh5OhgM"
      },
      "outputs": [],
      "source": [
        "# Initialise the data generator\n",
        "ds = data_generator(batch_size, n_digits, seed)\n",
        "\n",
        "if train_model:\n",
        "  # Train the model\n",
        "  train_losses_list = []\n",
        "  per_token_train_losses_list = []\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(n_training_steps)):\n",
        "\n",
        "      tokens, base_adds, make_carry1s, sum9s, use_carry1s, use_sum9s = next(ds)\n",
        "      logits = model(tokens)\n",
        "\n",
        "      per_token_train_losses_raw, _ = logits_to_tokens_loss(logits, tokens)\n",
        "      per_token_train_losses = loss_fn(per_token_train_losses_raw)\n",
        "      per_token_train_losses_list.append(utils.to_numpy(per_token_train_losses))\n",
        "\n",
        "      train_loss = per_token_train_losses.mean()\n",
        "      train_loss.backward()\n",
        "      train_losses_list.append(train_loss.item())\n",
        "\n",
        "      calculate_ba_loss(tokens, per_token_train_losses_raw, base_adds, use_carry1s)\n",
        "      calculate_uc1_loss(tokens, per_token_train_losses_raw, use_carry1s, sum9s)\n",
        "      calculate_us9_loss(tokens, per_token_train_losses_raw, use_sum9s)\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "          print(epoch, train_loss.item())\n",
        "          unit_test_data_generator(tokens, use_carry1s, make_carry1s)\n",
        "          unit_test_nonoverlapping()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMLa9vDrM5GN"
      },
      "outputs": [],
      "source": [
        "# Even at the end of training, the loss can wobble between epochs, perhaps based on #of rare edge cases in the training data.\n",
        "# Use the average of last 5 training losses as the \"final accuracy\"\n",
        "print_config()\n",
        "print( \"Final training loss\", round((train_losses_list[-5]+train_losses_list[-4]+train_losses_list[-3]+train_losses_list[-2]+train_losses_list[-1])/5,6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE3AXNwW27wy"
      },
      "outputs": [],
      "source": [
        "if train_model:\n",
        "  # Save the model to file\n",
        "  torch.save(model.state_dict(), pth_location)\n",
        "else:\n",
        "  # Load the model from file\n",
        "  model.load_state_dict(torch.load(pth_location))\n",
        "  model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qZxJXGaS5e9"
      },
      "source": [
        "# Part 5: Training Loss Analysis - High Level Graphs\n",
        "\n",
        "This section analyses the training loss by graphing it at a high level.\n",
        "\n",
        "The loss curve for all digits show visible inflection points (bumps), but is too high level to help understand the algorithm.\n",
        "\n",
        "When this graph is decomposed into 'per digit' graphs, the interesting distinct 'per digit' curves appear, showing each digit is being refined semi-independently, with the model algorithm refining each digit separately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3RFX1_zS-jh"
      },
      "outputs": [],
      "source": [
        "epochs_to_graph=1200\n",
        "\n",
        "# Helper function to plot multiple lines\n",
        "def lines(raw_lines_list, x=None, mode='lines', labels=None, xaxis='Epoch', yaxis='Loss', title = '', log_y=False, hover=None, all_epochs=True, **kwargs):\n",
        "\n",
        "    lines_list = raw_lines_list if all_epochs==False else [row[:epochs_to_graph] for row in raw_lines_list]\n",
        "    log_suffix = '' if log_y==False else ' (Log)'\n",
        "    epoch_suffix = '' if all_epochs==False else ' (' + str(epochs_to_graph) + ' steps)'\n",
        "    full_title = title + log_suffix + epoch_suffix\n",
        "\n",
        "    if type(lines_list)==torch.Tensor:\n",
        "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
        "    if x is None:\n",
        "        x=np.arange(len(lines_list[0]))\n",
        "    if save_graph_to_file :\n",
        "      fig = go.Figure(layout={})\n",
        "      print(full_title)\n",
        "    else:\n",
        "      fig = go.Figure(layout={'title':full_title})\n",
        "\n",
        "    fig.update_xaxes(title=xaxis)\n",
        "    fig.update_yaxes(title=yaxis + log_suffix)\n",
        "    for c, line in enumerate(lines_list):\n",
        "        if type(line)==torch.Tensor:\n",
        "            line = utils.to_numpy(line)\n",
        "        if labels is not None:\n",
        "            label = labels[c]\n",
        "        else:\n",
        "            label = c\n",
        "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
        "    if log_y:\n",
        "        fig.update_layout(yaxis_type=\"log\")\n",
        "    if save_graph_to_file:\n",
        "        fig.update_layout(margin=dict(l=10, r=10, t=10, b=10),width=1200,height=300)\n",
        "\n",
        "    fig.show(bbox_inches=\"tight\")\n",
        "\n",
        "    if save_graph_to_file:\n",
        "        filename = full_title.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"&\", \"\").replace(\",\", \"\").replace(\"%\", \"\")   +'.pdf'\n",
        "        pio.write_image(fig, filename)\n",
        "\n",
        "\n",
        "if train_model:\n",
        "  title_suffix = ' Loss Curves for ' + str(n_digits) + ' digit addition'\n",
        "  per_token_losses = np.stack(per_token_train_losses_list, axis=0)\n",
        "\n",
        "  line(train_losses_list,\n",
        "      title=title_suffix)\n",
        "\n",
        "  all_epochs = True;\n",
        "  for i in range(2):\n",
        "    lines([per_token_losses[:, i] for i in range(1+n_digits)]+[train_losses_list],\n",
        "          labels = [f'digit {i}' for i in range(1+n_digits)]+['all_digits'],\n",
        "          title='Per digit'+title_suffix,\n",
        "          all_epochs=all_epochs)\n",
        "\n",
        "    #lines([per_token_losses[:, i] for i in range(1+n_digits)]+[train_losses_list],\n",
        "    #      labels = [f'digit {i}' for i in range(1+n_digits)]+['all_digits'],\n",
        "    #      title='Per digit'+title_suffix,\n",
        "    #      all_epochs=all_epochs,\n",
        "    #      log_y=True)\n",
        "\n",
        "    all_epochs = False;\n",
        "\n",
        "  for i in range(1+n_digits):\n",
        "    print('Final Loss for digit ' + str(i) + ' is ', per_token_losses[-1, i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD4jY88YTp_7"
      },
      "source": [
        "# Part 6: Training Loss Analysis - Single task (multiple digits)\n",
        "The previous section graphed across all the (BA, UC1, US9) tasks. This section graphs one of the addition sub-tasks at a time:\n",
        "- The all-digits graphs for one task (say BA) again show several inflection points but do not provide significant insights.\n",
        "- The per digit graphs for one task (say BA) again show distinct per digit curves. More useful but no significant insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gd2Jq88TEmB"
      },
      "outputs": [],
      "source": [
        "# Graph per digit series using \"normal\" and \"log\" scale\n",
        "def graph_perdigit(losslist, num_series, title_suffix, showlog, all_epochs=True):\n",
        "    lines([losslist[i] for i in range(num_series)],\n",
        "          labels = [f'digit {i}' for i in range(num_series)],\n",
        "          title='PerDigit '+title_suffix,\n",
        "          all_epochs=all_epochs)\n",
        "\n",
        "    if showlog:\n",
        "      lines([losslist[i] for i in range(num_series)],\n",
        "            labels = [f'digit {i}' for i in range(num_series)],\n",
        "            title='PerDigit '+title_suffix,\n",
        "            all_epochs=all_epochs,\n",
        "            log_y=True)\n",
        "\n",
        "    if all_epochs==True :\n",
        "      total_loss = 0\n",
        "      for i in range(num_series):\n",
        "        print('Final Loss for digit ' + str(i) + ' is', losslist[i][-1])\n",
        "        total_loss += losslist[i][-1]\n",
        "      print('Mean Loss is', total_loss/num_series)\n",
        "      print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL3ImZzfTwXs"
      },
      "source": [
        "## Base Add task graphs\n",
        "Graphs token loss vs step in use case where only BA (not UC1 or US9) is needed to get the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuMJ9tTbTw0x"
      },
      "outputs": [],
      "source": [
        "if train_model:\n",
        "  perc = (int)(100 * ba_total_cases / (ba_total_cases + uc1_total_cases + us9_total_cases))\n",
        "  print('BA Loss' + ' (' + str(ba_total_cases) + ' cases, ' + str(perc) + '%)')\n",
        "\n",
        "  the_title = 'BA Loss'\n",
        "\n",
        "  # For use cases where use_carry1s is false for all columns simultaneously, so BA can be used on all digits\n",
        "  line(ba_alldigits_loss, title='AllDigits '+the_title)\n",
        "\n",
        "  # For each digit independently\n",
        "  graph_perdigit(ba_perdigit_loss, n_digits, the_title, False, True)\n",
        "  graph_perdigit(ba_perdigit_loss, n_digits, the_title, False, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKA2vtSlT2S_"
      },
      "source": [
        "## Use Carry 1 (excluding Use Sum 9) task graphs\n",
        "Graphs token loss vs step where use_carry1s is used at least once over the digits columns (and Sum9 is not used at all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYIfLTU_T21V"
      },
      "outputs": [],
      "source": [
        "if train_model:\n",
        "  perc = (int)(100 * uc1_total_cases / (ba_total_cases + uc1_total_cases + us9_total_cases))\n",
        "  print( 'UC1 Loss (' + str(uc1_total_cases) + ' cases, ' + str(perc) + '%)' )\n",
        "  the_title = 'UC1 Loss'\n",
        "\n",
        "  lines([uc1_anydigits_loss],\n",
        "        labels = ['at least 1 digit'],\n",
        "        title='AllDigits '+the_title)\n",
        "\n",
        "  # For each digit independently\n",
        "  graph_perdigit(uc1_perdigit_loss, n_digits, the_title, False, True)\n",
        "  graph_perdigit(uc1_perdigit_loss, n_digits, the_title, False, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ec4VUbW4oH"
      },
      "source": [
        "## Use Sum 9 task graphs\n",
        "Graphs token loss vs step where US9 is used at least once over the digits columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05g7lzEUW5XU"
      },
      "outputs": [],
      "source": [
        "if train_model:\n",
        "  perc = (int)(100 * us9_total_cases / (ba_total_cases + uc1_total_cases + us9_total_cases))\n",
        "  print( 'US9 Loss (' + str(us9_total_cases) + ' cases, ' + str(perc) + '%)')\n",
        "  the_title = 'US9 Loss'\n",
        "\n",
        "  lines([us9_anydigits_loss],\n",
        "        labels = ['any digits'],\n",
        "        title='AllDigits '+the_title)\n",
        "\n",
        "  # For each digit independently\n",
        "  graph_perdigit(us9_perdigit_loss, n_digits, the_title, False, True)\n",
        "  graph_perdigit(us9_perdigit_loss, n_digits, the_title, False, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFZIq-7JW_hp"
      },
      "source": [
        "# Part 7: Training Loss Analysis - Single digit (multiple tasks)\n",
        "This section graphs show multiple tasks but only one digit. These graphs provide some insights, including:\n",
        "- The lowest value digit (D0) has a simple, steep loss curve\n",
        "- The middle value digits (D1, D2 and D3 in 5 digit addition) have similar loss curves\n",
        "- The highest value digit (D4 in 5 digit addition) has a different loss curve from the middle digits, suggesting the alogrithm for this digit differs from the middle digits.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQJ3sQEnXCsd"
      },
      "source": [
        "## Per digit BA & UC1 task graphs\n",
        "For each digit, graph the BasedAdd and UC1 tasks for curve comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heYi3DfwXDRG"
      },
      "outputs": [],
      "source": [
        "if train_model:\n",
        "  for whichdigit in range(n_digits):\n",
        "\n",
        "    the_title = 'Loss for BA & UC1 tasks for Digit ' + str(whichdigit)\n",
        "\n",
        "    lines([ba_perdigit_loss[whichdigit]]+[uc1_perdigit_loss[whichdigit]],\n",
        "          labels = ['BA']+['UC1'],\n",
        "          title=the_title)\n",
        "    lines([ba_perdigit_loss[whichdigit]]+[uc1_perdigit_loss[whichdigit]],\n",
        "          labels = ['BA']+['UC1'],\n",
        "          title=the_title,\n",
        "          log_y=True)\n",
        "    lines([ba_perdigit_loss[whichdigit]]+[uc1_perdigit_loss[whichdigit]],\n",
        "          labels = ['BA']+['UC1'],\n",
        "          title=the_title,\n",
        "          all_epochs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndYEU57BXKL0"
      },
      "source": [
        "## Per digit BA, UC1 & US9 task graphs\n",
        "For each digit, graph the BasedAdd, UC1 & US9 tasks for curve comparison.\n",
        "\n",
        "The high variability (noise) in the US9 curve comes from the rareness of this use case. There are ~4 examples in each training batch of 64. A single prediction error adds significant loss. The average US9 loss matches the BA and UC1 curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIbnD1iZXK-I"
      },
      "outputs": [],
      "source": [
        "if train_model:\n",
        "  for whichdigit in range(n_digits):\n",
        "\n",
        "    the_title = 'Loss for BA, UC1 & US9 Tasks for Digit ' + str(whichdigit)\n",
        "\n",
        "    lines([ba_perdigit_loss[whichdigit]]+[uc1_perdigit_loss[whichdigit]]+[us9_perdigit_loss[whichdigit]],\n",
        "          labels = ['BA']+['UC1']+['US9'],\n",
        "          title=the_title)\n",
        "    lines([ba_perdigit_loss[whichdigit]]+[uc1_perdigit_loss[whichdigit]]+[us9_perdigit_loss[whichdigit]],\n",
        "          labels = ['BA']+['UC1']+['US9'],\n",
        "          title=the_title,\n",
        "          log_y=True)\n",
        "    lines([ba_perdigit_loss[whichdigit]]+[uc1_perdigit_loss[whichdigit]]+[us9_perdigit_loss[whichdigit]],\n",
        "          labels = ['BA']+['UC1']+['US9'],\n",
        "          title=the_title,\n",
        "          all_epochs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K3YyDytmKHx"
      },
      "source": [
        "# Part 8: Questions Set Up\n",
        "\n",
        "Create sets of sample questions (by task) to ask the model to predict\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd24OefDcdm3"
      },
      "outputs": [],
      "source": [
        "# Insert a number into the question\n",
        "def insert_question_number(the_question, index, first_digit_index, the_digits, n):\n",
        "\n",
        "  last_digit_index = first_digit_index + the_digits - 1\n",
        "\n",
        "  for j in range(the_digits):\n",
        "    the_question[index, last_digit_index-j] = n % 10\n",
        "    n = n // 10\n",
        "\n",
        "\n",
        "# Create a single question\n",
        "def make_a_question(the_question, index, q1, q2):\n",
        "  a = q1 + q2\n",
        "\n",
        "  insert_question_number(the_question, index, 0, n_digits, q1)\n",
        "\n",
        "  the_question[index, n_digits] = PLUS_INDEX\n",
        "\n",
        "  insert_question_number( the_question, index, n_digits+1, n_digits, q2)\n",
        "\n",
        "  if long_equals == True:\n",
        "    the_question[index, 2*n_digits+1] = E_INDEX\n",
        "    the_question[index, 2*n_digits+2] = Q_INDEX\n",
        "    the_question[index, 2*n_digits+3] = U_INDEX\n",
        "    the_question[index, 2*n_digits+4] = A_INDEX\n",
        "    the_question[index, 2*n_digits+5] = L_INDEX\n",
        "    the_question[index, 2*n_digits+6] = S_INDEX\n",
        "    offset = 7\n",
        "  else:\n",
        "    the_question[index, 2*n_digits+1] = EQUALS_INDEX\n",
        "    offset = 2\n",
        "\n",
        "  insert_question_number(the_question, index, 2*n_digits + offset, n_digits+1, q1+q2)\n",
        "\n",
        "\n",
        "def prediction_to_string(max_indices):\n",
        "  answer = \"\".join([str(i) for i in utils.to_numpy(max_indices)[0]])\n",
        "  return answer;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0-lbl8eM4a7"
      },
      "outputs": [],
      "source": [
        "def make_ba_questions():\n",
        "  questions = torch.zeros((17, n_ctx)).to(torch.int64)\n",
        "  if n_digits >= 5 :\n",
        "    make_a_question( questions, 0, 12345, 33333)\n",
        "    make_a_question( questions, 1, 33333, 12345)\n",
        "    make_a_question( questions, 2, 45762, 33113)\n",
        "    make_a_question( questions, 3, 888, 11111)\n",
        "    make_a_question( questions, 4, 2362, 23123)\n",
        "    make_a_question( questions, 5, 15, 81)\n",
        "    make_a_question( questions, 6, 1000, 4440)\n",
        "    make_a_question( questions, 7, 4440, 1000)\n",
        "    make_a_question( questions, 8, 24033, 25133)\n",
        "    make_a_question( questions, 9, 23533, 21133)\n",
        "    make_a_question( questions, 10, 32500, 1)\n",
        "    make_a_question( questions, 11, 31500, 1111)\n",
        "    make_a_question( questions, 12, 5500, 12323)\n",
        "    make_a_question( questions, 13, 4500, 2209)\n",
        "    make_a_question( questions, 14, 10990, 44000)\n",
        "    make_a_question( questions, 15, 60000, 30000)\n",
        "    make_a_question( questions, 16, 10000, 20000)\n",
        "  return questions\n",
        "\n",
        "\n",
        "def make_uc1_questions():\n",
        "  n = 24 if n_digits >= 10 else 19\n",
        "  questions = torch.zeros((n, n_ctx)).to(torch.int64)\n",
        "  if n_digits >= 5 :\n",
        "    make_a_question( questions, 0, 15, 45)\n",
        "    make_a_question( questions, 1, 25, 55)\n",
        "    make_a_question( questions, 2, 35, 59)\n",
        "    make_a_question( questions, 3, 40035, 40049)\n",
        "    make_a_question( questions, 4, 5025, 5059)\n",
        "    make_a_question( questions, 5, 15, 65)\n",
        "    make_a_question( questions, 6, 44000, 46000)\n",
        "    make_a_question( questions, 7, 70000, 40000)\n",
        "    make_a_question( questions, 8, 15000, 25000)\n",
        "    make_a_question( questions, 9, 35000, 35000)\n",
        "    make_a_question( questions, 10, 45000, 85000)\n",
        "    make_a_question( questions, 11, 67000, 85000)\n",
        "    make_a_question( questions, 12, 99000, 76000)\n",
        "    make_a_question( questions, 13, 1500, 4500)\n",
        "    make_a_question( questions, 14, 2500, 5500)\n",
        "    make_a_question( questions, 15, 3500, 5900)\n",
        "    make_a_question( questions, 16, 15020, 45091)\n",
        "    make_a_question( questions, 17, 25002, 55019)\n",
        "    make_a_question( questions, 18, 35002, 59019)\n",
        "  if n_digits >= 10 :\n",
        "    make_a_question( questions, 19, 25000000, 55000000)\n",
        "    make_a_question( questions, 20, 35000000, 59000000)\n",
        "    make_a_question( questions, 21, 150200000, 450910000)\n",
        "    make_a_question( questions, 22, 250020000, 550190000)\n",
        "    make_a_question( questions, 23, 350020000, 590190000)\n",
        "  return questions\n",
        "\n",
        "\n",
        "def make_simple_us9_questions():\n",
        "  questions = torch.zeros((18, n_ctx)).to(torch.int64)\n",
        "  if n_digits >= 5 :\n",
        "    make_a_question( questions, 0, 55, 45)\n",
        "    make_a_question( questions, 1, 45, 55)\n",
        "    make_a_question( questions, 2, 45, 59)\n",
        "    make_a_question( questions, 3, 35, 69)\n",
        "    make_a_question( questions, 4, 25, 79)\n",
        "    make_a_question( questions, 5, 15, 85)\n",
        "    make_a_question( questions, 6, 15, 88)\n",
        "    make_a_question( questions, 7, 15508, 14500)\n",
        "    make_a_question( questions, 8, 14508, 15500)\n",
        "    make_a_question( questions, 9, 24533, 25933)\n",
        "    make_a_question( questions, 10, 23533, 26933)\n",
        "    make_a_question( questions, 11, 32500, 7900)\n",
        "    make_a_question( questions, 12, 31500, 8500)\n",
        "    make_a_question( questions, 13, 550, 450)\n",
        "    make_a_question( questions, 14, 450, 550)\n",
        "    make_a_question( questions, 15, 10880, 41127)\n",
        "    make_a_question( questions, 16, 41127, 10880)\n",
        "    make_a_question( questions, 17, 12386, 82623)\n",
        "  return questions\n",
        "\n",
        "\n",
        "def make_cascade_us9_questions(clean = True):\n",
        "  questions = torch.zeros((29, n_ctx)).to(torch.int64)\n",
        "  if n_digits >= 5 :\n",
        "    # These are two level UseSum9 cascades\n",
        "    make_a_question( questions, 0, 555, 445)\n",
        "    make_a_question( questions, 1, 3340, 6660)\n",
        "    make_a_question( questions, 2, 8880, 1120)\n",
        "    make_a_question( questions, 3, 1120, 8880)\n",
        "    make_a_question( questions, 4, 123, 877)\n",
        "    make_a_question( questions, 5, 877, 123)\n",
        "    make_a_question( questions, 6, 321, 679)\n",
        "    make_a_question( questions, 7, 679, 321)\n",
        "    make_a_question( questions, 8, 1283, 88786)\n",
        "    # These are three level UseSum9 cascades\n",
        "    make_a_question( questions, 9, 5555, 4445)\n",
        "    make_a_question( questions, 10, 55550, 44450)\n",
        "    make_a_question( questions, 11, 334, 666)\n",
        "    make_a_question( questions, 12, 3340, 6660)\n",
        "    make_a_question( questions, 13, 33400, 66600)\n",
        "    make_a_question( questions, 14, 888, 112)\n",
        "    make_a_question( questions, 15, 8880, 1120)\n",
        "    make_a_question( questions, 16, 88800, 11200)\n",
        "    make_a_question( questions, 17, 1234, 8766)\n",
        "    make_a_question( questions, 18, 4321, 5679)\n",
        "    # These are four level UseSum9 cascades\n",
        "    make_a_question( questions, 19, 44445, 55555)\n",
        "    make_a_question( questions, 20, 33334, 66666)\n",
        "    make_a_question( questions, 21, 88888, 11112)\n",
        "    make_a_question( questions, 22, 12345, 87655)\n",
        "    make_a_question( questions, 23, 54321, 45679)\n",
        "    make_a_question( questions, 24, 45545, 54455)\n",
        "    make_a_question( questions, 25, 36634, 63366)\n",
        "    make_a_question( questions, 26, 81818, 18182)\n",
        "    make_a_question( questions, 27, 87345, 12655)\n",
        "    make_a_question( questions, 28, 55379, 44621)\n",
        "  return questions\n",
        "\n",
        "\n",
        "# These questions focus mainly on 1 digit at a time\n",
        "# (We're assuming that the 0 + 0 digit additions are trivial bigrams)\n",
        "def make_step_questions():\n",
        "  questions = torch.zeros((26, n_ctx)).to(torch.int64)\n",
        "  if n_digits >= 5 :\n",
        "      make_a_question( questions, 0, 1, 0)\n",
        "      make_a_question( questions, 1, 4, 3)\n",
        "      make_a_question( questions, 2, 5, 5)\n",
        "      make_a_question( questions, 3, 8, 1)\n",
        "      make_a_question( questions, 4, 40, 30)\n",
        "      make_a_question( questions, 5, 44, 46)\n",
        "      make_a_question( questions, 6, 400, 300)\n",
        "      make_a_question( questions, 7, 440, 460)\n",
        "      make_a_question( questions, 8, 800, 100)\n",
        "      make_a_question( questions, 9, 270, 470)\n",
        "      make_a_question( questions, 10, 600, 300)\n",
        "      make_a_question( questions, 11, 4000, 3000)\n",
        "      make_a_question( questions, 12, 4400, 4600)\n",
        "      make_a_question( questions, 13, 6000, 3000)\n",
        "      make_a_question( questions, 14, 7000, 4000)\n",
        "      make_a_question( questions, 15, 40000, 30000)\n",
        "      make_a_question( questions, 16, 44000, 46000)\n",
        "      make_a_question( questions, 17, 60000, 30000)\n",
        "      make_a_question( questions, 17, 70000, 40000)\n",
        "      make_a_question( questions, 19, 10000, 20000)\n",
        "      make_a_question( questions, 20, 15000, 25000)\n",
        "      make_a_question( questions, 21, 35000, 35000)\n",
        "      make_a_question( questions, 22, 45000, 85000)\n",
        "      make_a_question( questions, 23, 67000, 85000)\n",
        "      make_a_question( questions, 24, 99000, 76000)\n",
        "      make_a_question( questions, 25, 76000, 99000)\n",
        "  return questions\n",
        "\n",
        "\n",
        "# Returns ~100 manually-chosen questions\n",
        "def all_manual_questions():\n",
        "  q1 = make_ba_questions()\n",
        "  q2 = make_uc1_questions()\n",
        "  q3 = make_simple_us9_questions()\n",
        "  q4 = make_cascade_us9_questions()\n",
        "  q5 = make_step_questions()\n",
        "\n",
        "  all_manual = torch.vstack((q1, q2, q3, q4, q5))\n",
        "\n",
        "  return all_manual\n",
        "\n",
        "\n",
        "# Returns 64 random and ~100 manually-chosen questions\n",
        "def make_varied_questions():\n",
        "  random_questions, _, _, _, _, _ = next(ds)\n",
        "\n",
        "  all_manual = all_manual_questions()\n",
        "\n",
        "  all_tokens = torch.vstack((random_questions.cuda(), all_manual.cuda()))\n",
        "\n",
        "  return all_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcWTa9wUZmLy"
      },
      "outputs": [],
      "source": [
        "num_questions = 0;\n",
        "correct_answers = 0;\n",
        "verbose = True\n",
        "total_mean_loss = 0.0\n",
        "\n",
        "\n",
        "# Clear the question summary results\n",
        "def clear_questions_results(title):\n",
        "  global num_questions\n",
        "  global correct_answers\n",
        "  global verbose\n",
        "  global total_mean_loss\n",
        "\n",
        "  num_questions = 0\n",
        "  correct_answers = 0\n",
        "  total_mean_loss = 0\n",
        "\n",
        "  if verbose:\n",
        "    print(title)\n",
        "\n",
        "\n",
        "# Ask model to predict answer for each question & collect results\n",
        "def do_questions(questions):\n",
        "    global num_questions\n",
        "    global correct_answers\n",
        "    global verbose\n",
        "    global total_mean_loss\n",
        "\n",
        "    num_questions = questions.shape[0]\n",
        "    for question_num in range(num_questions):\n",
        "      q = questions[question_num]\n",
        "\n",
        "      # Run with no hook\n",
        "      the_logits = model(q.cuda())\n",
        "\n",
        "      q_2d = q.unsqueeze(0)\n",
        "      losses_raw, max_indices = logits_to_tokens_loss(the_logits, q_2d.cuda())\n",
        "      losses = loss_fn(losses_raw)\n",
        "      mean_loss = utils.to_numpy(losses.mean())\n",
        "      total_mean_loss = total_mean_loss + mean_loss\n",
        "\n",
        "      model_answer_str = prediction_to_string(max_indices)\n",
        "      model_answer_num = int(model_answer_str)\n",
        "\n",
        "      i = n_digits*2 + 7 if long_equals == True else n_digits*2 + 2\n",
        "\n",
        "      a = 0\n",
        "      # 5 digit addition yields a 6 digit answer. Hence n_digits+1\n",
        "      for j in range(n_digits+1):\n",
        "        a = a * 10 + q[i+j]\n",
        "\n",
        "      correct = (model_answer_num == a)\n",
        "      if correct :\n",
        "        correct_answers += 1\n",
        "\n",
        "      if verbose:\n",
        "        print(tokens_to_string(q), \"ModelAnswer:\", model_answer_str, \"Correct:\", correct, \"Loss:\", mean_loss )\n",
        "\n",
        "\n",
        "# Print the question summary results\n",
        "def print_questions_results(prefix):\n",
        "  global num_questions\n",
        "  global correct_answers\n",
        "  global verbose\n",
        "  global total_mean_loss\n",
        "\n",
        "  print(prefix, num_questions, \"questions.\", correct_answers, \"correct. % Correct:\", 100*correct_answers/num_questions, \"Mean loss:\", total_mean_loss/num_questions)\n",
        "  if verbose:\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdreQ2XwTI0U"
      },
      "outputs": [],
      "source": [
        "# Build a test batch of 64 random and ~100 manually-chosen questions\n",
        "varied_questions = make_varied_questions();\n",
        "\n",
        "# Run the sample batch, gather the cache\n",
        "model.reset_hooks()\n",
        "model.set_use_attn_result(True)\n",
        "sample_logits, sample_cache = model.run_with_cache(varied_questions.cuda())\n",
        "print(sample_cache) # Gives names of datasets in the cache\n",
        "sample_losses_raw, sample_max_indices = logits_to_tokens_loss(sample_logits, varied_questions.cuda())\n",
        "sample_loss_mean = utils.to_numpy(loss_fn(sample_losses_raw).mean())\n",
        "print(\"Sample Mean Loss\", sample_loss_mean) # Loss < 0.04 is good\n",
        "\n",
        "# attn.hook_z is a key output of the attention heads, combining pattern and value\n",
        "hook_l0_z_name = 'blocks.0.attn.hook_z'\n",
        "sample_attn_z = sample_cache[hook_l0_z_name]\n",
        "print(\"Sample\", hook_l0_z_name, sample_attn_z.shape) # gives [64, 18, 3, 170] = batch_size, num_tokens, n_heads, d_model\n",
        "mean_attn_z = torch.mean(sample_attn_z, dim=0, keepdim=True)\n",
        "print(\"Mean\", hook_l0_z_name, mean_attn_z.shape) # gives [1, 18, 3, 170] = 1, num_tokens, n_heads, d_model\n",
        "hook_l1_z_name = 'blocks.1.attn.hook_z'\n",
        "\n",
        "hook_l0_resid_post_name = 'blocks.0.hook_resid_post'\n",
        "sample_resid_post = sample_cache[hook_l0_resid_post_name]\n",
        "print(\"Sample\", hook_l0_resid_post_name, sample_resid_post.shape) # gives [64, 18, 510] = batch_size, num_tokens, d_model\n",
        "mean_resid_post = torch.mean(sample_resid_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", hook_l0_resid_post_name, mean_resid_post.shape) # gives [1, 18, 510] = 1, num_tokens, d_model\n",
        "hook_l1_resid_post_name = 'blocks.1.hook_resid_post'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQmGSsTlYYV3"
      },
      "source": [
        "# Part 9: Prediction Analysis - By Use Case\n",
        "This section sets up BA, UC1 and US9 test cases that will be re-used in later experiments to show the impact of ablating heads or steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQQDT67fz8zo"
      },
      "outputs": [],
      "source": [
        "print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJsOxOlCMj-5"
      },
      "outputs": [],
      "source": [
        "def do_ba_questions():\n",
        "  clear_questions_results(\"Simple BaseAdd cases\")\n",
        "  questions = make_ba_questions()\n",
        "  do_questions(questions)\n",
        "  print_questions_results(\"BaseAdd:\")\n",
        "\n",
        "def do_uc1_questions():\n",
        "  clear_questions_results(\"These are Use Carry 1 (UC1) examples (not UseSum9 examples)\")\n",
        "  questions = make_uc1_questions()\n",
        "  do_questions(questions)\n",
        "  print_questions_results(\"UseCarry1:\")\n",
        "\n",
        "def do_simple_us9_questions():\n",
        "  clear_questions_results(\"These are simple (one level) UseSum9 exampless\")\n",
        "  questions = make_simple_us9_questions()\n",
        "  do_questions(questions)\n",
        "  print_questions_results(\"SimpleUS9\")\n",
        "\n",
        "def do_cascade_us9_questions():\n",
        "  clear_questions_results(\"These are UseSum9 two, three and four level cascades\")\n",
        "  questions = make_cascade_us9_questions()\n",
        "  do_questions(questions)\n",
        "  print_questions_results(\"CascadeUS9\")\n",
        "\n",
        "def do_step_questions():\n",
        "  clear_questions_results(\"These questions focus on different steps\")\n",
        "  questions = make_step_questions()\n",
        "  do_questions(questions)\n",
        "  print_questions_results(\"Steps\")\n",
        "\n",
        "verbose = False\n",
        "\n",
        "do_ba_questions()\n",
        "do_uc1_questions()\n",
        "do_simple_us9_questions()\n",
        "do_cascade_us9_questions()\n",
        "do_step_questions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgybEeqynJqs"
      },
      "source": [
        "# Part 10: What prediction steps does the model does actually use?\n",
        "\n",
        "Here we ablate all heads in each step and see if loss increases to show which steps (if any) are **not** used by the algorithm. These steps can be excluded from further analysis.\n",
        "\n",
        "This section overrides (ablates) the model memory (residual stream) at each step. It confirms that for:\n",
        "- n_digits = 5, n_layers = 1 : the addition algorithm does **not** use any data generated in steps 0 to 10 inclusive. In these steps the model has **not** yet seen the full question and every digit in the question is independent of every other digit, making accurate answer prediction infeasible. The model also does not use the last (17th) step. Therefore, the addition is started and completed in 6 steps (11 to 16)\n",
        "- n_digits = 5, n_layers = 2 : the addition algorithm does **not** use any data generated in steps 0 to 7 inclusive. The model also does not use the last (17th) step. Therefore, the addition is started and completed in 9 steps (8 to 16).\n",
        "- n_digits = 10, n_layers = 2 :  TBA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYKge2BopYfu"
      },
      "outputs": [],
      "source": [
        "# Experiment 3: Apply a hook to override residual stream (blocks.0.hook_resid_post) in each step in turn and see impact on loss\n",
        "\n",
        "exp3_step = 0  # zero-based step to ablate\n",
        "exp3_threshold = 0.02\n",
        "\n",
        "def exp3_hook(value, hook):\n",
        "  #print( \"In hook\", hook_l0_resid_post_name, exp3_ablate, exp3_step, value.shape) # Get [64, 18, 510] = batch_size, num_tokens, d_model\n",
        "\n",
        "  # Copy the mean resid post values in step N to all the batch questions\n",
        "  value[:,exp3_step,:] = mean_resid_post[0,exp3_step,:].clone()\n",
        "\n",
        "\n",
        "if n_digits >= 5 :\n",
        "  exp3_fwd_hooks = [(hook_l0_resid_post_name, exp3_hook)]\n",
        "\n",
        "  for exp3_step in range(n_ctx):\n",
        "    model.reset_hooks()\n",
        "    exp3_logits = model.run_with_hooks(varied_questions.cuda(), return_type=\"logits\", fwd_hooks=exp3_fwd_hooks)\n",
        "    exp3_losses_raw, resid_post_max_indices = logits_to_tokens_loss(exp3_logits, varied_questions.cuda())\n",
        "    exp3_loss_mean = utils.to_numpy(loss_fn(exp3_losses_raw).mean())\n",
        "\n",
        "    loss_description = \"Good\" if exp3_loss_mean < exp3_threshold else \"BAD\"\n",
        "    print(\"Sample Loss\", hook_l0_resid_post_name, \"Loss\", exp3_loss_mean, \"Step\", exp3_step, \"Ablate\", loss_description)\n",
        "\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 11: Set Up Run Framework\n",
        "\n",
        "Create way to get model to predict sample question answers and analysis/show results"
      ],
      "metadata": {
        "id": "t649kL0GNSZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep9LxDFw6rFw"
      },
      "outputs": [],
      "source": [
        "# Compare each digit in the answer. Returns a yyNNy pattern where y means the digits match and N means a failure\n",
        "def get_digit_accuracy_pattern(a_int, answer_str):\n",
        "  a_str = str(a_int.cpu().numpy()).zfill(n_digits+1)\n",
        "  match_str = \"\"\n",
        "  for i in range(n_digits+1):\n",
        "      if answer_str[i] == a_str[i]:\n",
        "          match_str += \"y\"  # Matching digit\n",
        "      else:\n",
        "          match_str += \"N\"  # Non-matching digit\n",
        "  return match_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSuux-EDQx2f"
      },
      "outputs": [],
      "source": [
        "# Build up a list of success/failure patterns found, and the frequency of each pattern\n",
        "exp2_step_counts = {}\n",
        "\n",
        "def clear_step_counts():\n",
        "  global exp2_step_counts\n",
        "\n",
        "  exp2_step_counts = {}\n",
        "\n",
        "def add_step_count(step_key):\n",
        "  global exp2_step_counts\n",
        "\n",
        "  if step_key in exp2_step_counts:\n",
        "    # If the key is already in the dictionary, increment its count\n",
        "    exp2_step_counts[step_key] += 1\n",
        "  else:\n",
        "    # If the key is not in the dictionary, add it with a count of 1\n",
        "    exp2_step_counts[step_key] = 1\n",
        "\n",
        "def print_step_counts():\n",
        "  global exp2_step_counts\n",
        "\n",
        "  if len(exp2_step_counts) > 0 :\n",
        "    print(\"  Step failures:\", exp2_step_counts)\n",
        "\n",
        "def get_step_counts_total():\n",
        "  global exp2_step_counts\n",
        "\n",
        "  if len(exp2_step_counts) == 0:\n",
        "    return 0\n",
        "\n",
        "  total_sum = 0\n",
        "  for key, value in exp2_step_counts.items():\n",
        "      if isinstance(value, int):\n",
        "          total_sum += value\n",
        "  return total_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_52Sg8A0xMdl"
      },
      "outputs": [],
      "source": [
        "# Analyse the question and return the use case as BA, MC, SimpleUS9 or CascadeUS9\n",
        "def get_question_case(q):\n",
        "  qa = utils.to_numpy(q)\n",
        "  qn = qa[:2*n_digits+2]\n",
        "\n",
        "  # Locate the MC and MS digits (if any)\n",
        "  mc = torch.zeros( n_digits).to(torch.int64)\n",
        "  ms = torch.zeros( n_digits).to(torch.int64)\n",
        "  for dn in range(n_digits):\n",
        "    if qn[dn] + qn[dn + n_digits + 1] == 9:\n",
        "      ms[n_digits-1-dn] = 1\n",
        "    if qn[dn] + qn[dn + n_digits +1] > 9:\n",
        "      mc[n_digits-1-dn] = 1\n",
        "\n",
        "  # Calculate the use case of a question\n",
        "  if torch.sum(mc) == 0:\n",
        "    return \"BA\"\n",
        "\n",
        "  if torch.sum(ms) == 0:\n",
        "    return \"MC1\"\n",
        "\n",
        "  for dn in range(n_digits):\n",
        "    if dn < n_digits-2 and mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1:\n",
        "      return \"CascadeUS9\"\n",
        "\n",
        "  for dn in range(n_digits):\n",
        "    if dn < n_digits-1 and mc[dn] == 1 and ms[dn+1] == 1:\n",
        "      return \"SimpleUS9\"\n",
        "\n",
        "  return \"MC1\"\n",
        "\n",
        "\n",
        "# Test that the above code works as expected\n",
        "def unit_test_get_question_case_core(correct_case, questions):\n",
        "  for i in range(questions.shape[0]):\n",
        "    question_case = get_question_case(questions[i])\n",
        "    if question_case != correct_case:\n",
        "      print( \"Case mismatch:\", correct_case, question_case, questions[i])\n",
        "\n",
        "def unit_test_get_question_case():\n",
        "  unit_test_get_question_case_core( \"BA\", make_ba_questions())\n",
        "  unit_test_get_question_case_core( \"MC1\", make_uc1_questions())\n",
        "  unit_test_get_question_case_core( \"SimpleUS9\", make_simple_us9_questions())\n",
        "  unit_test_get_question_case_core( \"CascadeUS9\", make_cascade_us9_questions())\n",
        "\n",
        "unit_test_get_question_case()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPdLICpnDkNX"
      },
      "outputs": [],
      "source": [
        "# Build up a count of cases found\n",
        "exp2_case_counts = {}\n",
        "\n",
        "def clear_case_counts():\n",
        "  global exp2_case_counts\n",
        "\n",
        "  exp2_case_counts = {}\n",
        "\n",
        "def add_case_count(case_key):\n",
        "  global exp2_case_counts\n",
        "\n",
        "  if case_key in exp2_case_counts:\n",
        "    # If the key is already in the dictionary, increment its count\n",
        "    exp2_case_counts[case_key] += 1\n",
        "  else:\n",
        "    # If the key is not in the dictionary, add it with a count of 1\n",
        "    exp2_case_counts[case_key] = 1\n",
        "\n",
        "def print_case_counts():\n",
        "  global exp2_case_counts\n",
        "\n",
        "  if len(exp2_case_counts) > 0:\n",
        "    print( \"  Case failures:\", exp2_case_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHqncBH0LEYJ"
      },
      "outputs": [],
      "source": [
        "def do_experiment_question(questions, the_hook, the_threshold):\n",
        "\n",
        "  num_questions = questions.shape[0]\n",
        "  for question_num in range(num_questions):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    model.reset_hooks()\n",
        "    exp_logits = model.run_with_hooks(q.cuda(), return_type=\"logits\", fwd_hooks=the_hook)\n",
        "\n",
        "    q_2d = q.unsqueeze(0)\n",
        "    exp_losses_raw, exp_max_indices = logits_to_tokens_loss(exp_logits, q_2d.cuda())\n",
        "    exp_loss_mean = utils.to_numpy(loss_fn(exp_losses_raw).mean())\n",
        "    exp_answer_str = prediction_to_string(exp_max_indices)\n",
        "\n",
        "    i = 17 if long_equals == True else 12\n",
        "    a = q[i+0] * 100000 + q[i+1] * 10000 + q[i+2] * 1000 + q[i+3] * 100 + q[i+4] * 10 + q[i+5] * 1;\n",
        "\n",
        "    # Only show the question if the loss exceeds the threshold (because of the ablated step)\n",
        "    if exp_loss_mean > the_threshold:\n",
        "      match_str = get_digit_accuracy_pattern( a, exp_answer_str )\n",
        "      # Only count the question if the model got the question wrong\n",
        "      if 'N' in match_str:\n",
        "        the_case = get_question_case(q)\n",
        "        if verbose:\n",
        "          print(tokens_to_string(q), \"ModelAnswer:\", exp_answer_str, \"Matches:\", match_str, \"Loss:\", exp_loss_mean, \"Case:\", the_case )\n",
        "        else:\n",
        "          add_case_count(the_case)\n",
        "          add_step_count(match_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Jz6e_uEiu4"
      },
      "source": [
        "# Part 11A: Impact on digit accuracy and task accuracy of ablating all heads in a step.\n",
        "Here we ablate all heads in each step and see if loss increases for specific **digits** and **tasks** shows which steps are associated with calculating which digits and tasks.\n",
        "\n",
        "Notes:\n",
        "* The paper claims that an answer digit (such as A3) is calculated one token before it is revealed. Visual inspection of the failures helps determine which answer digit is dependent on each step.\n",
        "* With n_layers = 1: There are examples, when ablating step 16 that model gets two digits wrong (e.g. Question: 35000 + 35000 = 70000 ModelAnswer: 060009) but the higher digit error is irrelevant as the model has predicted and revealed the higher digit a few tokens ago. Only errors in digits that have not yet been revealed at the step being tested are significant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDaXJg-GEqPy"
      },
      "outputs": [],
      "source": [
        "# Experiment 2 - Ablate all heads in each step to see what question digits and tasks then fail.\n",
        "\n",
        "exp2_step = 0 # zero-based step to ablate\n",
        "exp2_threshold = 0.1\n",
        "exp2_questions = make_varied_questions()\n",
        "\n",
        "verbose = False;\n",
        "\n",
        "def exp2_hook(value, hook):\n",
        "  # Copy the mean resid post values in step N to all the batch questions\n",
        "  #value[:,exp2_step,:] = mean_resid_post[0,exp2_step,:].clone()\n",
        "  value[:,exp2_step,:] = 0\n",
        "\n",
        "\n",
        "if n_digits >= 5 :\n",
        "  exp2_fwd_hooks = [(hook_l0_resid_post_name, exp2_hook)]\n",
        "\n",
        "  print_config()\n",
        "  print(\"num_questions=\", exp2_questions.shape[0])\n",
        "  print()\n",
        "\n",
        "  for exp2_step in range(n_ctx):\n",
        "    clear_case_counts()\n",
        "    clear_step_counts()\n",
        "\n",
        "    do_experiment_question(exp2_questions, exp2_fwd_hooks, exp2_threshold)\n",
        "\n",
        "    # Skip steps with no errors\n",
        "    if (not verbose) and ((len(exp2_case_counts) > 0) or (len(exp2_step_counts) > 0)):\n",
        "      print( \"Ablating all heads in step\", exp2_step)\n",
        "      print_case_counts()\n",
        "      print_step_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1NqqLoH1_1C"
      },
      "source": [
        "# Part 11B: Impact on digit accuracy and task accuracy of ablating some heads in a step.\n",
        "Ablating each head in each layer in each step and seeing if the loss increases shows which head+layer+step are / aren't used by the algorithm.\n",
        "\n",
        "In 2 layer model, most second layer head+step are not used.\n",
        "The results are diagrammed in StaircaseA3L2_Part1.drawio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRDFKBHM2K_p"
      },
      "outputs": [],
      "source": [
        "exp7_step = 0 # zero-based step to ablate. 0 to say 17\n",
        "exp7_layer = 0 # zero-based layer to ablate. 0 to 1\n",
        "exp7_head = 0 # zero-based head to ablate. 0 to 2\n",
        "exp7_threshold = 0.1\n",
        "exp7_questions = varied_questions\n",
        "exp7_l0_failures = [[0 for _ in range(n_heads)] for _ in range(n_ctx)]\n",
        "exp7_l1_failures = [[0 for _ in range(n_heads)] for _ in range(n_ctx)]\n",
        "verbose = False;\n",
        "\n",
        "\n",
        "def exp7_reset_failures():\n",
        "  global exp7_l0_failures\n",
        "  global exp7_l1_failures\n",
        "\n",
        "  exp7_l0_failures = [[0 for _ in range(n_heads)] for _ in range(n_ctx)]\n",
        "  exp7_l1_failures = [[0 for _ in range(n_heads)] for _ in range(n_ctx)]\n",
        "\n",
        "\n",
        "def exp7_hook(value, hook):\n",
        "  global exp7_step\n",
        "  global exp7_head\n",
        "\n",
        "  # print( \"In hook\", hook_l0_z_name, value.shape) # Get [1, 18, 3, 170] = ???, n_ctx, n_heads, d_head\n",
        "\n",
        "  # Copy the mean resid post values in step N to all the batch questions\n",
        "  value[:,exp7_step,exp7_head,:] = mean_attn_z[:,exp7_step,exp7_head,:].clone() # Mean ablate\n",
        "\n",
        "\n",
        "def exp7_perform_core():\n",
        "  global exp7_step\n",
        "  global exp7_head\n",
        "  global exp7_threshold\n",
        "  global exp7_questions\n",
        "  global exp7_l0_failures\n",
        "  global exp7_l1_failures\n",
        "\n",
        "  clear_case_counts()\n",
        "  clear_step_counts()\n",
        "\n",
        "  the_hook = [(hook_l0_z_name, exp7_hook)] if exp7_layer == 0 else [(hook_l1_z_name, exp7_hook)]\n",
        "  do_experiment_question(exp7_questions, the_hook, exp7_threshold)\n",
        "\n",
        "  if exp7_layer==0:\n",
        "    exp7_l0_failures[exp7_step][exp7_head] = exp7_l0_failures[exp7_step][exp7_head] + get_step_counts_total()\n",
        "  else:\n",
        "    exp7_l1_failures[exp7_step][exp7_head] = exp7_l1_failures[exp7_step][exp7_head] + get_step_counts_total()\n",
        "\n",
        "  if verbose:\n",
        "    failures = exp7_l0_failures[exp7_step][exp7_head] if exp7_layer==0 else exp7_l1_failures[exp7_step][exp7_head]\n",
        "    print( \"  Step\", exp7_step, \"  layer\", exp7_layer, \"head\", exp7_head, \"#failures\", failures )\n",
        "    print_case_counts()\n",
        "    print_step_counts()\n",
        "\n",
        "\n",
        "def exp7_perform(title):\n",
        "  global exp7_step\n",
        "  global exp7_layer\n",
        "  global exp7_head\n",
        "  global exp7_questions\n",
        "  global exp7_l0_failures\n",
        "  global exp7_l1_failures\n",
        "\n",
        "  if n_digits >= 5 :\n",
        "    if n_layers == 2 :\n",
        "\n",
        "      exp7_reset_failures()\n",
        "\n",
        "      for exp7_step in range(n_ctx):\n",
        "        for exp7_layer in range(n_layers):\n",
        "          for exp7_head in range(n_heads):\n",
        "\n",
        "            exp7_perform_core()\n",
        "\n",
        "      print(title, exp7_questions.shape[0])\n",
        "      for exp7_step in range(n_ctx):\n",
        "        if sum(exp7_l0_failures[exp7_step]) > 0 or sum(exp7_l1_failures[exp7_step]) > 0 :\n",
        "          print(\"  Step\", exp7_step, \"  L0Hn #failures\", exp7_l0_failures[exp7_step], \"L1Hn #failures\", exp7_l1_failures[exp7_step])\n",
        "      print()\n",
        "\n",
        "\n",
        "exp7_questions = varied_questions\n",
        "exp7_perform(\"# varied questions:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-gpCl5ZmCUP"
      },
      "source": [
        "## Part 11C - BA Analysis\n",
        "For n_digits = 5, n_layers = 2:\n",
        "- S0 to S11 and S17 are not relevant\n",
        "- L1 is not relevant\n",
        "- L0H1 is not relevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ3jvCPamCyV"
      },
      "outputs": [],
      "source": [
        "exp7_questions = make_ba_questions()\n",
        "exp7_perform(\"# BA questions:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r895EpE2mwVd"
      },
      "source": [
        "## Part 11D - UC1 Analysis\n",
        "For n_digits = 5, n_layers = 2:\n",
        "- S0 to S11 and S17 are not relevant\n",
        "- L1 is not relevant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epyw9cxzmwpj"
      },
      "outputs": [],
      "source": [
        "exp7_questions = make_uc1_questions()\n",
        "exp7_perform(\"# UC1 questions:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5dxVqxamwzy"
      },
      "source": [
        "## Part 11E - Simple US9 Analysis\n",
        "For n_digits = 5, n_layers = 2 for SimpleUS9:\n",
        "- S0 to S7 and S17 are not relevant\n",
        "- L1 is not relevant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "477j1tsRmw9S"
      },
      "outputs": [],
      "source": [
        "exp7_questions = make_simple_us9_questions()\n",
        "exp7_perform(\"#SimpleUS9 questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hryy5h-Eo4ir"
      },
      "source": [
        "## Part 11F - Cascade US9 Analysis\n",
        "For n_digits = 5, n_layers = 2 for Cascade US9:\n",
        "- S0 to S7 and S17 are not relevant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bLBUxMfo3w0"
      },
      "outputs": [],
      "source": [
        "exp7_questions = make_cascade_us9_questions()\n",
        "exp7_perform(\"#CascadeUS9 questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0w7xYmImZs"
      },
      "source": [
        "# Part 12: Prediction Analysis - Shape\n",
        "\n",
        "The \"Prediction\" sections analyses the model after it has been trained, by looking at how it predicts answers to questions. This section shows the shape of the data available. No insights derived.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRpeURSFZSCX"
      },
      "source": [
        "Get some new tokens from the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF06JPXVZ2y0"
      },
      "outputs": [],
      "source": [
        "tokens, base_adds, make_carry1s, sum9s, use_carry1s, use_sum9s = next(ds)\n",
        "\n",
        "print(\"tokens.shape\", tokens.shape)\n",
        "print(\"sample tokens\", tokens[:4])\n",
        "print(tokens_to_string(tokens[0]))\n",
        "print(tokens_to_string(tokens[1]))\n",
        "print(tokens_to_string(tokens[2]))\n",
        "print(tokens_to_string(tokens[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFdKmEoBbrkS"
      },
      "source": [
        "Run the model on the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpCn6IsLYZST"
      },
      "outputs": [],
      "source": [
        "original_logits, cache = model.run_with_cache(tokens)\n",
        "print(\"original_logits.numel\", original_logits.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6_N9dsnYgDy"
      },
      "source": [
        "Get key weight matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMylMgSTYg_n"
      },
      "outputs": [],
      "source": [
        "W_E = model.embed.W_E[:-1]\n",
        "print(\"W_E shape:\", W_E.shape)\n",
        "\n",
        "W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in\n",
        "print(\"W_neur shape:\", W_neur.shape)\n",
        "\n",
        "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "print(\"W_logit shape:\", W_logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfXl3-QnYlF4"
      },
      "outputs": [],
      "source": [
        "per_token_train_losses_original, _ = logits_to_tokens_loss(logits, tokens)\n",
        "original_loss = loss_fn(per_token_train_losses_original).mean()\n",
        "print(\"Original Loss:\", utils.to_numpy(original_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shEXTlZJY1nY"
      },
      "outputs": [],
      "source": [
        "pattern_a = cache[\"pattern\", 0, \"attn\"][:, :, -1, 0]\n",
        "pattern_b = cache[\"pattern\", 0, \"attn\"][:, :, -1, 1]\n",
        "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "neuron_pre_acts = cache[\"pre\", 0, \"mlp\"][:, -1, :]\n",
        "\n",
        "for param_name, param in cache.items():\n",
        "    print(param_name + ' shape:', param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4p4p7E0XrIU"
      },
      "source": [
        "# Part 13: Prediction Analysis - Attention Patterns\n",
        "Attention patterns show which token(s) the model's attention heads are paying attention to in each step of the prediction calculation.\n",
        "\n",
        "For the default CoLab set up, the  model has 3 attention heads, and performs 5 digit addition. The attention pattern is 18 by 18 squares (as 54321+77779=132100 is 18 tokens). Time proceeds vertically downwards, with one additional token being revealed horizontally at each step, giving the overall triangle shape. This visualisation provided insights. After the question is fully revealed (at step 11), each head starts attending to pairs of question digits from left to right (i.e. high-value digits before lower-value digits) giving the double staircase\" shape. The three heads attend to a given digit pair in three different steps, giving a time ordering of heads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZHHzrYkYpCA"
      },
      "source": [
        "### Show attention patterns for some randomly chosen tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGUaupu6Xttu"
      },
      "outputs": [],
      "source": [
        "def show_token_attention_patterns(index, layer, token_at_index, use_case):\n",
        "\n",
        "  the_tokens = [str(token) for token in token_at_index.tolist()]\n",
        "  if layer == 0:\n",
        "    tokens_str = tokens_to_string(the_tokens)\n",
        "    print(\"Attention patterns for\", tokens_str)\n",
        "\n",
        "  attention_pattern=cache[\"pattern\", layer, \"attn\"][index]\n",
        "  display(cv.attention.attention_patterns(\n",
        "      tokens=the_tokens,\n",
        "      attention=attention_pattern,\n",
        "      attention_head_names=[f\"L{layer}H{i}\" for i in range(n_heads)],\n",
        "  ))\n",
        "\n",
        "\n",
        "sample_size = 3\n",
        "\n",
        "# Show attention patterns for some randomly chosen tokens\n",
        "for i in range(sample_size):\n",
        "  for layer in range(n_layers):\n",
        "    show_token_attention_patterns(i, layer, tokens[i], \"Misc\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYEqpkPSCtBY"
      },
      "outputs": [],
      "source": [
        "if save_graph_to_file:\n",
        "\n",
        "  tokens_str = []\n",
        "  for i in range(n_heads):\n",
        "    one_token_str = []\n",
        "    for j in tokens[i]:\n",
        "      one_token_str.append(str(utils.to_numpy(j)))\n",
        "    tokens_str.append(one_token_str)\n",
        "\n",
        "  # Refer https://github.com/callummcdougall/CircuitsVis/blob/main/python/circuitsvis/circuitsvis_demo.ipynb\n",
        "\n",
        "  html_object = cv.attention.from_cache(\n",
        "      cache = cache,\n",
        "      tokens = tokens_str, # list of list of strings\n",
        "      return_mode = \"html\",\n",
        "  )\n",
        "\n",
        "  # Create a CoLab file containing the attention pattern(s) in HTML\n",
        "  filename = \"AttentionPattern\" + str(n_digits) + \"Digits\" + str(n_heads) + \"Heads.html\"\n",
        "  with open(filename, \"w\") as f:\n",
        "      f.write(html_object.data)\n",
        "\n",
        "  # Manually download the CoLab \"html\" file and open in your local browser.\n",
        "  # Install and use the Edge extension \"FireShot\" to save a portion of the HTML page as a PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1RSXtlXfaBo"
      },
      "source": [
        "### Show attention patterns for some tokens which BA only\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6OZHTfAE2HU"
      },
      "outputs": [],
      "source": [
        "any_use_carry1s = torch.any(use_carry1s.bool(), dim=1)\n",
        "no_use_carry1s = ~ any_use_carry1s\n",
        "ba_num_cases = utils.to_numpy(torch.sum(no_use_carry1s))\n",
        "if ba_num_cases >= sample_size :\n",
        "  print(f\"Attention patterns for first few BA-only tokens ({ba_num_cases} of {tokens.shape[0]})\")\n",
        "  ba_tokens = tokens[no_use_carry1s==1]\n",
        "  for i in range(sample_size):\n",
        "    for layer in range(n_layers):\n",
        "      show_token_attention_patterns(i, layer, ba_tokens[i], \"BAOnly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0AmmTpyVGAb"
      },
      "source": [
        "Show attention patterns for some tokens which UC1 (and not US9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIlA6DA9FdZ6"
      },
      "outputs": [],
      "source": [
        "num_use_carry1s = torch.sum(use_carry1s, dim=1)\n",
        "any_use_carry1s = torch.where( num_use_carry1s != 0, 1, 0 ) # At least one digit uses UC1\n",
        "num_sum9s = torch.sum(use_sum9s, dim=1)\n",
        "no_sum9s = torch.where( num_sum9s == 0, 1, 0 ) # No digits have Sum9 true\n",
        "filtered_cases = any_use_carry1s & no_sum9s\n",
        "uc1_num_cases = utils.to_numpy(torch.sum(filtered_cases))\n",
        "if uc1_num_cases >= sample_size :\n",
        "  print(f\"Attention patterns for first few UC1-only (and not US9) tokens ({uc1_num_cases} of {tokens.shape[0]})\")\n",
        "  uc1_tokens = tokens[filtered_cases==1]\n",
        "  for i in range(sample_size):\n",
        "    for layer in range(n_layers):\n",
        "      show_token_attention_patterns(i, layer, uc1_tokens[i], \"UC1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIOtgL0uVJ5V"
      },
      "source": [
        "Show attention patterns for some tokens which US9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8aeLj_LFdna"
      },
      "outputs": [],
      "source": [
        "num_sum9s = torch.sum(use_sum9s, dim=1)\n",
        "any_sum9s = torch.where( num_sum9s != 0, 1, 0 ) # At least one digit uses Sum9\n",
        "us9_cases = utils.to_numpy(torch.sum(any_sum9s))\n",
        "if us9_cases >= sample_size :\n",
        "  print(f\"Attention patterns for first few US9 tokens ({us9_cases} of {tokens.shape[0]})\")\n",
        "  us9_tokens = tokens[any_sum9s==1]\n",
        "  for i in range(sample_size):\n",
        "    for layer in range(n_layers):\n",
        "      show_token_attention_patterns(i, layer, us9_tokens[i], \"US9\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNkPCP_JvtmS"
      },
      "source": [
        "# Part 14A: Test Hypothesis D4.T1 is calculated at S11L0H2\n",
        "\n",
        "When n_digits = 5 and n_layers = 2, test whether D4.T1 is calculated at S11L0H2.\n",
        "If it is, when we ablate S11L0H2 we expect the A4 and maybe A5 question answer digits to be inaccurate."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if n_digits >= 5 and n_layers == 2:\n",
        "  exp7_step = 11 # zero-based step to ablate. 0 to say 17\n",
        "  exp7_layer = 0 # zero-based layer to ablate. 0 to 1\n",
        "  exp7_head = 2 # zero-based head to ablate. 0 to 2\n",
        "\n",
        "  exp7_questions = torch.zeros((7, n_ctx)).to(torch.int64)\n",
        "  make_a_question( exp7_questions, 0, 15508, 14500)\n",
        "  make_a_question( exp7_questions, 1, 14508, 15500)\n",
        "  make_a_question( exp7_questions, 2, 24533, 25933)\n",
        "  make_a_question( exp7_questions, 3, 23533, 26933)\n",
        "  make_a_question( exp7_questions, 4, 10880, 41127)\n",
        "  make_a_question( exp7_questions, 5, 41127, 10880)\n",
        "  make_a_question( exp7_questions, 6, 12386, 82623)\n",
        "\n",
        "  verbose = True\n",
        "  exp7_reset_failures()\n",
        "  exp7_perform_core()"
      ],
      "metadata": {
        "id": "H9d0jptswC_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "OxBe5uWwKtXo",
        "EPLArf0NMGay",
        "l3Ui9q02MMIx",
        "8Sil5uniOECa",
        "uEI4jjJmOevS",
        "3qZxJXGaS5e9",
        "AD4jY88YTp_7",
        "qFZIq-7JW_hp",
        "cQmGSsTlYYV3",
        "BgybEeqynJqs",
        "WT0w7xYmImZs",
        "f4p4p7E0XrIU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}