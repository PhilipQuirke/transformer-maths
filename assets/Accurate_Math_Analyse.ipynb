{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG2gZSoSJD5C"
      },
      "source": [
        "# Accurate Integer Mathematics in Transformers - Analyse the Model\n",
        "\n",
        "This CoLab analyses a Transformer model that performs integer addition, subtraction and multiplication e.g. 133357+182243=+0315600, 123450-345670=-0123230 and 000345*000823=+283935. Each digit is a separate token. For 6 digit questions, the model is given 14 \"question\" (input) tokens, and must then predict the corresponding 8 \"answer\" (output) tokens.\n",
        "\n",
        "The model weightings created by the sister CoLab [Accurate_Math_Train](https://github.com/PhilipQuirke/transformer-maths/blob/main/assets/Accurate_Math_Train.ipynb) are loaded from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzkGrSqHJKqN"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGPkaokJQM5"
      },
      "source": [
        "# Part 1: Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmjGdFcdJat3"
      },
      "outputs": [],
      "source": [
        "# Tokens used in vocab. (Token indexes 0 to 9 represent digits 0 to 9)\n",
        "PLUS_INDEX = 10\n",
        "MINUS_INDEX = 11\n",
        "EQUALS_INDEX = 12\n",
        "\n",
        "class Config():\n",
        "  #@markdown Model\n",
        "  n_layers: int = 2 #@param\n",
        "  n_heads: int = 3 #@param\n",
        "\n",
        "  d_vocab: int = EQUALS_INDEX+1\n",
        "  d_model: int = ( 512 // n_heads ) * n_heads # About 512, and divisible by n_heads\n",
        "  d_mlp: int = 4 * d_model\n",
        "  d_head: int = d_model // n_heads  # About 170 when n_heads == 3\n",
        "  seed: int = 129000 #@param\n",
        "\n",
        "  #@markdown Data\n",
        "  n_digits: int = 5 #@param\n",
        "  n_ctx: int = 3 * n_digits + 3\n",
        "  act_fn: str = 'relu'\n",
        "  batch_size: int = 64 #@param\n",
        "\n",
        "  #@markdown Optimizer\n",
        "  n_training_steps: int = 30000 #@param\n",
        "  lr: float = 0.00008 #@param\n",
        "  weight_decay: int = 0.1 #@param\n",
        "\n",
        "  # Save graphs to CoLab temp files as PDF and HTML. Can manually export files for re-use in papers.\n",
        "  save_graph_to_file: bool = True\n",
        "\n",
        "  # The format to output prettytable in. Options are text|html|json|csv|latex\n",
        "  # Use Text for this CoLab, latex for Overleaf output, and html for GitHub blog output\n",
        "  table_out_format: str = \"text\"\n",
        "\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTd3nmsMJV5T"
      },
      "source": [
        "# Part 2: Import libraries\n",
        "Imports standard libraries. Don't bother reading.\n",
        "You will need to give permission for this CoLab to access your Google Drive to load the model weights (created by the \"Accurate Addition - Train\" CoLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czb-vRg_I1c_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp63uecjJel7",
        "outputId": "4422831d-aa06-498a-99aa-318f4e4b7920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "rootdir is /content/drive/MyDrive/AI/CoLabOutput\n",
            "model will save to /content/drive/MyDrive/AI/CoLabOutput/add_digits5_layer2_heads3_dmodel510_dhead170_ctx18_seed129000_train30000.pt\n"
          ]
        }
      ],
      "source": [
        "GLOBAL=True\n",
        "if GLOBAL:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    rootdir=Path('/content/drive/MyDrive/AI/CoLabOutput/')\n",
        "else:\n",
        "    rootdir=Path('./')\n",
        "\n",
        "base_fname = '_digits{}_layer{}_heads{}_dmodel{}_dhead{}_ctx{}_seed{}_train{}.pt'.format(cfg.n_digits, cfg.n_layers, cfg.n_heads, cfg.d_model, cfg.d_head, cfg.n_ctx, cfg.seed, cfg.n_training_steps)\n",
        "\n",
        "add_fname = 'add' + base_fname\n",
        "model_save_location = rootdir/f'{add_fname}'\n",
        "\n",
        "print(f'rootdir is {rootdir}')\n",
        "print('model will save to {}'.format(str(model_save_location)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCdmr6-_Jkzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "826d9935-e7db-417f-ffc9-8230898ca352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.2\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.12)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.2)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kaleido-0.2.1\n",
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-1.12.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.23.0 (from transformer_lens)\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer_lens)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.0)\n",
            "Collecting torch!=2.0,!=2.1.0,>=1.10 (from transformer_lens)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.35.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.5.0)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.1)\n",
            "Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer_lens)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (0.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: typeguard, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, gitdb, nvidia-cusolver-cu12, GitPython, wandb, torch, datasets, accelerate, transformer_lens\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 accelerate-0.25.0 beartype-0.14.1 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 einops-0.7.0 fancy-einsum-0.0.3 gitdb-4.0.11 jaxtyping-0.2.25 multiprocess-0.70.15 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pyarrow-hotfix-0.6 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 torch-2.1.2 transformer_lens-1.12.0 typeguard-2.13.3 wandb-0.16.1\n",
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.1.2)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->torchtyping) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Installing collected packages: torchtyping\n",
            "Successfully installed torchtyping-0.1.4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    %pip install --upgrade numpy\n",
        "    %pip install matplotlib\n",
        "    %pip install prettytable\n",
        "    %pip install seaborn\n",
        "\n",
        "    %pip install kaleido\n",
        "    %pip install transformer_lens\n",
        "    %pip install torchtyping\n",
        "    %pip install transformers\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up2QLAZLJnG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afd410e-4c1f-447d-b47d-8d21ae10f51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using renderer: colab\n"
          ]
        }
      ],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve-TndERJoaJ"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6zOEFryJqGN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from prettytable import PrettyTable\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use seaborn library to display heatmaps\n",
        "use_sns= True\n",
        "try:\n",
        "  import seaborn as sns\n",
        "except Exception as e:\n",
        "  # Suggested work around for conflicts between multiple packages importing different numpy versions\n",
        "  !pip install numpy==1.24.1\n",
        "  try:\n",
        "    import seaborn as sns\n",
        "  except Exception as e:\n",
        "    print(\"sns import exception\", e)\n",
        "    use_sns = False\n",
        "\n",
        "# Use Principal Component Analysis (PCA) library\n",
        "use_pca = True\n",
        "try:\n",
        "  from sklearn.decomposition import PCA\n",
        "except Exception as e:\n",
        "  print(\"pca import exception\", e)\n",
        "  use_pca = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6TE7A9SxySA",
        "outputId": "50d4d6bc-2bc0-47e5-923a-28e0b5a91453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.1\n",
            "  Downloading numpy-1.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.2\n",
            "    Uninstalling numpy-1.26.2:\n",
            "      Successfully uninstalled numpy-1.26.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sns import exception module 'numpy' has no attribute '_no_nep50_warning'\n",
            "pca import exception module 'numpy' has no attribute '_no_nep50_warning'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8VQ4e0QJsIB"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8RfHXneJw6n"
      },
      "source": [
        "# Part 3: Create model\n",
        "This section defines the token embedding / unembedding and creates the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QYFZIalJ3tK"
      },
      "outputs": [],
      "source": [
        "# Embedding / Unembedding\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    tokens = utils.to_numpy(tokens)\n",
        "    x = \"\".join([str(i) for i in tokens[:cfg.n_digits]])\n",
        "    y = \"\".join([str(i) for i in tokens[cfg.n_digits+1:cfg.n_digits*2+1]])\n",
        "    z = \"\".join([str(i) for i in tokens[cfg.n_ctx-cfg.n_digits-1:]])\n",
        "    equals = \"=\"\n",
        "    operator = \"+\"\n",
        "    return f\"{x}{operator}{y}{equals}{z}\"\n",
        "\n",
        "def string_to_tokens(string, batch: bool=False):\n",
        "    lookup = {str(i):i for i in range(10)}\n",
        "    lookup['+']=PLUS_INDEX\n",
        "    lookup['-']=MINUS_INDEX\n",
        "    lookup['=']=EQUALS_INDEX\n",
        "\n",
        "    tokens = [lookup[i] for i in string if i not in '\\n ']\n",
        "    if batch:\n",
        "        return torch.tensor(tokens)[None, :]\n",
        "    else:\n",
        "        return torch.tensor(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA16Nb2PJ7MB"
      },
      "outputs": [],
      "source": [
        "# Transformer creation\n",
        "\n",
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = HookedTransformerConfig(\n",
        "    n_layers = cfg.n_layers,\n",
        "    n_heads = cfg.n_heads,\n",
        "    d_model = cfg.d_model,\n",
        "    d_head = cfg.d_head,\n",
        "    d_mlp = cfg.d_mlp,\n",
        "    act_fn = cfg.act_fn,\n",
        "    normalization_type = 'LN',\n",
        "    d_vocab = cfg.d_vocab,\n",
        "    d_vocab_out = cfg.d_vocab,\n",
        "    n_ctx = cfg.n_ctx,\n",
        "    init_weights = True,\n",
        "    device = \"cuda\",\n",
        "    seed = cfg.seed,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(ht_cfg)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(),\n",
        "                        lr = cfg.lr,\n",
        "                        weight_decay = cfg.weight_decay,\n",
        "                        betas = (0.9, 0.98))\n",
        "\n",
        "# Gives 2e-9 loss (but changes the cells used by the model)\n",
        "# max_iter = cfg.n_training_steps\n",
        "# warmup_iter = max_iter // 5\n",
        "# scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=int(warmup_iter))\n",
        "# scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(np.ceil((max_iter-warmup_iter))))\n",
        "# scheduler  = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[int(warmup_iter)])\n",
        "\n",
        "# Gives 3e-8 loss\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiJhch4KCej"
      },
      "source": [
        "# Part 4: Data Generator. Addition sub-task categorisation\n",
        "This section defines the loss function and the training/tesing data generator.\n",
        "\n",
        "It also defines functions to categorise the training data by the addition sub-task defined in the paper. The addition sub tasks are abbreviated as:\n",
        "- BA is Base Add. Calculates the sum of two digits Dn and Dn' modulo 10, ignoring any carry over from previous columns.\n",
        "- MC1 is Make Carry 1. Evaluates to true if adding digits Dn and Dn' results in a carry over of 1 to the next column.\n",
        "- MS9 is Make Sum 9. Evaluates to true if adding digits Dn and Dn' gives exactly 9.\n",
        "- UC1 is Use Carry 1. Takes the previous column's carry output and adds it to the sum of the current digit pair.\n",
        "- US9 is Use Sum 9. Propagates (aka cascades) a carry over of 1 to the next column if the current column sums to 9 and the previous column generated a carry over. US9 is the most complex task as it spans three digits. For some rare questions (e.g. 00555 + 00445 = 01000) US9 applies to up to four sequential digits, causing a chain effect, with the MC1 cascading through multiple digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ2iNO-nKDBW"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "\n",
        "# Calculate the per-token probability by comparing a batch of prediction \"logits\" to answer \"tokens\"\n",
        "def logits_to_tokens_loss(logits, tokens):\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the addition answer probabilities\n",
        "  ans_logits = logits[:, -(cfg.n_digits+2):-1]\n",
        "\n",
        "  # Convert raw score (logits) vector into a probability distribution.\n",
        "  # Emphasize the largest scores and suppress the smaller ones, to make them more distinguishable.\n",
        "  ans_probs = F.log_softmax(ans_logits.to(torch.float64), dim=-1)\n",
        "\n",
        "  max_indices = torch.argmax(ans_probs, dim=-1)\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the model’s answer.\n",
        "  ans_tokens = tokens[:, -(cfg.n_digits+1):]\n",
        "\n",
        "  # Extract values from the ans_probs tensor, based on indices from the ans_tokens tensor\n",
        "  ans_loss = torch.gather(ans_probs, -1, ans_tokens[:, :, None])[..., 0]\n",
        "\n",
        "  return ans_loss, max_indices\n",
        "\n",
        "# Calculate loss as negative of average per-token mean probability\n",
        "def loss_fn(ans_loss):\n",
        "  return -ans_loss.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSp8pS1eKHf6"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" data generator function. Invoked using next().\n",
        "# \"Addition\" batch entries are formated XXXXX+YYYYY=ZZZZZZ e.g. 55003+80002=135005\n",
        "# \"Subtraction\" batch entries are formated XXXXX-YYYYY=ZZZZZZ e.g. 55003-80002=-24999, 80002-55003=024999\n",
        "# Note that answer has one more digit than the question\n",
        "# Returns characteristics of each batch entry to aid later analysis\n",
        "def data_generator():\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    while True:\n",
        "        #generate a batch of questions (answers calculated below)\n",
        "        batch = torch.zeros((cfg.batch_size, cfg.n_ctx)).to(torch.int64)\n",
        "        x = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "        y = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "\n",
        "\n",
        "        # The UseSum9 task is compound and rare and so hard to learn.\n",
        "        # For some batches, we increase the MakeSum9 case frequency\n",
        "        # UseSum9 also relies on MakeCarry1 (50%) from previous column.\n",
        "        # So UseSum9 frequency is increased by 60% * 40% * 50% = 12%\n",
        "        if random.randint(1, 5) < 3: # 60%\n",
        "          # Flatten x and y to 1D tensors\n",
        "          x_flat = x.view(-1)\n",
        "          y_flat = y.view(-1)\n",
        "\n",
        "          num_elements_to_modify = int(0.40 * x.numel()) # 40%\n",
        "          indices_to_modify = torch.randperm(x_flat.numel())[:num_elements_to_modify]\n",
        "          if random.randint(1, 2) == 1:\n",
        "            x_flat[indices_to_modify] = 9 - y_flat[indices_to_modify]\n",
        "          else:\n",
        "            y_flat[indices_to_modify] = 9 - x_flat[indices_to_modify]\n",
        "\n",
        "          # Reshape x and y back to its original shape\n",
        "          x = x_flat.view(x.shape)\n",
        "          y = y_flat.view(x.shape)\n",
        "\n",
        "\n",
        "        batch[:, :cfg.n_digits] = x\n",
        "        batch[:, cfg.n_digits] = PLUS_INDEX\n",
        "        batch[:, 1+cfg.n_digits:1+cfg.n_digits*2] = y\n",
        "        batch[:, 1+cfg.n_digits*2] = EQUALS_INDEX\n",
        "\n",
        "        # These attributes are used for testing addition\n",
        "        base_adds = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        make_carry1s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        sum9s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        use_carry1s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        use_sum9s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "\n",
        "        # generate the addition question answers & other info for testing\n",
        "        for i in range(cfg.n_digits):\n",
        "            # the column in the test attributes being updated\n",
        "            test_col = cfg.n_digits-1-i\n",
        "\n",
        "            base_add = batch[:, cfg.n_digits-1-i] + batch[:, 2*cfg.n_digits-i]\n",
        "            base_adds[:, test_col] = base_add % 10\n",
        "\n",
        "            sum9 = (base_add == 9)\n",
        "            sum9s[:, test_col] = sum9\n",
        "\n",
        "            if i>0:\n",
        "              use_carry1s[:, test_col] = make_carry1s[:, test_col+1]\n",
        "            use_carry = use_carry1s[:, test_col]\n",
        "\n",
        "            use_sum9s[:, test_col] = sum9 & use_carry;\n",
        "\n",
        "            digit_sum = base_add + use_carry1s[:, test_col]\n",
        "\n",
        "            make_carry = (digit_sum >= 10)\n",
        "            make_carry1s[:, test_col] = make_carry\n",
        "\n",
        "            batch[:, -1-i] = (digit_sum % 10)\n",
        "\n",
        "        # Final (possible) carry to highest digit of the sum\n",
        "        batch[:, -1-cfg.n_digits] = make_carry1s[:, 0]\n",
        "\n",
        "        yield batch.cuda(), base_adds.cuda(), make_carry1s.cuda(), sum9s.cuda(), use_carry1s.cuda(), use_sum9s.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqzljhQ4KJU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7f4202-2f13-441c-c434-dc07e692bdb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5,  0,  4,  4,  4, 10,  6,  4,  5,  7,  1, 12,  1,  1,  5,  0,  1,  5],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "ds = data_generator()\n",
        "\n",
        "tokens, base_adds, make_carry1s, sum9s, use_carry1s, use_sum9s = next(ds)\n",
        "\n",
        "print(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KJhCxFtNKfm"
      },
      "source": [
        "# Part 5 Load Model from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2EgOq5wNPWu"
      },
      "outputs": [],
      "source": [
        "def print_config():\n",
        "  print(\"n_digits=\", cfg.n_digits, \"n_heads=\", cfg.n_heads, \"n_layers=\", cfg.n_layers, \"n_ctx=\", cfg.n_ctx, \"seed=\", cfg.seed, \"n_training_steps=\", cfg.n_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMkB_8GNRc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b8843d-514d-4667-be45-4b270d08fbbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from file /content/drive/MyDrive/AI/CoLabOutput/add_digits5_layer2_heads3_dmodel510_dhead170_ctx18_seed129000_train30000.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HookedTransformer(\n",
              "  (embed): Embed()\n",
              "  (hook_embed): HookPoint()\n",
              "  (pos_embed): PosEmbed()\n",
              "  (hook_pos_embed): HookPoint()\n",
              "  (blocks): ModuleList(\n",
              "    (0-1): 2 x TransformerBlock(\n",
              "      (ln1): LayerNorm(\n",
              "        (hook_scale): HookPoint()\n",
              "        (hook_normalized): HookPoint()\n",
              "      )\n",
              "      (ln2): LayerNorm(\n",
              "        (hook_scale): HookPoint()\n",
              "        (hook_normalized): HookPoint()\n",
              "      )\n",
              "      (attn): Attention(\n",
              "        (hook_k): HookPoint()\n",
              "        (hook_q): HookPoint()\n",
              "        (hook_v): HookPoint()\n",
              "        (hook_z): HookPoint()\n",
              "        (hook_attn_scores): HookPoint()\n",
              "        (hook_pattern): HookPoint()\n",
              "        (hook_result): HookPoint()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (hook_pre): HookPoint()\n",
              "        (hook_post): HookPoint()\n",
              "      )\n",
              "      (hook_attn_in): HookPoint()\n",
              "      (hook_q_input): HookPoint()\n",
              "      (hook_k_input): HookPoint()\n",
              "      (hook_v_input): HookPoint()\n",
              "      (hook_mlp_in): HookPoint()\n",
              "      (hook_attn_out): HookPoint()\n",
              "      (hook_mlp_out): HookPoint()\n",
              "      (hook_resid_pre): HookPoint()\n",
              "      (hook_resid_mid): HookPoint()\n",
              "      (hook_resid_post): HookPoint()\n",
              "    )\n",
              "  )\n",
              "  (ln_final): LayerNorm(\n",
              "    (hook_scale): HookPoint()\n",
              "    (hook_normalized): HookPoint()\n",
              "  )\n",
              "  (unembed): Unembed()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "print(\"Loading model from file\", model_save_location)\n",
        "model.load_state_dict(torch.load(model_save_location))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxoBWHNKRf0"
      },
      "source": [
        "# Part 8: Sample Questions Set Up\n",
        "\n",
        "Create sets of sample questions (by task) to ask the model to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj_xSuSSKR9t"
      },
      "outputs": [],
      "source": [
        "# Insert a number into the question\n",
        "def insert_question_number(the_question, index, first_digit_index, the_digits, n):\n",
        "\n",
        "  last_digit_index = first_digit_index + the_digits - 1\n",
        "\n",
        "  for j in range(the_digits):\n",
        "    the_question[index, last_digit_index-j] = n % 10\n",
        "    n = n // 10\n",
        "\n",
        "\n",
        "# Create a single question\n",
        "def make_a_question(the_question, index, q1, q2):\n",
        "  a = q1 + q2\n",
        "\n",
        "  insert_question_number(the_question, index, 0, cfg.n_digits, q1)\n",
        "\n",
        "  the_question[index, cfg.n_digits] = PLUS_INDEX\n",
        "\n",
        "  insert_question_number( the_question, index, cfg.n_digits+1, cfg.n_digits, q2)\n",
        "\n",
        "  the_question[index, 2*cfg.n_digits+1] = EQUALS_INDEX\n",
        "  offset = 2\n",
        "\n",
        "  insert_question_number(the_question, index, 2*cfg.n_digits + offset, cfg.n_digits+1, q1+q2)\n",
        "\n",
        "\n",
        "# Create a batch of questions from a 2D matrix of ints\n",
        "def make_questions(q_matrix):\n",
        "  length = len(q_matrix)\n",
        "\n",
        "  questions = torch.zeros((length, cfg.n_ctx)).to(torch.int64)\n",
        "\n",
        "  limit = 10 ** cfg.n_digits\n",
        "  for i in range(length):\n",
        "    if (q_matrix[i][0] < limit) and (q_matrix[i][1] < limit) :\n",
        "      make_a_question(questions, i, q_matrix[i][0], q_matrix[i][1])\n",
        "\n",
        "  return questions\n",
        "\n",
        "\n",
        "def prediction_to_string(max_indices):\n",
        "  answer = \"\".join([str(i) for i in utils.to_numpy(max_indices)[0]])\n",
        "  return answer;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xiOHRfGKW-W"
      },
      "outputs": [],
      "source": [
        "# Analyse the question and return the use case as BA, MC, SimpleUS9 or CascadeUS9\n",
        "def get_question_case(q):\n",
        "  qa = utils.to_numpy(q)\n",
        "  qn = qa[:2*cfg.n_digits+2]\n",
        "\n",
        "  # Locate the MC and MS digits (if any)\n",
        "  mc = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  ms = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if qn[dn] + qn[dn + cfg.n_digits + 1] == 9:\n",
        "      ms[cfg.n_digits-1-dn] = 1\n",
        "    if qn[dn] + qn[dn + cfg.n_digits +1] > 9:\n",
        "      mc[cfg.n_digits-1-dn] = 1\n",
        "\n",
        "  # Calculate the use case of a question\n",
        "  if torch.sum(mc) == 0:\n",
        "    return \"BA\"\n",
        "\n",
        "  if torch.sum(ms) == 0:\n",
        "    return \"MC1\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-2 and mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1:\n",
        "      return \"CascadeUS9\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-1 and mc[dn] == 1 and ms[dn+1] == 1:\n",
        "      return \"SimpleUS9\"\n",
        "\n",
        "  return \"MC1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRCPyETTKaEs"
      },
      "outputs": [],
      "source": [
        "# Manually create some questions that strongly test one use case\n",
        "\n",
        "\n",
        "def make_ba_questions():\n",
        "    return make_questions(\n",
        "      [[12345, 33333],\n",
        "      [33333, 12345],\n",
        "      [45762, 33113],\n",
        "      [888, 11111],\n",
        "      [2362, 23123],\n",
        "      [15, 81],\n",
        "      [1000, 4440],\n",
        "      [4440, 1000],\n",
        "      [24033, 25133],\n",
        "      [23533, 21133],\n",
        "      [32500, 1],\n",
        "      [31500, 1111],\n",
        "      [5500, 12323],\n",
        "      [4500, 2209],\n",
        "      [ 33345, 66643], # =099988\n",
        "      [ 66643, 33345], # =099988\n",
        "      [10990, 44000],\n",
        "      [60000, 30000],\n",
        "      [10000, 20000]])\n",
        "\n",
        "\n",
        "def make_uc1_questions():\n",
        "    return make_questions(\n",
        "      [[ 15, 45],\n",
        "      [ 25, 55],\n",
        "      [ 35, 59],\n",
        "      [ 40035, 40049],\n",
        "      [ 5025, 5059],\n",
        "      [ 15, 65],\n",
        "      [ 44000, 46000],\n",
        "      [ 70000, 40000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 1500, 4500],\n",
        "      [ 2500, 5500],\n",
        "      [ 3500, 5900],\n",
        "      [ 15020, 45091],\n",
        "      [ 25002, 55019],\n",
        "      [ 35002, 59019]])\n",
        "\n",
        "\n",
        "def make_simple_us9_questions():\n",
        "    return make_questions(\n",
        "      [[ 55, 45],\n",
        "      [ 45, 55],\n",
        "      [ 45, 59],\n",
        "      [ 35, 69],\n",
        "      [ 25, 79],\n",
        "      [ 15, 85],\n",
        "      [ 15, 88],\n",
        "      [ 15508, 14500],\n",
        "      [ 14508, 15500],\n",
        "      [ 24533, 25933],\n",
        "      [ 23533, 26933],\n",
        "      [ 32500, 7900],\n",
        "      [ 31500, 8500],\n",
        "      [ 550, 450],\n",
        "      [ 450, 550],\n",
        "      [ 10880, 41127],\n",
        "      [ 41127, 10880],\n",
        "      [ 12386, 82623]])\n",
        "\n",
        "\n",
        "def make_cascade_us9_questions(clean = True):\n",
        "    return make_questions(\n",
        "      # These are two level UseSum9 cascades\n",
        "      [[ 555, 445],\n",
        "      [ 3340, 6660],\n",
        "      [ 8880, 1120],\n",
        "      [ 1120, 8880],\n",
        "      [ 123, 877],\n",
        "      [ 877, 123],\n",
        "      [ 321, 679],\n",
        "      [ 679, 321],\n",
        "      [ 1283, 88786],\n",
        "      # These are three level UseSum9 cascades\n",
        "      [ 5555, 4445],\n",
        "      [ 55550, 44450],\n",
        "      [ 334, 666],\n",
        "      [ 3340, 6660],\n",
        "      [ 33400, 66600],\n",
        "      [ 888, 112],\n",
        "      [ 8880, 1120],\n",
        "      [ 88800, 11200],\n",
        "      [ 1234, 8766],\n",
        "      [ 4321, 5679],\n",
        "      # These are four level UseSum9 cascades\n",
        "      [ 44445, 55555],\n",
        "      [ 33334, 66666],\n",
        "      [ 88888, 11112],\n",
        "      [ 12345, 87655],\n",
        "      [ 54321, 45679],\n",
        "      [ 45545, 54455],\n",
        "      [ 36634, 63366],\n",
        "      [ 81818, 18182],\n",
        "      [ 87345, 12655],\n",
        "      [ 55379, 44621]])\n",
        "\n",
        "\n",
        "# These questions focus mainly on 1 digit at a time\n",
        "# (We're assuming that the 0 + 0 digit additions are trivial bigrams)\n",
        "def make_answerdigit_questions():\n",
        "    return make_questions(\n",
        "      [[ 1, 0],\n",
        "      [ 4, 3],\n",
        "      [ 5, 5],\n",
        "      [ 8, 1],\n",
        "      [ 40, 30],\n",
        "      [ 44, 46],\n",
        "      [ 400, 300],\n",
        "      [ 440, 460],\n",
        "      [ 800, 100],\n",
        "      [ 270, 470],\n",
        "      [ 600, 300],\n",
        "      [ 4000, 3000],\n",
        "      [ 4400, 4600],\n",
        "      [ 6000, 3000],\n",
        "      [ 7000, 4000],\n",
        "      [ 40000, 30000],\n",
        "      [ 44000, 46000],\n",
        "      [ 60000, 30000],\n",
        "      [ 70000, 40000],\n",
        "      [ 10000, 20000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 76000, 99000]])\n",
        "\n",
        "\n",
        "# Returns 128 random and ~100 manually-chosen questions\n",
        "def make_varied_questions():\n",
        "  q0, _, _, _, _, _ = next(ds)\n",
        "  q1 = make_ba_questions()\n",
        "  q2 = make_uc1_questions()\n",
        "  q3 = make_simple_us9_questions()\n",
        "  q4 = make_cascade_us9_questions()\n",
        "  q5 = make_answerdigit_questions()\n",
        "  q6, _, _, _, _, _ = next(ds)\n",
        "\n",
        "  questions = torch.vstack((q0.cuda(), q1.cuda(), q2.cuda(), q3.cuda(), q4.cuda(), q5.cuda(), q6.cuda()))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFzN_OxmKcEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea03022b-e005-49d4-f907-d7c2b427e28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BA #Questions= 19\n",
            "MC1 #Questions= 19\n",
            "SimpleUS9 #Questions= 18\n",
            "CascadeUS9 #Questions= 29\n"
          ]
        }
      ],
      "source": [
        "# Test that the get_question_case code works as expected\n",
        "def unit_test_get_question_case_core(correct_case, questions):\n",
        "  num_questions = questions.shape[0]\n",
        "  print( correct_case, \"#Questions=\", num_questions)\n",
        "  for i in range(num_questions):\n",
        "    question_case = get_question_case(questions[i])\n",
        "    if question_case != correct_case:\n",
        "      print( \"Case mismatch:\", correct_case, question_case, questions[i])\n",
        "\n",
        "def unit_test_get_question_case():\n",
        "  unit_test_get_question_case_core( \"BA\", make_ba_questions())\n",
        "  unit_test_get_question_case_core( \"MC1\", make_uc1_questions())\n",
        "  unit_test_get_question_case_core( \"SimpleUS9\", make_simple_us9_questions())\n",
        "  unit_test_get_question_case_core( \"CascadeUS9\", make_cascade_us9_questions())\n",
        "\n",
        "unit_test_get_question_case()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E56siA_QKe0W"
      },
      "outputs": [],
      "source": [
        "num_questions = 0;\n",
        "correct_answers = 0;\n",
        "verbose = True\n",
        "total_mean_loss = 0.0\n",
        "\n",
        "\n",
        "# Clear the question summary results\n",
        "def clear_questions_results(title):\n",
        "  global num_questions\n",
        "  global correct_answers\n",
        "  global verbose\n",
        "  global total_mean_loss\n",
        "\n",
        "  num_questions = 0\n",
        "  correct_answers = 0\n",
        "  total_mean_loss = 0\n",
        "\n",
        "  if verbose:\n",
        "    print(title)\n",
        "\n",
        "\n",
        "# Ask model to predict answer for each question & collect results\n",
        "def do_questions(questions):\n",
        "    global num_questions\n",
        "    global correct_answers\n",
        "    global verbose\n",
        "    global total_mean_loss\n",
        "\n",
        "    num_questions = questions.shape[0]\n",
        "    for question_num in range(num_questions):\n",
        "      q = questions[question_num]\n",
        "\n",
        "      # Run with no hook\n",
        "      the_logits = model(q.cuda())\n",
        "\n",
        "      q_2d = q.unsqueeze(0)\n",
        "      losses_raw, max_indices = logits_to_tokens_loss(the_logits, q_2d.cuda())\n",
        "      losses = loss_fn(losses_raw)\n",
        "      mean_loss = utils.to_numpy(losses.mean())\n",
        "      total_mean_loss = total_mean_loss + mean_loss\n",
        "\n",
        "      model_answer_str = prediction_to_string(max_indices)\n",
        "      model_answer_num = int(model_answer_str)\n",
        "\n",
        "      i = cfg.n_digits*2 + 2\n",
        "\n",
        "      a = 0\n",
        "      # 5 digit addition yields a 6 digit answer. Hence cfg.n_digits+1\n",
        "      for j in range(cfg.n_digits+1):\n",
        "        a = a * 10 + q[i+j]\n",
        "\n",
        "      correct = (model_answer_num == a)\n",
        "      if correct :\n",
        "        correct_answers += 1\n",
        "\n",
        "      if verbose:\n",
        "        print(tokens_to_string(q), \"ModelAnswer:\", model_answer_str, \"Correct:\", correct, \"Loss:\", mean_loss )\n",
        "\n",
        "\n",
        "# Print the question summary results\n",
        "def print_questions_results(prefix, output_table):\n",
        "  global num_questions\n",
        "  global correct_answers\n",
        "  global total_mean_loss\n",
        "\n",
        "  output_table.add_row([prefix, num_questions, str(correct_answers), 100*correct_answers/num_questions, total_mean_loss/num_questions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RHrsjKqKhR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0af209-acc1-486f-9d9b-c92dda86dc9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.attn.hook_result', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.attn.hook_result', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
            "Sample Mean Loss 2.4244332144461047e-08\n",
            "Sample blocks.0.attn.hook_z torch.Size([239, 18, 3, 170])\n",
            "Mean blocks.0.attn.hook_z torch.Size([1, 18, 3, 170])\n",
            "Sample blocks.0.hook_resid_post torch.Size([239, 18, 510])\n",
            "Mean blocks.0.hook_resid_post torch.Size([1, 18, 510])\n",
            "Sample blocks.0.mlp.hook_post torch.Size([239, 18, 2040])\n",
            "Mean blocks.0.mlp.hook_post torch.Size([1, 18, 2040])\n"
          ]
        }
      ],
      "source": [
        "# Build a test batch of 64 random and ~100 manually-chosen questions\n",
        "varied_questions = make_varied_questions();\n",
        "\n",
        "\n",
        "# Run the sample batch, gather the cache\n",
        "model.reset_hooks()\n",
        "model.set_use_attn_result(True)\n",
        "sample_logits, sample_cache = model.run_with_cache(varied_questions.cuda())\n",
        "print(sample_cache) # Gives names of datasets in the cache\n",
        "sample_losses_raw, sample_max_indices = logits_to_tokens_loss(sample_logits, varied_questions.cuda())\n",
        "sample_loss_mean = utils.to_numpy(loss_fn(sample_losses_raw).mean())\n",
        "print(\"Sample Mean Loss\", sample_loss_mean) # Loss < 0.04 is good\n",
        "\n",
        "\n",
        "# attn.hook_z is the \"attention head output\" hook point name (at a specified layer)\n",
        "# Used in h_set_attn_hook_z and t_*_hook functions\n",
        "l_attn_hook_z_name = ['blocks.0.attn.hook_z','blocks.1.attn.hook_z']\n",
        "sample_attn_z = sample_cache[l_attn_hook_z_name[0]]\n",
        "print(\"Sample\", l_attn_hook_z_name[0], sample_attn_z.shape) # gives [239, 18, 3, 170] = #questions, cfg.n_ctx, n_heads, d_head\n",
        "mean_attn_z = torch.mean(sample_attn_z, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_attn_hook_z_name[0], mean_attn_z.shape) # gives [1, 18, 3, 170] = 1, cfg.n_ctx, n_heads, d_head\n",
        "\n",
        "\n",
        "# hook_resid_pre is the \"pre residual memory update\" hook point name (at a specified layer)\n",
        "# Used in o_*_hook functions\n",
        "l_hook_resid_pre_name = ['blocks.0.hook_resid_pre','blocks.1.hook_resid_pre']\n",
        "\n",
        "\n",
        "# hook_resid_post is the \"post residual memory update\" hook point name (at a specified layer)\n",
        "# Used in c_*_hook and t_*_hook functions\n",
        "l_hook_resid_post_name = ['blocks.0.hook_resid_post','blocks.1.hook_resid_post']\n",
        "sample_resid_post = sample_cache[l_hook_resid_post_name[0]]\n",
        "print(\"Sample\", l_hook_resid_post_name[0], sample_resid_post.shape) # gives [239, 18, 510] = #questions, cfg.n_ctx, d_model\n",
        "mean_resid_post = torch.mean(sample_resid_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_hook_resid_post_name[0], mean_resid_post.shape) # gives [1, 18, 510] = 1, cfg.n_ctx, d_model\n",
        "\n",
        "\n",
        "# mlp.hook_post is the \"MLP layer\" hook point name (at a specified layer)\n",
        "# Used in m_*_hook functions\n",
        "l_mlp_hook_post_name = ['blocks.0.mlp.hook_post','blocks.1.mlp.hook_post']\n",
        "sample_mlp_hook_post = sample_cache[l_mlp_hook_post_name[0]]\n",
        "print(\"Sample\", l_mlp_hook_post_name[0], sample_mlp_hook_post.shape) # gives [239, 18, 2040] = #questions, cfg.n_ctx, d_model*4\n",
        "mean_mlp_hook_post = torch.mean(sample_mlp_hook_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_mlp_hook_post_name[0], mean_mlp_hook_post.shape) # gives [1, 18, 2040] = 1, cfg.n_ctx, d_model*4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lag8C3d_KkeQ"
      },
      "source": [
        "# Part 9: Prediction Analysis By Use Case\n",
        "This section sets up BA, UC1 and US9 test cases that will be re-used in later experiments to show the impact of ablating heads or token positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z6RdolRKnnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dbffa9-d70e-4d63-81b6-a9e9982ab5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_digits= 5 n_heads= 3 n_layers= 2 n_ctx= 18 seed= 129000 n_training_steps= 30000\n",
            "\n",
            "+--------------+------------+----------+----------+------------------------+\n",
            "|     Case     | #Questions | #Correct | %Correct |       Mean loss        |\n",
            "+--------------+------------+----------+----------+------------------------+\n",
            "|   BaseAdd    |     19     |    19    |  100.0   | 1.7719098271315032e-08 |\n",
            "|  UseCarry1   |     19     |    19    |  100.0   | 2.1052305377443917e-08 |\n",
            "|  SimpleUS9   |     18     |    18    |  100.0   | 5.0681080184985254e-08 |\n",
            "|  CascadeUS9  |     29     |    29    |  100.0   | 3.191186344753348e-08  |\n",
            "| AnswerDigits |     26     |    26    |  100.0   | 1.969638678290433e-08  |\n",
            "|   OVERALL    |    111     |          |          | 3.0864662089901377e-06 |\n",
            "+--------------+------------+----------+----------+------------------------+\n"
          ]
        }
      ],
      "source": [
        "exp0_output = PrettyTable()\n",
        "exp0_output.field_names = [\"Case\", \"#Questions\", \"#Correct\", \"%Correct\", \"Mean loss\"]\n",
        "verbose = False\n",
        "\n",
        "clear_questions_results(\"Simple BaseAdd cases\")\n",
        "do_questions(make_ba_questions())\n",
        "print_questions_results(\"BaseAdd\", exp0_output)\n",
        "sum_total_mean_loss = total_mean_loss\n",
        "sum_num_questions = num_questions\n",
        "\n",
        "clear_questions_results(\"These are Use Carry 1 (UC1) examples (not UseSum9 examples)\")\n",
        "do_questions(make_uc1_questions())\n",
        "print_questions_results(\"UseCarry1\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "clear_questions_results(\"These are simple (one level) UseSum9 exampless\")\n",
        "do_questions(make_simple_us9_questions())\n",
        "print_questions_results(\"SimpleUS9\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "clear_questions_results(\"These are UseSum9 two, three and four level cascades\")\n",
        "do_questions(make_cascade_us9_questions())\n",
        "print_questions_results(\"CascadeUS9\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "clear_questions_results(\"These questions focus on different answer digits\")\n",
        "do_questions(make_answerdigit_questions())\n",
        "print_questions_results(\"AnswerDigits\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "exp0_output.add_row([\"OVERALL\", sum_num_questions, \"\", \"\", sum_total_mean_loss])\n",
        "\n",
        "print_config()\n",
        "print()\n",
        "print(exp0_output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyK7QeUjLLFm"
      },
      "source": [
        "# Part 11: Set Up \"Count\" Framework\n",
        "\n",
        "Create way to get model to predict sample question answers and analysis/show results. Use prefix \"c_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYPI4tw0LNhV"
      },
      "outputs": [],
      "source": [
        "# Build up a list of success/failure by case (BA, MC1, US9) found, and the frequency of each case\n",
        "c_case_counts = {}\n",
        "\n",
        "\n",
        "def count_question_cases(questions):\n",
        "  global c_case_counts\n",
        "\n",
        "  c_case_counts = {}\n",
        "\n",
        "  for i in range(questions.shape[0]):\n",
        "    q_case = get_question_case(questions[i])\n",
        "\n",
        "    if q_case in c_case_counts:\n",
        "      # If the key is already in the dictionary, increment its count\n",
        "      c_case_counts[q_case] += 1\n",
        "    else:\n",
        "      # If the key is not in the dictionary, add it with a count of 1\n",
        "      c_case_counts[q_case] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZXEfGBMLPFW"
      },
      "outputs": [],
      "source": [
        "# Compare each digit in the answer. Returns a A45 pattern where 4 each digit means a failed digit\n",
        "def get_digit_accuracy_impact(a_int, answer_str):\n",
        "  a_str = str(a_int.cpu().numpy()).zfill(cfg.n_digits+1)\n",
        "  match_str = \"A\"\n",
        "  for i in range(cfg.n_digits+1):\n",
        "    match_str += \"\" if answer_str[i] == a_str[i] else str(cfg.n_digits-i)\n",
        "\n",
        "  return \"\" if match_str == \"A\" else match_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liT_UbPOLRhO"
      },
      "outputs": [],
      "source": [
        "# Build up a list of success/failure digit-patterns found, and the frequency of each pattern\n",
        "c_pattern_fails = {}\n",
        "\n",
        "\n",
        "def clear_pattern_fails():\n",
        "  global c_pattern_fails\n",
        "\n",
        "  c_pattern_fails = {}\n",
        "\n",
        "\n",
        "def add_pattern_fail(match_str):\n",
        "  global c_pattern_fails\n",
        "\n",
        "  if match_str in c_pattern_fails:\n",
        "    # If the key is already in the dictionary, increment its count\n",
        "    c_pattern_fails[match_str] += 1\n",
        "  else:\n",
        "    # If the key is not in the dictionary, add it with a count of 1\n",
        "    c_pattern_fails[match_str] = 1\n",
        "\n",
        "\n",
        "def get_pattern_fails():\n",
        "  global c_pattern_fails\n",
        "\n",
        "  results = \"\"\n",
        "  top_result = \"\"\n",
        "  if len(c_pattern_fails) > 0 :\n",
        "    sorted_fails = dict(sorted(c_pattern_fails.items(), key=lambda item: item[1], reverse=True))\n",
        "    for key, value in sorted_fails.items():\n",
        "      this_cell = key + \"=\" + str(value)\n",
        "\n",
        "      results = results + this_cell + \" \"\n",
        "\n",
        "      if top_result == \"\":\n",
        "        top_result = this_cell\n",
        "      else:\n",
        "        top_result = top_result + \", \" + this_cell\n",
        "\n",
        "  return results, top_result\n",
        "\n",
        "\n",
        "def get_pattern_fails_total():\n",
        "  global c_pattern_fails\n",
        "\n",
        "  if len(c_pattern_fails) == 0:\n",
        "    return 0\n",
        "\n",
        "  total_sum = 0\n",
        "  for key, value in c_pattern_fails.items():\n",
        "      if isinstance(value, int):\n",
        "          total_sum += value\n",
        "  return total_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TudM4_8PLTh1"
      },
      "outputs": [],
      "source": [
        "# Build up a count of failure cases\n",
        "c_case_fails = {}\n",
        "\n",
        "\n",
        "def clear_case_fails():\n",
        "  global c_case_fails\n",
        "\n",
        "  c_case_fails = {}\n",
        "\n",
        "\n",
        "def add_case_fail(case_key):\n",
        "  global c_case_fails\n",
        "\n",
        "  if case_key in c_case_fails:\n",
        "    # If the key is already in the dictionary, increment its count\n",
        "    c_case_fails[case_key] += 1\n",
        "  else:\n",
        "    # If the key is not in the dictionary, add it with a count of 1\n",
        "    c_case_fails[case_key] = 1\n",
        "\n",
        "\n",
        "def total_case_fails():\n",
        "  global c_case_fails\n",
        "\n",
        "  answer = 0\n",
        "  for _, value in c_case_fails.items():\n",
        "    answer = answer + value\n",
        "  return answer\n",
        "\n",
        "\n",
        "def get_case_fails():\n",
        "  global c_case_fails\n",
        "  global c_case_counts\n",
        "\n",
        "  results = \"\"\n",
        "  num_results = len(c_case_fails)\n",
        "  if num_results > 0:\n",
        "    sorted_fails = dict(sorted(c_case_fails.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    for key, value in sorted_fails.items():\n",
        "      percent = round(100 * value / c_case_counts[key])\n",
        "      results = results + \"%\" + key + \"=\" + str(percent)+ \" \"\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqmNRgCgLWV1"
      },
      "outputs": [],
      "source": [
        "def predict_experiment_question(questions, the_hook, the_threshold):\n",
        "\n",
        "  c_loss_mean = 0\n",
        "\n",
        "  clear_case_fails()\n",
        "  clear_pattern_fails()\n",
        "  count_question_cases(questions)\n",
        "\n",
        "  answer_str = \"\"\n",
        "  for question_num in range(questions.shape[0]):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    model.reset_hooks()\n",
        "    model.set_use_attn_result(True)\n",
        "    exp_logits = model.run_with_hooks(q.cuda(), return_type=\"logits\", fwd_hooks=the_hook)\n",
        "\n",
        "    q_2d = q.unsqueeze(0)\n",
        "    exp_losses_raw, exp_max_indices = logits_to_tokens_loss(exp_logits, q_2d.cuda())\n",
        "    c_loss_mean = utils.to_numpy(loss_fn(exp_losses_raw).mean())\n",
        "\n",
        "    # Only show the question if the loss exceeds the threshold (because of the ablated token position)\n",
        "    if c_loss_mean > the_threshold:\n",
        "      answer_str = prediction_to_string(exp_max_indices)\n",
        "\n",
        "      i = 12\n",
        "      a = q[i+0] * 100000 + q[i+1] * 10000 + q[i+2] * 1000 + q[i+3] * 100 + q[i+4] * 10 + q[i+5] * 1;\n",
        "\n",
        "      match_str = get_digit_accuracy_impact( a, answer_str )\n",
        "      # Only count the question if the model got the question wrong\n",
        "      if 'A' in match_str:\n",
        "        the_case = get_question_case(q)\n",
        "        add_case_fail(the_case)\n",
        "        add_pattern_fail(match_str)\n",
        "        if verbose:\n",
        "          print(tokens_to_string(q), \"ModelAnswer:\", answer_str, \"Matches:\", match_str, \"Loss:\", c_loss_mean, \"Case:\", the_case )\n",
        "\n",
        "  return c_loss_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmsGWUbILYin"
      },
      "source": [
        "# Part 12: Ablate ALL Heads in EACH token position. What is the impact on Loss?\n",
        "\n",
        "Here we ablate all heads in each token position (overriding the model memory aka residual stream) and see if loss increases. If loss increases the token position is used by the algorithm. Unused token positions can be excluded from further analysis. Use \"C_\" prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx7LwECBLajQ"
      },
      "outputs": [],
      "source": [
        "class C_Config():\n",
        "  position : int = 0  # zero-based token position to ablate\n",
        "  threshold : float = 0.01\n",
        "  questions = varied_questions\n",
        "  output = PrettyTable()\n",
        "  perc_list = []\n",
        "  hook_calls : int = 0\n",
        "\n",
        "  min_useful_position : int = -1 # Minimum useful position where loss increases on ablation\n",
        "  max_useful_position : int = -1 # Maximum useful position where loss increases on ablation\n",
        "\n",
        "\n",
        "ccfg = C_Config()\n",
        "ccfg.output.field_names = [\"Position\", \"Fails\", \"% Fails by Case\", \"# Fails by Patterns\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGL57MRBLdUh"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "\n",
        "\n",
        "def c_set_resid_post_hook(value, hook):\n",
        "  global ccfg\n",
        "\n",
        "  #print( \"In hook\", l_hook_resid_post_name[ccfg.layer], ccfg.ablate, ccfg.position, value.shape) # Get [64, 18, 510] = cfg.batch_size, num_tokens, d_model\n",
        "\n",
        "  # Copy the mean resid post values in position N to all the batch questions\n",
        "  value[:,ccfg.position,:] = mean_resid_post[0,ccfg.position,:].clone()\n",
        "\n",
        "\n",
        "num_questions = 0\n",
        "if cfg.n_digits >= 5 :\n",
        "  c_fwd_hooks = [(l_hook_resid_post_name[0], c_set_resid_post_hook)] if cfg.n_layers == 1 else [(l_hook_resid_post_name[0], c_set_resid_post_hook),(l_hook_resid_post_name[1], c_set_resid_post_hook)]\n",
        "\n",
        "  num_questions = ccfg.questions.shape[0]\n",
        "\n",
        "  for ccfg.position in range(cfg.n_ctx):\n",
        "    loss_mean = predict_experiment_question(ccfg.questions, c_fwd_hooks, ccfg.threshold)\n",
        "\n",
        "    num_fails = total_case_fails()\n",
        "    perc_fails = 0\n",
        "    if num_fails > 0:\n",
        "      perc_fails = round(100 * num_fails / num_questions)\n",
        "\n",
        "      if ccfg.min_useful_position == -1:\n",
        "        ccfg.min_useful_position = ccfg.position\n",
        "      ccfg.max_useful_position = ccfg.position\n",
        "\n",
        "    ccfg.perc_list = ccfg.perc_list + [perc_fails]\n",
        "\n",
        "    (pattern_results, top_pattern) = get_pattern_fails()\n",
        "    ccfg.output.add_row([str(ccfg.position), str(perc_fails)+\"%\", get_case_fails(), pattern_results])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpRp5YMmLe1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06882da-a828-4928-ef96-436d963a843f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_digits= 5 n_heads= 3 n_layers= 2 n_ctx= 18 seed= 129000 n_training_steps= 30000\n",
            "num_questions= 239 min_useful_position= 8 max_useful_position= 16\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----------------------------------------------+------------------------------+\n",
            "| Position | Fails |                % Fails by Case                |     # Fails by Patterns      |\n",
            "+----------+-------+-----------------------------------------------+------------------------------+\n",
            "|    0     |   0%  |                                               |                              |\n",
            "|    1     |   0%  |                                               |                              |\n",
            "|    2     |   0%  |                                               |                              |\n",
            "|    3     |   0%  |                                               |                              |\n",
            "|    4     |   0%  |                                               |                              |\n",
            "|    5     |   0%  |                                               |                              |\n",
            "|    6     |   0%  |                                               |                              |\n",
            "|    7     |   0%  |                                               |                              |\n",
            "|    8     |   5%  |         %SimpleUS9=19 %CascadeUS9=10          |         A4=9 A54=4           |\n",
            "|    9     |   8%  |         %SimpleUS9=21 %CascadeUS9=23          |   A3=8 A43=6 A543=3 A32=2    |\n",
            "|    10    |  17%  |         %CascadeUS9=64 %SimpleUS9=34          | A2=16 A5432=12 A32=9 A432=4  |\n",
            "|    11    |  62%  | %MC1=54 %BA=100 %SimpleUS9=53 %CascadeUS9=41  |           A5=147             |\n",
            "|    12    |  65%  |  %MC1=83 %BA=74 %SimpleUS9=45 %CascadeUS9=31  |           A4=156             |\n",
            "|    13    |  63%  |  %MC1=74 %BA=72 %SimpleUS9=64 %CascadeUS9=23  |           A3=151             |\n",
            "|    14    |  91%  |  %MC1=89 %BA=96 %SimpleUS9=85 %CascadeUS9=97  |           A2=218             |\n",
            "|    15    |  95%  | %MC1=93 %BA=94 %SimpleUS9=94 %CascadeUS9=100  |           A1=226             |\n",
            "|    16    |  94%  |  %MC1=92 %BA=96 %SimpleUS9=91 %CascadeUS9=97  |           A0=224             |\n",
            "|    17    |   0%  |                                               |                              |\n",
            "+----------+-------+-----------------------------------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "print_config()\n",
        "print(\"num_questions=\", num_questions, \"min_useful_position=\", ccfg.min_useful_position, \"max_useful_position=\", ccfg.max_useful_position )\n",
        "print()\n",
        "\n",
        "plt.hist(ccfg.perc_list, cfg.n_ctx, facecolor='blue', alpha=0.5)\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(r'Histogram of IQ: $\\mu=100$, $\\sigma=15$')\n",
        "# Tweak spacing to prevent clipping of ylabel\n",
        "plt.subplots_adjust(left=0.15)\n",
        "plt.show()\n",
        "\n",
        "print(ccfg.output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904WBkTOLg_5"
      },
      "source": [
        "# Part 13: Setup: Cell matrix\n",
        "\n",
        "Uses \"u_\" prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qgSo4acLj5D"
      },
      "outputs": [],
      "source": [
        "class UsefulCell():\n",
        "  # Is this cell an attention head? If not, it must be an MLP layer\n",
        "  is_head: bool = True\n",
        "\n",
        "  position: int = 0  # token-position\n",
        "  layer: int = 0\n",
        "  head: int = 0\n",
        "\n",
        "\n",
        "# We (once) calculate the list of cells (attention head and MLP layers per position) that are useful to the model.\n",
        "calc_useful_cells = True\n",
        "# Once this list of useful cells is calculated (available) it is used to speed up functions.\n",
        "useful_cells = []\n",
        "\n",
        "\n",
        "def add_useful_cell(the_is_head, the_position, the_layer, the_head):\n",
        "  global calc_useful_cells\n",
        "  global useful_cells\n",
        "\n",
        "  if calc_useful_cells:\n",
        "    useful_cell = UsefulCell()\n",
        "    useful_cell.is_head = the_is_head\n",
        "    useful_cell.position = the_position\n",
        "    useful_cell.layer = the_layer\n",
        "    useful_cell.head = the_head\n",
        "\n",
        "    useful_cells = useful_cells + [useful_cell]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTaSo-UzLlZ_"
      },
      "outputs": [],
      "source": [
        "class U_Config():\n",
        "  # This is a head+MLP (row) by token (column) matrix of percent of failure percentages with associated notes\n",
        "  fail_percs = [[]]\n",
        "  fail_notes = [[]]\n",
        "  num_heads = 0\n",
        "  num_mlps = 0\n",
        "\n",
        "\n",
        "ucfg = U_Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wSoqTt1LofE"
      },
      "outputs": [],
      "source": [
        "def ucfg_reset():\n",
        "  global ucfg\n",
        "\n",
        "  ucfg.fail_percs = [[0 for _ in range(cfg.n_ctx)] for _ in range((cfg.n_heads + 1) * cfg.n_layers)]\n",
        "  ucfg.fail_notes = [[\"\" for _ in range(cfg.n_ctx)] for _ in range((cfg.n_heads + 1) * cfg.n_layers)]\n",
        "  ucfg.num_heads = 0\n",
        "  ucfg.num_mlps = 0\n",
        "\n",
        "\n",
        "ucfg_reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ28dx5YLs0P"
      },
      "outputs": [],
      "source": [
        "def add_u_fail_perc( the_position, the_layer, the_head, perc_fails, notes ):\n",
        "  global ucfg\n",
        "\n",
        "  the_row = the_layer * (cfg.n_heads+1) + the_head\n",
        "\n",
        "  if ucfg.fail_percs[the_row][the_position] == 0 :\n",
        "    ucfg.fail_percs[the_row][the_position] = perc_fails\n",
        "\n",
        "    add_useful_cell(the_head != cfg.n_heads, the_position, the_layer, the_head)\n",
        "\n",
        "  else:\n",
        "    print( \"add_u_fail_perc: Bad index\", the_row, the_position)\n",
        "\n",
        "  ucfg.fail_percs[the_row][the_position] = perc_fails\n",
        "  ucfg.fail_notes[the_row][the_position] = notes\n",
        "\n",
        "\n",
        "def add_head_fail_perc( the_position, the_layer, the_head, perc_fails, notes ):\n",
        "  global ucfg\n",
        "\n",
        "  add_u_fail_perc( the_position, the_layer, the_head, perc_fails, notes )\n",
        "  ucfg.num_heads += 1\n",
        "\n",
        "\n",
        "def add_mlp_fail_perc( the_position, the_layer, perc_fails, notes ):\n",
        "  global ucfg\n",
        "\n",
        "  add_u_fail_perc( the_position, the_layer, cfg.n_heads, perc_fails, notes )\n",
        "  ucfg.num_mlps += 1\n",
        "\n",
        "\n",
        "def get_column_headings():\n",
        "  datums = [\"Position\"]\n",
        "  for i in range(ccfg.min_useful_position, ccfg.max_useful_position+1):\n",
        "    datums = datums + [\"P\"+str(i)]\n",
        "  return datums\n",
        "\n",
        "\n",
        "def get_row_heading(i):\n",
        "  head = i % (cfg.n_heads + 1)\n",
        "  layer = i // (cfg.n_heads + 1)\n",
        "  return ( \"L\" + str(layer) + \"H\" + str(head) ) if head < cfg.n_heads else \"MLP \"\n",
        "\n",
        "\n",
        "# Print a 2 by 2 matrix of the percentage failures.\n",
        "def print_u_fail_percs():\n",
        "  global ucfg\n",
        "  global mcfg\n",
        "\n",
        "  print(\"The % failure rate when each head or MLP in each position is ablated, # failed heads =\", ucfg.num_heads, \", # failed mlps =\", ucfg.num_mlps )\n",
        "\n",
        "  cell_output = PrettyTable()\n",
        "\n",
        "  col_headings = get_column_headings()\n",
        "  cell_output.field_names = col_headings\n",
        "\n",
        "  num_rows = (cfg.n_heads + 1) * cfg.n_layers\n",
        "  num_cols = ccfg.max_useful_position - ccfg.min_useful_position + 1\n",
        "  percs_matrix = torch.zeros((num_rows, num_cols)).to(torch.int64)\n",
        "  mask_matrix = torch.zeros((num_rows, num_cols)).to(torch.int64)\n",
        "  row_headings = []\n",
        "\n",
        "  for i in range(num_rows):\n",
        "    row_heading = get_row_heading(i)\n",
        "    row_headings = row_headings + [row_heading]\n",
        "\n",
        "    datums = [row_heading]\n",
        "    for j in range(num_cols):\n",
        "      value = ucfg.fail_percs[i][ccfg.min_useful_position + j]\n",
        "      datums = datums + [\"\" if value == 0 else str(value)+\"%\"]\n",
        "      percs_matrix[i,j] = value\n",
        "      mask_matrix[i,j] = ( value == 0 )\n",
        "\n",
        "    cell_output.add_row(datums)\n",
        "\n",
        "  # Display a 2D heat map of the percentages\n",
        "  if use_sns == True:\n",
        "\n",
        "    sns.set_theme(rc={\"figure.dpi\": 96}) # use higher resolution\n",
        "    # %config InlineBackend.figure_format = \"svg\"\n",
        "    sns.set(font_scale=0.8)\n",
        "    sns.heatmap(utils.to_numpy(percs_matrix), annot=True, mask=utils.to_numpy(mask_matrix), cmap=\"YlGnBu\", xticklabels=col_headings[1:], yticklabels=row_headings)\n",
        "    plt.show()\n",
        "  else:\n",
        "    # Display a \"pretty\" table in html for use in blog\n",
        "    print(cell_output.get_formatted_string(out_format=cfg.table_out_format))\n",
        "\n",
        "\n",
        "# Print a 2 by 2 matrix of notes.\n",
        "def print_u_fail_notes():\n",
        "  global ucfg\n",
        "  global mcfg\n",
        "\n",
        "  print(\"The most common failure pattern (with associated failure #) when each head or MLP in each position is ablated\")\n",
        "\n",
        "  cell_output = PrettyTable()\n",
        "  cell_output.field_names = get_column_headings()\n",
        "\n",
        "  for i in range((cfg.n_heads + 1) * cfg.n_layers):\n",
        "    datums = [get_row_heading(i)]\n",
        "    for j in range(ccfg.min_useful_position, ccfg.max_useful_position+1):\n",
        "      datums = datums + [ucfg.fail_notes[i][j]]\n",
        "    cell_output.add_row(datums)\n",
        "\n",
        "  print(cell_output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99LIC1l4MD32"
      },
      "source": [
        "# Part 14: Setup: Ablate each MLP in EACH position. Impact on Loss?\n",
        "Ablating the MLP in each layer in each position and seeing if the loss increases shows which head+layer+MLP are used by the algorithm. Use \"m_\" prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOxNc9SAMGRH"
      },
      "outputs": [],
      "source": [
        "class M_Config():\n",
        "  position : int = 0  # zero-based token-position to ablate\n",
        "  layer : int = 0 # zero-based layer to ablate. 0 to 1\n",
        "  threshold : float = 0.12\n",
        "  questions = varied_questions\n",
        "  output = PrettyTable()\n",
        "  hook_calls : int = 0\n",
        "\n",
        "\n",
        "mcfg = M_Config()\n",
        "\n",
        "\n",
        "def m_reset():\n",
        "  global mcfg\n",
        "\n",
        "  mcfg.output = PrettyTable()\n",
        "  mcfg.output.field_names = [\"Position\", \"MLP Layer\", \"% Fails\", \"% Fails by Case\", \"# Fails by Patterns\"]\n",
        "  mcfg.hook_calls = 0\n",
        "\n",
        "\n",
        "def m_mlp_hook_post(value, hook):\n",
        "  global mcfg\n",
        "\n",
        "  mcfg.hook_calls += 1\n",
        "  #print( \"In m_mlp_hook_post\", value.shape) # Get [1, 18, 2040] = ???, cfg.n_ctx, ???\n",
        "\n",
        "  # Mean ablate. Copy the mean resid post values in position N to the MLP\n",
        "  value[:,mcfg.position,:] =  mean_mlp_hook_post[:,mcfg.position,:].clone()\n",
        "\n",
        "\n",
        "def m_perform_core(show_all = False):\n",
        "  global mcfg\n",
        "\n",
        "  the_hook = [(l_mlp_hook_post_name[mcfg.layer], m_mlp_hook_post)]\n",
        "  loss_mean = predict_experiment_question(mcfg.questions, the_hook, mcfg.threshold)\n",
        "\n",
        "  num_fails = total_case_fails()\n",
        "  if show_all or (num_fails > 0):\n",
        "    perc_fails = round(100 * num_fails / mcfg.questions.shape[0])\n",
        "    (pattern_results, top_pattern) = get_pattern_fails()\n",
        "\n",
        "    mcfg.output.add_row([str(mcfg.position), str(mcfg.layer), perc_fails, get_case_fails(), pattern_results])\n",
        "\n",
        "    add_mlp_fail_perc( mcfg.position, mcfg.layer, perc_fails, top_pattern )\n",
        "\n",
        "\n",
        "def m_perform(all_cells):\n",
        "  global mcfg\n",
        "\n",
        "  if cfg.n_digits >= 5 :\n",
        "    ucfg_reset()\n",
        "    m_reset()\n",
        "    if all_cells:\n",
        "      for mcfg.position in range(cfg.n_ctx):\n",
        "        for mcfg.layer in range(cfg.n_layers):\n",
        "          m_perform_core()\n",
        "    else:\n",
        "      for useful_cell in useful_cells:\n",
        "        if not useful_cell.is_head:\n",
        "          mcfg.position = useful_cell.position\n",
        "          mcfg.layer = useful_cell.layer\n",
        "          m_perform_core()\n",
        "\n",
        "\n",
        "def m_print_results(title):\n",
        "    global mcfg\n",
        "\n",
        "    print_config()\n",
        "    print()\n",
        "    print(title, mcfg.questions.shape[0])\n",
        "    print(mcfg.output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKZF_B6OMIq3"
      },
      "source": [
        "# Part 15: Setup: Ablate EACH head in EACH position. Impact on Digit & Task Loss?\n",
        "Ablating each head in each layer in each position and seeing if the loss increases shows which position+layer+head are used by the algorithm. Use \"h_\" prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2EnisO6MMGQ"
      },
      "outputs": [],
      "source": [
        "class H_Config():\n",
        "  position : int = 0 # zero-based token position to ablate. 0 to say 17\n",
        "  layer : int = 0 # zero-based layer to ablate. 0 to 1\n",
        "  head : int = 0 # zero-based head to ablate. 0 to 2\n",
        "  threshold : float = 0.12\n",
        "  questions = varied_questions\n",
        "  output = PrettyTable()\n",
        "  hook_calls: int = 0\n",
        "\n",
        "\n",
        "hcfg = H_Config()\n",
        "\n",
        "\n",
        "def h_reset():\n",
        "  global hcfg\n",
        "\n",
        "  hcfg.output = PrettyTable()\n",
        "  hcfg.output.field_names = [\"Position\", \"Layer\", \"Head\", \"% Fails\", \"% Fails by Case\", \"# Fails by Impact\"]\n",
        "  hcfg.hook_calls = 0\n",
        "\n",
        "\n",
        "def h_set_attn_hook_z(value, hook):\n",
        "  global hcfg\n",
        "\n",
        "  hcfg.hook_calls += 1\n",
        "  # print( \"In h_set_attn_hook_z\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "\n",
        "  # Mean ablate. Copy the mean resid post values in position N to all the batch questions\n",
        "  value[:,hcfg.position,hcfg.head,:] = mean_attn_z[:,hcfg.position,hcfg.head,:].clone()\n",
        "\n",
        "\n",
        "def h_perform_core(show_all = False):\n",
        "  global hcfg\n",
        "\n",
        "  the_hook = [(l_attn_hook_z_name[hcfg.layer], h_set_attn_hook_z)]\n",
        "  loss_mean = predict_experiment_question(hcfg.questions, the_hook, hcfg.threshold)\n",
        "\n",
        "  num_fails = total_case_fails()\n",
        "  if show_all or (num_fails > 0):\n",
        "    perc_fails = round(100 * num_fails / hcfg.questions.shape[0])\n",
        "    (pattern_results, top_pattern) = get_pattern_fails()\n",
        "\n",
        "    hcfg.output.add_row([str(hcfg.position), str(hcfg.layer), str(hcfg.head), perc_fails, get_case_fails(), pattern_results])\n",
        "\n",
        "    add_head_fail_perc( hcfg.position, hcfg.layer, hcfg.head, perc_fails, top_pattern)\n",
        "\n",
        "  return num_fails\n",
        "\n",
        "\n",
        "def h_perform(all_cells):\n",
        "  global hcfg\n",
        "\n",
        "  if cfg.n_digits >= 5 :\n",
        "    h_reset()\n",
        "    if all_cells:\n",
        "      for hcfg.position in range(ccfg.min_useful_position, ccfg.max_useful_position+1):\n",
        "        for hcfg.layer in range(cfg.n_layers):\n",
        "          for hcfg.head in range(cfg.n_heads):\n",
        "            h_perform_core()\n",
        "    else:\n",
        "      for useful_cell in useful_cells:\n",
        "        if useful_cell.is_head:\n",
        "          hcfg.position = useful_cell.position\n",
        "          hcfg.layer = useful_cell.layer\n",
        "          hcfg.head = useful_cell.head\n",
        "          h_perform_core()\n",
        "\n",
        "\n",
        "def h_print_results(title, the_format=\"\"):\n",
        "  global hcfg\n",
        "\n",
        "  print_config()\n",
        "  print()\n",
        "  print(title, hcfg.questions.shape[0], \", #hook_calls=\", hcfg.hook_calls)\n",
        "\n",
        "  if the_format == \"\":\n",
        "    the_format = cfg.table_out_format\n",
        "  print(hcfg.output.get_formatted_string(out_format=the_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpkyhHRoMOSw"
      },
      "source": [
        "# Part 16: Calculate show cell matrixes\n",
        "\n",
        "Show the percentage failure rate (incorrect prediction) when individual Attention Heads and MLPs are ablated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Uluv96KMQJS"
      },
      "outputs": [],
      "source": [
        "def calc_cell_matrices(title, questions, all_cells):\n",
        "  global mcfg\n",
        "  global hcfg\n",
        "  global verbose\n",
        "\n",
        "  mcfg.questions = questions\n",
        "  m_perform(all_cells)\n",
        "  if verbose:\n",
        "    m_print_results(title)\n",
        "\n",
        "  hcfg.questions = questions\n",
        "  h_perform(all_cells)\n",
        "  if verbose:\n",
        "    h_print_results(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE64pXy1MRsw"
      },
      "outputs": [],
      "source": [
        "def print_cell_matrices():\n",
        "  global verbose\n",
        "\n",
        "  verbose = False\n",
        "\n",
        "  print_config()\n",
        "  print()\n",
        "  print_u_fail_percs()\n",
        "  print()\n",
        "  print_u_fail_notes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as9ot9RMMTAi"
      },
      "outputs": [],
      "source": [
        "def run_cell_matrices():\n",
        "  global calc_useful_cells\n",
        "\n",
        "  calc_useful_cells = True\n",
        "  calc_cell_matrices(\"Varied questions\", varied_questions, True)\n",
        "  calc_useful_cells = False\n",
        "\n",
        "  print_cell_matrices()\n",
        "\n",
        "\n",
        "#PQ temp\n",
        "#run_cell_matrices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN1S8LoDMXEC"
      },
      "source": [
        "# Part 17 - Case Analysis\n",
        "Just processing BA, MC, US9 questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utqnefc2MYxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fb294a-867d-40d5-86b4-ee9ebe8a40e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_digits= 5 n_heads= 3 n_layers= 2 n_ctx= 18 seed= 129000 n_training_steps= 30000\n",
            "\n",
            "The % failure rate when each head or MLP in each position is ablated, # failed heads = 0 , # failed mlps = 0\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "\n",
            "The most common failure pattern (with associated failure #) when each head or MLP in each position is ablated\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n"
          ]
        }
      ],
      "source": [
        "calc_cell_matrices(\"BA questions\", make_ba_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biUdemo5MZAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57ac9a6-a2a8-4729-ea26-f5bc29047d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_digits= 5 n_heads= 3 n_layers= 2 n_ctx= 18 seed= 129000 n_training_steps= 30000\n",
            "\n",
            "The % failure rate when each head or MLP in each position is ablated, # failed heads = 0 , # failed mlps = 0\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "\n",
            "The most common failure pattern (with associated failure #) when each head or MLP in each position is ablated\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n"
          ]
        }
      ],
      "source": [
        "calc_cell_matrices(\"UC1 questions\", make_uc1_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV1zYsCeMarq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e038bdff-33f2-44a7-87e1-74d27441fcf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_digits= 5 n_heads= 3 n_layers= 2 n_ctx= 18 seed= 129000 n_training_steps= 30000\n",
            "\n",
            "The % failure rate when each head or MLP in each position is ablated, # failed heads = 0 , # failed mlps = 0\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "\n",
            "The most common failure pattern (with associated failure #) when each head or MLP in each position is ablated\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n"
          ]
        }
      ],
      "source": [
        "calc_cell_matrices(\"Simple US9 questions\", make_simple_us9_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjLzcox9McGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d50380-e994-4d09-a41b-720c224d5b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_digits= 5 n_heads= 3 n_layers= 2 n_ctx= 18 seed= 129000 n_training_steps= 30000\n",
            "\n",
            "The % failure rate when each head or MLP in each position is ablated, # failed heads = 0 , # failed mlps = 0\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "\n",
            "The most common failure pattern (with associated failure #) when each head or MLP in each position is ablated\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| Position | P8 | P9 | P10 | P11 | P12 | P13 | P14 | P15 | P16 |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|   L0H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L0H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H0   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H1   |    |    |     |     |     |     |     |     |     |\n",
            "|   L1H2   |    |    |     |     |     |     |     |     |     |\n",
            "|   MLP    |    |    |     |     |     |     |     |     |     |\n",
            "+----------+----+----+-----+-----+-----+-----+-----+-----+-----+\n"
          ]
        }
      ],
      "source": [
        "calc_cell_matrices(\"Cascade US9 questions\", make_cascade_us9_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHSY3blNMe7I"
      },
      "source": [
        "#Part 18: SetUp: Calc and graph PCA decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCiBsiQAMhS_"
      },
      "outputs": [],
      "source": [
        "tn_questions = 100\n",
        "\n",
        "# These are n_digit addition questions where the first test_digits add up from 0 to 8\n",
        "# Randomise the last test_digits-1 digits of both numbers\n",
        "def make_t8_questions(test_digit):\n",
        "    limit = 10 ** test_digit\n",
        "    questions = []\n",
        "    for i in range(tn_questions):\n",
        "        x = random.randint(0, 8)\n",
        "        y = random.randint(0, 8-x)\n",
        "        x = x * limit + random.randint(0, limit-1)\n",
        "        y = y * limit + random.randint(0, limit-1)\n",
        "        questions.append([x, y])\n",
        "    return make_questions(questions)\n",
        "\n",
        "\n",
        "# These are n_digit addition questions where the first test_digits add up to 9\n",
        "# Randomise the last test_digits-1 digits of both numbers\n",
        "def make_t9_questions(test_digit):\n",
        "    limit = 10 ** test_digit\n",
        "    questions = []\n",
        "    for i in range(tn_questions):\n",
        "        x = random.randint(0, 9)\n",
        "        y = 9 - x\n",
        "        x = x * limit + random.randint(0, limit-1)\n",
        "        y = y * limit + random.randint(0, limit-1)\n",
        "        questions.append([x, y])\n",
        "    return make_questions(questions)\n",
        "\n",
        "\n",
        "# These are n_digit addition questions where the first test_digits add up to 10 to 18\n",
        "# Randomise the last test_digits-1 digits of both numbers\n",
        "def make_t10_questions(test_digit):\n",
        "    limit = 10 ** test_digit\n",
        "    questions = []\n",
        "    for i in range(tn_questions):\n",
        "        x = random.randint(1, 9)\n",
        "        y = random.randint(10-x, 9)\n",
        "        x = x * limit + random.randint(0, limit-1)\n",
        "        y = y * limit + random.randint(0, limit-1)\n",
        "        questions.append([x, y])\n",
        "    return make_questions(questions)\n",
        "\n",
        "\n",
        "def make_tricase_questions(test_digit):\n",
        "  q1 = make_t8_questions(test_digit)\n",
        "  q2 = make_t9_questions(test_digit)\n",
        "  q3 = make_t10_questions(test_digit)\n",
        "\n",
        "  questions = torch.vstack((q1, q2, q3))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWCPgoQZMjgc"
      },
      "outputs": [],
      "source": [
        "# Do one Principal Component Analysis\n",
        "def calc_tricase_pca(t_position, t_layer, t_head, t_digit):\n",
        "  global tn_questions\n",
        "\n",
        "  t_questions = make_tricase_questions(t_digit)\n",
        "  #print('Sample t8 question:', t_questions[0].tolist())\n",
        "  #print('Sample t9 question:', t_questions[tn_questions].tolist())\n",
        "  #print('Sample t10 question:', t_questions[2*tn_questions].tolist())\n",
        "\n",
        "  t_logits, t_cache = model.run_with_cache(t_questions)\n",
        "\n",
        "  # Gather attention patterns for all the (randomly chosen) questions\n",
        "  attention_outputs = []\n",
        "  for i in range(len(t_questions)):\n",
        "\n",
        "    # Output of individual heads, without final bias\n",
        "    attention_cache=t_cache[\"result\", t_layer, \"attn\"] # Output of individual heads, without final bias\n",
        "    attention_output=attention_cache[i]  # Shape [n_ctx, n_head, d_model]\n",
        "    attention_outputs.append(attention_output[t_position, t_head, :])\n",
        "\n",
        "  attn_outputs = torch.stack(attention_outputs, dim=0).cpu()\n",
        "\n",
        "  pca = PCA(n_components=6)\n",
        "  pca.fit(attn_outputs)\n",
        "  pca_attn_outputs = pca.transform(attn_outputs)\n",
        "\n",
        "  title = 'P' + str(t_position) + '.L' + str(t_layer) + '.H'+str(t_head) + ', A'+str(t_digit)\n",
        "\n",
        "  return (pca, pca_attn_outputs, title)\n",
        "\n",
        "\n",
        "# Plot one PCA scatter graph\n",
        "def graph_pca(pca, pca_attn_outputs, ax, title):\n",
        "  global tn_questions\n",
        "\n",
        "  ax.scatter(pca_attn_outputs[:tn_questions, 0], pca_attn_outputs[:tn_questions, 1], color='red', label='0-8') # t8 questions\n",
        "  ax.scatter(pca_attn_outputs[tn_questions:2*tn_questions, 0], pca_attn_outputs[tn_questions:2*tn_questions, 1], color='green', label='9') # t9 questions\n",
        "  ax.scatter(pca_attn_outputs[2*tn_questions:, 0], pca_attn_outputs[2*tn_questions:, 1], color='blue', label='10-18') # t10 questionsset\n",
        "\n",
        "  if title != \"\" :\n",
        "    ax.set_title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk2K3y7pMlQr"
      },
      "outputs": [],
      "source": [
        "# Graph the PCA of Sn.Ln.Hn's attention pattern, using T8, T9, T10 questions that differ in the An digit\n",
        "def add_one_pca_subplot(ax, t_position, t_layer, t_head, t_digit):\n",
        "  pca, pca_attn_outputs, title = calc_tricase_pca(t_position, t_layer, t_head, t_digit)\n",
        "  graph_pca( pca, pca_attn_outputs, ax, title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEF7MQqCMngv"
      },
      "outputs": [],
      "source": [
        "def save_plt_to_file( full_title ):\n",
        "  if cfg.save_graph_to_file:\n",
        "    filename = full_title.replace(\" \", \"_\").replace(\",\", \"\").replace(\":\", \"_\")  + '.png'\n",
        "    plt.savefig(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbiau9foMp3h"
      },
      "source": [
        "#Part 19: PCA decomposition tri-state results\n",
        "\n",
        "Plot attention heads in the positions 8 to 16 with a clear \"tri-state\" response to (exactly) one An."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ5fS3XNMs8e"
      },
      "outputs": [],
      "source": [
        "if cfg.n_digits == 5 and cfg.n_layers == 2 and use_pca :\n",
        "\n",
        "  # graph all useful early cells\n",
        "  fig, axs = plt.subplots(4, 2)\n",
        "  fig.set_figheight(8)\n",
        "  fig.set_figwidth(5)\n",
        "\n",
        "  # Plot all useful attention heads in the positions 8 to 12 with the clearest An selected\n",
        "  add_one_pca_subplot(axs[0, 0], 8, 0, 1, 2)    # S8.L0.H1 is clear only for A2\n",
        "  add_one_pca_subplot(axs[0, 1], 9, 0, 1, 1)    # S9.L0.H1 is clear only for A1\n",
        "  add_one_pca_subplot(axs[1, 0], 11, 0, 1, 3)   # S11.L0.H1 is clear only for A3\n",
        "  add_one_pca_subplot(axs[1, 1], 11, 0, 2, 4)   # S11.L0.H2 is clear only for A4\n",
        "  add_one_pca_subplot(axs[2, 0], 12, 0, 1, 3)   # S12.L0.H1 is clear only for A3\n",
        "  add_one_pca_subplot(axs[2, 1], 13, 0, 1, 2)   # S13.L0.H1 is clear only for A2\n",
        "  add_one_pca_subplot(axs[3, 0], 14, 0, 1, 1)   # S14.L0.H1 is clear only for A1\n",
        "\n",
        "  lines_labels = [axs[0,0].get_legend_handles_labels()]\n",
        "  lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "  fig.legend(lines, labels, loc='lower center', ncol=4)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file('PCA_Trigrams')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3611cpuEFhoW"
      },
      "source": [
        "#Part 19B: PCA decomposition bi-state results\n",
        "\n",
        "Plot attention heads in the positions 8 to 16 with a clear \"bi-state\" response to (exactly) one An."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAT8Z54oMuur"
      },
      "outputs": [],
      "source": [
        "if cfg.n_digits == 5 and cfg.n_layers == 2 and use_pca :\n",
        "\n",
        "  # graph all useful early cells\n",
        "  fig, axs = plt.subplots(1, 2)\n",
        "  fig.set_figheight(2)\n",
        "  fig.set_figwidth(5)\n",
        "\n",
        "  # Plot all useful attention heads in the positions 8 to 12 with the clearest An selected\n",
        "  add_one_pca_subplot(axs[0], 10, 0, 1, 0)   # S10.L0.H1 is clear only for A0\n",
        "  add_one_pca_subplot(axs[1], 15, 0, 1, 0)   # S15.L0.H1 is clear only for A0\n",
        "\n",
        "  lines_labels = [axs[0].get_legend_handles_labels()]\n",
        "  lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "  fig.legend(lines, labels, loc='lower center', ncol=4)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file('PCA_Bigrams')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQhbJTFTMwU_"
      },
      "outputs": [],
      "source": [
        "# Do one Principal Component Analysis and graph it\n",
        "def run_one_tricase_pca(t_position, t_layer, t_head, t_digit):\n",
        "\n",
        "  pca, pca_attn_outputs, title = calc_tricase_pca(t_position, t_layer, t_head, t_digit)\n",
        "\n",
        "  # Plot the PCA results\n",
        "  fig, ax = plt.subplots()\n",
        "  graph_pca(pca, pca_attn_outputs, ax, \"\")\n",
        "\n",
        "  full_title = 'PCA of attention: n_digits=' + str(cfg.n_digits) + ', ' + title\n",
        "  plt.title(full_title + ', EVR[0]=' + str(round(pca.explained_variance_ratio_[0],3)) )\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file(full_title)\n",
        "  plt.show()\n",
        "\n",
        "  print( \"First few principal components explain variance of:\", pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIu3Pr9CMx3l"
      },
      "source": [
        "#Part 19C: PCA decomposition of useful cells with digits 0 to 4\n",
        "\n",
        "Parts 19A and 19B are selective. This part is not. Use it to find (verify) the interesting parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdfpkXmAMzg4"
      },
      "outputs": [],
      "source": [
        "def graph_all_pca_results():\n",
        "\n",
        "  for useful_cell in useful_cells:\n",
        "      if useful_cell.is_head:\n",
        "        position = useful_cell.position\n",
        "        layer = useful_cell.layer\n",
        "        head = useful_cell.head\n",
        "        print( \"PCA: position=\", position, \"layer=\", layer, \"head=\", head)\n",
        "\n",
        "        fig, axs = plt.subplots(3, 2)\n",
        "\n",
        "        add_one_pca_subplot(axs[0, 0], position, layer, head, 0)\n",
        "        add_one_pca_subplot(axs[0, 1], position, layer, head, 1)\n",
        "        add_one_pca_subplot(axs[1, 0], position, layer, head, 2)\n",
        "        add_one_pca_subplot(axs[1, 1], position, layer, head, 3)\n",
        "        add_one_pca_subplot(axs[2, 0], position, layer, head, 4)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if use_pca :\n",
        "  graph_all_pca_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 20: Implement Mathematical framework\n",
        "\n",
        "Demonstrates that the mathematical framework (not the model) can do 1,000,000 additions without error."
      ],
      "metadata": {
        "id": "XO3kH-aQyM3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tri_case(dn,dnd):\n",
        "  s = dn + dnd\n",
        "  if s >= 10:\n",
        "    return 10\n",
        "  if s == 9:\n",
        "    return 9\n",
        "  return 8\n",
        "\n",
        "def tri_add(dn_cx, dm_cy):\n",
        "  if dn_cx == 10 or (dn_cx == 9 and dm_cy == 10):\n",
        "    return 10\n",
        "  if dn_cx == 8 and dm_cy == 10:\n",
        "    return 9\n",
        "  return 8\n",
        "\n",
        "def addition_psuedo_code( d4, d3, d2, d1, d0, d4d, d3d, d2d, d1d, d0d ):\n",
        "  # V2.C | TriCase(D2, D2’) | P8.L0.H1 and P8.L0.MLP\n",
        "  v2_c = tri_case(d2, d2d)\n",
        "\n",
        "  # V1.C | TriCase(D1, D1') | P9.L0.H1 and P9.L0.MLP\n",
        "  v1_c = tri_case(d1, d1d)\n",
        "\n",
        "  # V1.C2 | TriAdd(V1.C, TriCase(D0, D0’)) | P10.L0.H1 and P10.L0.MLP\n",
        "  v1_c2 =  tri_add( v1_c, tri_case(d0, d0d) )\n",
        "\n",
        "  # V3.C4 | TriAdd(TriCase(D3, D3’), TriAdd(V2.C,V1.C2)) | P11.L0.H1\n",
        "  v3_c4 = tri_add( tri_case(d3, d3d), tri_add(v2_c,v1_c2) )\n",
        "\n",
        "  # V4.C | TriCase(D4, D4’) | P11.L0.H2\n",
        "  v4_c = tri_case(d4, d4d)\n",
        "\n",
        "  # V4.C5 | TriAdd(V4.C, V3.C4) | P11.L0.MLP\n",
        "  v4_c5 = tri_add(v4_c, v3_c4)\n",
        "\n",
        "  # A5 | (V4.C5 == 10) | P11.L1.MLP\n",
        "  a5 = 1 if v4_c5 == 10 else 0\n",
        "\n",
        "  # V4.BA | (D4 + D4') % 10 | P12.L0.H0 + H2\n",
        "  v4_ba = (d4 + d4d) % 10\n",
        "\n",
        "  # V3.C4 | TriAdd(TriCase(D3, D3’), TriAdd(V2.C,V1.C2)) | P12.L0.H1\n",
        "  v3_c4 = tri_add( tri_case(d3, d3d), tri_add(v2_c, v1_c2) )\n",
        "\n",
        "  # A4 | (V4.BA + V3.C4 / 10) % 10 | P12.L0.MLP and P12.L1.MLP\n",
        "  a4 = (v4_ba + (v3_c4 // 10)) % 10\n",
        "\n",
        "  #V3.BA | (D3 + D3') % 10 | P13.L0.H0 + H2\n",
        "  v3_ba = (d3 + d3d) % 10\n",
        "\n",
        "  # V2.C3 | TriAdd(V2.C,V1.C2) | P13.L0.H1\n",
        "  v2_c3 = tri_add(v2_c, v1_c2)\n",
        "\n",
        "  # A3 | (V3.BA + V2.C3 / 10) % 10 | P13.L0.MLP and P13.L1.MLP\n",
        "  a3 = (v3_ba + (v2_c3 // 10)) % 10\n",
        "\n",
        "  # V2.BA | (D2 + D2') % 10 | P14.L0.H0 + H2\n",
        "  v2_ba = (d2 + d2d) % 10\n",
        "\n",
        "  # V1.C2 | Copy from P10 | P14.L0.H1\n",
        "  # skip\n",
        "\n",
        "  # A2 | (V2.BA + V1.C2 / 10) % 10 | P14.L0.MLP and P14.L1.MLP\n",
        "  a2 = (v2_ba + (v1_c2 // 10)) % 10\n",
        "\n",
        "  # V1.BA | (D1 + D1') % 10 | P15.L0.H0 + H2\n",
        "  v1_ba = (d1 + d1d) % 10\n",
        "\n",
        "  # D0.MC | (D0 + D0') // 10 | P15.L0.H1\n",
        "  v0_mc = (d0 + d0d) // 10\n",
        "\n",
        "  # A1 | (V1.BA + D0.MC) % 10 | P15.L0.MLP and P15.L1.MLP\n",
        "  a1 = (v1_ba + v0_mc) % 10\n",
        "\n",
        "  # A0 | (D0 + D0') % 10 | P16.L0.H0 + H2 P16.L0.MLP and P16.L1.MLP\n",
        "  a0 = (d0 + d0d) % 10\n",
        "\n",
        "  return a5, a4, a3, a2, a1, a0\n",
        "\n",
        "\n",
        "def do_addition_question(question):\n",
        "  if cfg.n_digits == 5:\n",
        "    d4 = int(question[0])\n",
        "    d3 = int(question[1])\n",
        "    d2 = int(question[2])\n",
        "    d1 = int(question[3])\n",
        "    d0 = int(question[4])\n",
        "    d4d = int(question[6])\n",
        "    d3d = int(question[7])\n",
        "    d2d = int(question[8])\n",
        "    d1d = int(question[9])\n",
        "    d0d = int(question[10])\n",
        "\n",
        "    a5, a4, a3, a2, a1, a0 = addition_psuedo_code( d4, d3, d2, d1, d0, d4d, d3d, d2d, d1d, d0d)\n",
        "\n",
        "    d = d4 * 10000 + d3 * 1000 + d2 * 100 + d1 * 10 + d0\n",
        "    dd = d4d * 10000 + d3d * 1000 + d2d * 100 + d1d * 10 + d0d\n",
        "    a = a5 * 100000 + a4 * 10000 + a3 * 1000 + a2 * 100 + a1 * 10 + a0\n",
        "\n",
        "    if d + dd != a :\n",
        "      print(d4, d3, d2, d1, d0, \"+\" ,d4d, d3d, d2d, d1d, d0d, \"=\", a5, a4, a3, a2, a1, a0 )\n",
        "      print(\"Bad addition:\", d, \"+\", dd, \"=\", a, \"Should be\", d+dd, \"Delta\", d+dd-a)\n",
        "      return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "OauytHdPyRyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_mathematical_framework():\n",
        "  if cfg.n_digits == 5:\n",
        "    num_successes = 0;\n",
        "    num_fails = 0\n",
        "\n",
        "    num_batches = 1000000//cfg.batch_size\n",
        "    for epoch in range(num_batches):\n",
        "      tokens, _, _, _, _, _ = next(ds)\n",
        "\n",
        "      for i in range(cfg.batch_size):\n",
        "        if not do_addition_question(tokens[i]):\n",
        "          num_fails += 1\n",
        "\n",
        "      if num_fails > 0:\n",
        "        break\n",
        "\n",
        "      num_successes += cfg.batch_size\n",
        "      if epoch % 250 == 0:\n",
        "          print(\"Batch\", epoch, \"of\", num_batches, \"#Successes=\", num_successes)\n",
        "\n",
        "    print(\"successes\", num_successes, \"num_fails\", num_fails)\n",
        "\n",
        "\n",
        "#verify_mathematical_framework()"
      ],
      "metadata": {
        "id": "nt7B0hdy-OZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 21A : Set Up Interchange Interventions\n",
        "\n",
        "Here we test our mapping of our mathematical framework (causual abstraction) to the model attention heads.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sgCog0mYkYPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class A_Config():\n",
        "  token_position : int = 15 # The token position we want to get/set. P8 to P11 contribute to A5 calculations\n",
        "  layer : int = 0 # The layer we want to get/set\n",
        "  heads = [] # The heads we want to get/set\n",
        "  threshold : int = 0.00001\n",
        "\n",
        "  hook_calls: int = 0\n",
        "  answer_failures : int = 0    # Failures of any digit\n",
        "\n",
        "  questions = []\n",
        "  store = []\n",
        "\n",
        "  null_hooks = []\n",
        "  get_hooks = []\n",
        "  put_hooks = []\n",
        "\n",
        "\n",
        "acfg = A_Config()"
      ],
      "metadata": {
        "id": "i8VoNc_ckfrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and put attention head value hooks\n",
        "\n",
        "def a_null_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  acfg.hook_calls += 1\n",
        "  #print(\"In a_null_attn_z_hook\", value.shape)  # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "\n",
        "\n",
        "def a_get_l0_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 0:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In a_get_l0_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "    acfg.store = value.clone()\n",
        "\n",
        "\n",
        "def a_get_l1_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 1:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In acfg.get_l1_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "    acfg.store = value.clone()\n",
        "\n",
        "\n",
        "def a_put_l0_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 0:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In a_l0_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "    for head_index in acfg.heads:\n",
        "      value[:,acfg.token_position,head_index,:] = acfg.store[:,acfg.token_position,head_index,:].clone()\n",
        "\n",
        "\n",
        "def a_put_l1_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 1:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In a_put_l1_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "    for head_index in acfg.heads:\n",
        "      value[:,acfg.token_position,head_index,:] = acfg.store[:,acfg.token_position,head_index,:].clone()\n",
        "\n",
        "\n",
        "def a_reset(token_position, layer, heads):\n",
        "  global acfg\n",
        "\n",
        "  acfg.token_position = token_position\n",
        "  acfg.layer = layer\n",
        "  acfg.heads = heads\n",
        "\n",
        "  acfg.hook_calls = 0\n",
        "  acfg.answer_failures = 0\n",
        "\n",
        "  acfg.null_hooks = [(l_attn_hook_z_name[0], a_null_attn_z_hook)]\n",
        "  acfg.get_hooks = [(l_attn_hook_z_name[0], a_get_l0_attn_z_hook),(l_attn_hook_z_name[1], a_get_l1_attn_z_hook)]\n",
        "  acfg.put_hooks = [(l_attn_hook_z_name[0], a_put_l0_attn_z_hook),(l_attn_hook_z_name[1], a_put_l1_attn_z_hook)]"
      ],
      "metadata": {
        "id": "mMdSybNpnU0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def a_predict_question(description, the_hooks, always):\n",
        "  global acfg\n",
        "  global model\n",
        "\n",
        "  acfg.hook_calls = 0\n",
        "  acfg.answer_failures = 0\n",
        "\n",
        "  for question_num in range(acfg.questions.shape[0]):\n",
        "    q = acfg.questions[question_num]\n",
        "\n",
        "    i = 12\n",
        "    a = q[i+0] * 100000 + q[i+1] * 10000 + q[i+2] * 1000 + q[i+3] * 100 + q[i+4] * 10 + q[i+5]\n",
        "\n",
        "    model.reset_hooks()\n",
        "    model.set_use_attn_result(True)\n",
        "    exp_logits = model.run_with_hooks(q.cuda(), return_type=\"logits\", fwd_hooks=the_hooks)\n",
        "\n",
        "    q_2d = q.unsqueeze(0)\n",
        "    exp_losses_raw, exp_max_indices = logits_to_tokens_loss(exp_logits, q_2d.cuda())\n",
        "    loss_mean = utils.to_numpy(loss_fn(exp_losses_raw).mean())\n",
        "\n",
        "    answer_str = prediction_to_string(exp_max_indices)\n",
        "\n",
        "    match_str = \"\"\n",
        "    if loss_mean > acfg.threshold:\n",
        "      acfg.answer_failures += 1\n",
        "      match_str = get_digit_accuracy_impact( a, answer_str )\n",
        "    if match_str == \"\":\n",
        "      match_str = \"(none)\"\n",
        "\n",
        "    if always or (loss_mean > acfg.threshold):\n",
        "      loss_str = \"(none)\" if loss_mean < 1e-7 else str(loss_mean)\n",
        "\n",
        "      print(description, \"  ModelPredicts:\", answer_str, \"  DigitsImpacted:\", match_str, \"  Loss:\", loss_str)\n"
      ],
      "metadata": {
        "id": "0SeIO06JnU7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def a_run_intervention_core(token_position, layer, heads, store_question, alter_question):\n",
        "  a_reset(token_position, layer, heads)\n",
        "\n",
        "  # Predict first question and store activation values (including the Vn.BA)\n",
        "  acfg.questions = make_questions([store_question])\n",
        "  a_predict_question(\"Unit test (null hook)\", acfg.null_hooks, False)\n",
        "  a_predict_question(\"Store activation\", acfg.get_hooks, False)\n",
        "\n",
        "  # Predict second question. Then rerun overriding Pn_Lm_Hp to give bad answer\n",
        "  acfg.questions = make_questions([alter_question])\n",
        "  a_predict_question(\"Unit test (null hook)\", acfg.null_hooks, False)\n",
        "  prompt = \"Intervening on P\" + str(token_position) + \".L\" + str(layer) + \".H\"\n",
        "  for head_index in acfg.heads:\n",
        "    prompt += str(head_index) + \",\"\n",
        "  a_predict_question(prompt, acfg.put_hooks, True)\n",
        "\n",
        "\n",
        "def a_run_intervention(description, token_position, layer, heads, store_question, alter_question):\n",
        "  if cfg.n_digits == 5 and cfg.n_layers == 2:\n",
        "    print(description)\n",
        "    a_run_intervention_core(token_position, layer, heads, store_question, alter_question)\n",
        "    print()"
      ],
      "metadata": {
        "id": "Jk0gVCF9Gr5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 21B : Run Interchange Interventions\n",
        "\n",
        "Here we test our mapping of our mathematical framework (casual abstraction) to the model attention heads.\n"
      ],
      "metadata": {
        "id": "7bbeIfUxvLzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P8.L0.H1 performs V2.C = TriCase(D2, D2’) impacting A4 and A5 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [44444, 55555] # Sum is 099999. V2 has no MC.\n",
        "alter_question = [11111, 11111] # Sum is 022222. V2 has no MC.\n",
        "a_run_intervention(\"No V2.MC: No impact expected\", 8, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [77711, 22711] # Sum is 100422. V2 has MC\n",
        "alter_question = [44444, 55555] # Sum is 099999. V2 has no MC\n",
        "a_run_intervention(\"Insert V2.MC: Expect A54 digit impacts. Expect 109999.\", 8, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [17711, 22711] # Sum is 035422. V2 has MC\n",
        "alter_question = [ 4444,  5555] # Sum is 009999. V2 has no MC\n",
        "a_run_intervention(\"Insert V2.MC: Expect A4 digit impact. Expect 019999.\", 8, 0, [1], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P8.L0.H1 is: Based on D2 and D2'. Triggers on a V2 carry value. Provides \"carry 1\" used in A5 and A4 calculation."
      ],
      "metadata": {
        "id": "sFLdZkmEHhIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd06dd64-5189-4d8e-f401-1ed9daa63e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P8.L0.H1 performs V2.C = TriCase(D2, D2’) impacting A4 and A5 accuracy\n",
            "\n",
            "No V2.MC: No impact expected\n",
            "Intervening on P8.L0.H1,   ModelPredicts: 022222   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Insert V2.MC: Expect A54 digit impacts. Expect 109999.\n",
            "Intervening on P8.L0.H1,   ModelPredicts: 109999   DigitsImpacted: A54   Loss: 5.299321775692151\n",
            "\n",
            "Insert V2.MC: Expect A4 digit impact. Expect 019999.\n",
            "Intervening on P8.L0.H1,   ModelPredicts: 019999   DigitsImpacted: A4   Loss: 3.2266855051444985\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P9.L0.H1 performs V1.C = TriCase(D1, D1’) impacting A5, A4 & A3 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [ 44444, 55555] # Sum is 099999. V1 has no MC.\n",
        "alter_question = [ 11111, 11111] # Sum is 022222. V1 has no MC\n",
        "a_run_intervention(\"No V1.MC: No impact expected\", 9, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [ 11171, 11171] # Sum is 022342. V1 has MC\n",
        "alter_question = [ 44444, 55555] # Sum is 099999. V1 has no MC.\n",
        "a_run_intervention(\"Insert V1.MC: Expect A543 digit impacts. Expect 100999.\", 9, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [ 11171, 11171] # Sum is 022342. V1 has MC\n",
        "alter_question = [  4444,  5555] # Sum is 009999. V1 has no MC\n",
        "a_run_intervention(\"Insert V1.MC: Expect A43 digit impacts. Expect 010999.\", 9, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [ 11171, 11171] # Sum is 022342. V1 has MC\n",
        "alter_question = [   444,   555] # Sum is 000999. V1 has no MC\n",
        "a_run_intervention(\"Insert V1.MC: Expect A3 digit impact. Expect 001999.\", 9, 0, [1], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P9.L0.H1 is: Based on D1 and D1'. Triggers on a V1 carry value. Provides \"carry 1\" used in A5, A4 & A3 calculation."
      ],
      "metadata": {
        "id": "dNtG_mlXeZKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8162172d-7301-416d-9483-85e08b95d8c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P9.L0.H1 performs V1.C = TriCase(D1, D1’) impacting A5, A4 & A3 accuracy\n",
            "\n",
            "No V1.MC: No impact expected\n",
            "Intervening on P9.L0.H1,   ModelPredicts: 022222   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Insert V1.MC: Expect A543 digit impacts. Expect 100999.\n",
            "Intervening on P9.L0.H1,   ModelPredicts: 100999   DigitsImpacted: A543   Loss: 5.7620570817856755\n",
            "\n",
            "Insert V1.MC: Expect A43 digit impacts. Expect 010999.\n",
            "Intervening on P9.L0.H1,   ModelPredicts: 010999   DigitsImpacted: A43   Loss: 4.482176019303971\n",
            "\n",
            "Insert V1.MC: Expect A3 digit impact. Expect 001999.\n",
            "Intervening on P9.L0.H1,   ModelPredicts: 001999   DigitsImpacted: A3   Loss: 3.2911737233840377\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P10.L0.H1 performs V1.C2 = TriAdd(V1.C, TriCase(D0, D0’)) impacting A5, A4, A3 & A2 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [ 11111, 33333] # Sum is 044444. V0 has no MC.\n",
        "alter_question = [ 44444, 55555] # Sum is 099999. V0 has no MC\n",
        "a_run_intervention(\"No impact expected\", 10, 0, [1], store_question, alter_question)\n",
        "# Results: No impact as expected\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [ 44444, 55555] # Sum is 099999. V0 has no MC\n",
        "a_run_intervention(\"Insert D0.MC: Expect A5432 digit impacts. Expect 100099.\", 10, 0, [1], store_question, alter_question)\n",
        "# Results: Impact on A5432 as expected. Got expected value 100099\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [  4444,  5555] # Sum is 009999. V0 has no MC\n",
        "a_run_intervention(\"Insert D0.MC: Expect A432 digit impacts. Expect 010099.\", 10, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [   444,   555] # Sum is 000999. V0 has no MC\n",
        "a_run_intervention(\"Insert D0.MC: Expect A32 digit impacts. Expect 001099\", 10, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [ 11117, 11117] # Sum is 022234. V0 has MC\n",
        "alter_question = [    44,    55] # Sum is 000099. V0 has no MC\n",
        "a_run_intervention(\"Insert D0.MC Expect A2 digit impacts. Expect 000199\", 10, 0, [1], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P10.L0.H1 is: Based on D0 and D0'. Triggers on a V0 carry value. Provides \"carry 1\" used in A5, A4, A3 & A2 calculation."
      ],
      "metadata": {
        "id": "UDt2LmNwiMGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4d97da-0fab-46d7-f7f0-2502ba154968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P10.L0.H1 performs V1.C2 = TriAdd(V1.C, TriCase(D0, D0’)) impacting A5, A4, A3 & A2 accuracy\n",
            "\n",
            "No impact expected\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 099999   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Insert D0.MC: Expect A5432 digit impacts. Expect 100099.\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 100099   DigitsImpacted: A5432   Loss: 10.939795728734431\n",
            "\n",
            "Insert D0.MC: Expect A432 digit impacts. Expect 010099.\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 010099   DigitsImpacted: A432   Loss: 9.112591770139232\n",
            "\n",
            "Insert D0.MC: Expect A32 digit impacts. Expect 001099\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 001099   DigitsImpacted: A32   Loss: 6.3806896571810405\n",
            "\n",
            "Insert D0.MC Expect A2 digit impacts. Expect 000199\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 000199   DigitsImpacted: A2   Loss: 3.3800129445057565\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P11.L0.H1 performs V3.C4 = TriAdd(TriCase(D3, D3’),TriAdd(V2.C,V1.C2)) impacting A5 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [44444, 44444] # Sum is 088888. V3 sums to 8 (has no MC).\n",
        "alter_question = [11111, 11111] # Sum is 022222. V3 has no MC.\n",
        "a_run_intervention(\"No V3.MC: No impact expected\", 11, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [16111, 13111] # Sum is 032111. V3 sums to 9 (has no MC).\n",
        "alter_question = [44444, 55555] # Sum is 099999. V3 has no MC\n",
        "a_run_intervention(\"No V3.MC: No impact expected\", 11, 0, [1], store_question, alter_question)\n",
        "\n",
        "store_question = [16111, 16111] # Sum is 032111. V3 has MC\n",
        "alter_question = [44444, 55555] # Sum is 099999. V3 has no MC\n",
        "a_run_intervention(\"Insert V3.MC: Expect A5 digit impact. Expect 199999.\", 11, 0, [1], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P11.L0.H1 is: Based on D3 and D3'. Triggers on a V3 carry value. Provides \"carry 1\" used in A5 calculations."
      ],
      "metadata": {
        "id": "uxGCodzep7qV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5c88b8-9985-4131-ab2b-93e8093cd81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P11.L0.H1 performs V3.C4 = TriAdd(TriCase(D3, D3’),TriAdd(V2.C,V1.C2)) impacting A5 accuracy\n",
            "\n",
            "No V3.MC: No impact expected\n",
            "Intervening on P11.L0.H1,   ModelPredicts: 022222   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "No V3.MC: No impact expected\n",
            "Intervening on P11.L0.H1,   ModelPredicts: 099999   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Insert V3.MC: Expect A5 digit impact. Expect 199999.\n",
            "Intervening on P11.L0.H1,   ModelPredicts: 199999   DigitsImpacted: A5   Loss: 3.042900461498787\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P11.L0.H2 performs V4.C = TriCase(D4, D4’) impacting A5 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [44444, 55555] # Sum is 099999. V4 has no MC.\n",
        "alter_question = [11111, 11111] # Sum is 022222. V4 has no MC.\n",
        "a_run_intervention(\"No V4.MC: No impact expected\", 11, 0, [2], store_question, alter_question)\n",
        "\n",
        "store_question = [71111, 71111] # Sum is 100422. V4 has MC\n",
        "alter_question = [44444, 55555] # Sum is 099999. V4 has no MC\n",
        "a_run_intervention(\"Insert V4.MC: Expect A5 digit impact. Expect 199999.\", 11, 0, [2], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P9.L0.H2 is: Based on D4 and D4'. Triggers on a V4 carry value. Provides \"carry 1\" used in A5 calculation."
      ],
      "metadata": {
        "id": "yo55vCvCoMq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5c3bf2-7109-4feb-f4d3-d0fcb1b76455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P11.L0.H2 performs V4.C = TriCase(D4, D4’) impacting A5 accuracy\n",
            "\n",
            "No V4.MC: No impact expected\n",
            "Intervening on P11.L0.H2,   ModelPredicts: 022222   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Insert V4.MC: Expect A5 digit impact. Expect 199999.\n",
            "Intervening on P11.L0.H2,   ModelPredicts: 199999   DigitsImpacted: A5   Loss: 4.241032295606187\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P12.L0.H0 and H2 performs V4.BA = (D4 + D4’) % 10 impacting A4 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [72222, 71111] # Sum is 143333\n",
        "alter_question = [12342, 56573] # Sum is 068915\n",
        "a_run_intervention(\"Override D4/D4'. Expect A4 digit impact. Expect 048915\", 12, 0, [0,2], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P12.L0.H0+H2 is: Adds D4 and D4'. Impacts A4"
      ],
      "metadata": {
        "id": "_jSE5S9UQFpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458b428c-cf8a-4dba-8a54-b3d73279a9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P12.L0.H0 and H2 performs V4.BA = (D4 + D4’) % 10 impacting A4 accuracy\n",
            "\n",
            "Override D4/D4'. Expect A4 digit impact. Expect 048915\n",
            "Intervening on P12.L0.H0,2,   ModelPredicts: 048915   DigitsImpacted: A4   Loss: 5.231093744310888\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P13.L0.H0 and H2 performs V3.BA = (D3 + D3’) % 10 impacting A3 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [23222, 13111] # Sum is 36333\n",
        "alter_question = [12342, 56573] # Sum is 68915\n",
        "a_run_intervention(\"Override D3/D3'. Expect A3 digit impact. Expect 66915\", 13, 0, [0,2], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P13.L0.H0+H2 is: Adds D3 and D3'. Impacts A3"
      ],
      "metadata": {
        "id": "b6UpIJbfPG6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781b38df-f5f4-481f-d614-45fd58549a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P13.L0.H0 and H2 performs V3.BA = (D3 + D3’) % 10 impacting A3 accuracy\n",
            "\n",
            "Override D3/D3'. Expect A3 digit impact. Expect 66915\n",
            "Intervening on P13.L0.H0,2,   ModelPredicts: 066915   DigitsImpacted: A3   Loss: 5.7013734450862685\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P14.L0.H0 and H2 performs V2.BA = (D2 + D2’) % 10 impacting A2 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [22322, 11311] # Sum is 33633. No V1.MC\n",
        "alter_question = [12342, 56573] # Sum is 68915. Has V1.MC\n",
        "a_run_intervention(\"Override D2/D2'. Expect A2 digit impact. Expect 68715\", 14, 0, [0,2], store_question, alter_question)\n",
        "\n",
        "store_question = [22322, 11311] # Sum is 33633. No V1.MC\n",
        "alter_question = [12133, 56133] # Sum is 68266. No V1.MC\n",
        "a_run_intervention(\"Override D2/D2'. Expect A2 digit impact. Expect 68666\", 14, 0, [0,2], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P12.L0.H0 and H2 both impact A2, and together sum D2 and D2'"
      ],
      "metadata": {
        "id": "rXiOPyZ1ONcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40bdf73e-0b36-4c20-bfe2-dcfd3079c4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P14.L0.H0 and H2 performs V2.BA = (D2 + D2’) % 10 impacting A2 accuracy\n",
            "\n",
            "Override D2/D2'. Expect A2 digit impact. Expect 68715\n",
            "Intervening on P14.L0.H0,2,   ModelPredicts: 068715   DigitsImpacted: A2   Loss: 4.770342229769021\n",
            "\n",
            "Override D2/D2'. Expect A2 digit impact. Expect 68666\n",
            "Intervening on P14.L0.H0,2,   ModelPredicts: 068666   DigitsImpacted: A2   Loss: 3.592845763765798\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P14.L0.H1 calculates V1.C1 but also relies on P10.V1.C2, impacting A2 accuracy\")\n",
        "print()\n",
        "\n",
        "\n",
        "store_question = [55555, 44454] # Sum is 100009. Has V1.MC\n",
        "alter_question = [22222, 33333] # Sum is 055555. No V1.MC\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 055655\", 14, 0, [1], store_question, alter_question) # Get 055655. Correct\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 055655\", 10, 0, [1], store_question, alter_question) # Get 055555. No impact.\n",
        "\n",
        "store_question = [55590, 44490] # Sum is 100080. Has V1.MC\n",
        "alter_question = [12345, 54321] # Sum is 066666. No V1.MC\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 066766\", 14, 0, [1], store_question, alter_question) # Get 066766. Correct\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 066766\", 10, 0, [1], store_question, alter_question) # Get 066666. No impact.\n",
        "\n",
        "store_question = [12345, 54321] # Sum is 066666. No V1.MC\n",
        "alter_question = [55590, 44490] # Sum is 100080. Has V1.MC\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 100980\", 14, 0, [1], store_question, alter_question) # Get 100980. Correct\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 100980\", 10, 0, [1], store_question, alter_question) # Get 100080. No impact.\n",
        "\n",
        "# Above shows:\n",
        "# - P14.L0.H1 behaviour is different from P10.L0.H1 behaviour\n",
        "# - P14.L0.H1 does not simply copy P10.L0.H1 (although this would be a valid way to get perfect accuracy in A2)\n",
        "print()\n",
        "\n",
        "store_question = [12345, 54321] # Sum is 066666. No V1.MC\n",
        "alter_question = [55555, 44445] # Sum is 100000. Has V0.MC, V1.MC, V1.C2\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C2. Expect A2 digit impact. Expect 100900\", 14, 0, [1], store_question, alter_question) # Get 100900. Correct\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 099900\", 10, 0, [1], store_question, alter_question) # Get 099900. Correct\n",
        "\n",
        "store_question = [22222, 33333] # Sum is 055555. No V1.MC\n",
        "alter_question = [66663, 33337] # Sum is 100000. Has V0.MC, V1.MC, V1.C2\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C2. Expect A2 digit impact. Expect 100900\", 14, 0, [1], store_question, alter_question) # Get 100900. Correct\n",
        "a_run_intervention(\"Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 099900\", 10, 0, [1], store_question, alter_question) # Get 099900. Correct\n",
        "\n",
        "# Above shows:\n",
        "# - P14.L0.H1 does rely on P10.L0.H1 for V1.C2 information when V1.C != V1.C2\n",
        "# - P14.L0.H1 calculates V1.C information itself from D1+D1'.\n",
        "\n",
        "# Overall confirmed: P14.L0.H1 calculates V1.C1 but also relies on P10.V1.C2 when V1.C != V1.C2. Impacts A2"
      ],
      "metadata": {
        "id": "LAzB924mQkyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1511b912-f197-4f7f-c1d2-a1504a0553a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P14.L0.H1 calculates V1.C1 but relies on P10.L0.H1 when V1.C != V1.C2, impacting A2 accuracy\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 055655\n",
            "Intervening on P14.L0.H1,   ModelPredicts: 055655   DigitsImpacted: A2   Loss: 2.929285317710518\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 055655\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 055555   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 066766\n",
            "Intervening on P14.L0.H1,   ModelPredicts: 066766   DigitsImpacted: A2   Loss: 3.0677803987390844\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 066766\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 066666   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 100980\n",
            "Intervening on P14.L0.H1,   ModelPredicts: 100980   DigitsImpacted: A2   Loss: 3.2824652714725233\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 100980\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 100080   DigitsImpacted: (none)   Loss: (none)\n",
            "\n",
            "Override V1.MC impacting V1.C2. Expect A2 digit impact. Expect 100900\n",
            "Intervening on P14.L0.H1,   ModelPredicts: 100900   DigitsImpacted: A2   Loss: 2.58894202227927\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 099900\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 099900   DigitsImpacted: A5432   Loss: 11.422521588544665\n",
            "\n",
            "Override V1.MC impacting V1.C2. Expect A2 digit impact. Expect 100900\n",
            "Intervening on P14.L0.H1,   ModelPredicts: 100900   DigitsImpacted: A2   Loss: 1.5136926019651438\n",
            "\n",
            "Override V1.MC impacting V1.C1. Expect A2 digit impact. Expect 099900\n",
            "Intervening on P10.L0.H1,   ModelPredicts: 099900   DigitsImpacted: A5432   Loss: 11.315933517465169\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P15.L0.H0 and H2 performs V1.BA = (D1 + D1’) % 10 impacting A1 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [22242, 11141] # Sum is 33383. No V0.MC\n",
        "alter_question = [12322, 56523] # Sum is 68845. No V0.MC\n",
        "a_run_intervention(\"Override D1/D1'. Expect A1 digit impact. Expect 68885\", 15, 0, [0,2], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P15.L0.H0 and H2 both impact A1, and together sum D1 and D1'"
      ],
      "metadata": {
        "id": "jJiVBPZe6DUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf81c341-75ed-424e-b8e9-39141740c190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P15.L0.H0 and H2 performs V1.BA = (D1 + D1’) % 10 impacting A1 accuracy\n",
            "\n",
            "Override D1/D1'. Expect A1 digit impact. Expect 68885\n",
            "Intervening on P15.L0.H0,2,   ModelPredicts: 068885   DigitsImpacted: A1   Loss: 3.1104054715185985\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P15.L0.H1 performs V0.MC = (D0 + D0’) / 10 impacting A1 (A one) accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [22244, 11149] # Sum is 33393. Has V0.MC\n",
        "alter_question = [12342, 56513] # Sum is 68855. No V0.MC\n",
        "a_run_intervention(\"Override D0.MC. Expect A1 digit impact. Expect 68865\", 15, 0, [1], store_question, alter_question)\n",
        "\n",
        "# Now test counter-claim that an intervention where both questions do NOT generate a D0.MC has NO impact on A1\n",
        "store_question = [22242, 11141] # Sum is 33383. No V0.MC\n",
        "alter_question = [12342, 56523] # Sum is 68865. No V0.MC\n",
        "a_run_intervention(\"No impact expected\", 15, 0, [1], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P15.L0.H1: Triggers when D0 + D0' > 10. Impacts A1 digit by 1"
      ],
      "metadata": {
        "id": "AVrX9QmOnVCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab13ca29-9e21-4faa-fa86-e8cdf3c067a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P15.L0.H1 performs V0.MC = (D0 + D0’) / 10 impacting A1 (A one) accuracy\n",
            "\n",
            "Override D0.MC. Expect A1 digit impact. Expect 68865\n",
            "Intervening on P15.L0.H1,   ModelPredicts: 068865   DigitsImpacted: A1   Loss: 3.142588321866339\n",
            "\n",
            "No impact expected\n",
            "Intervening on P15.L0.H1,   ModelPredicts: 068865   DigitsImpacted: (none)   Loss: (none)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( \"Test claim that P16.L0.H0 and H2 performs D0.BA = (D0 + D0’) % 10 impacting A0 accuracy\")\n",
        "print()\n",
        "\n",
        "store_question = [22225, 11114] # Sum is 33339\n",
        "alter_question = [12342, 56563] # Sum is 68905\n",
        "a_run_intervention(\"Override D0/D0'. Expect A0 digit impact. Expect 68909\", 16, 0, [0,2], store_question, alter_question)\n",
        "\n",
        "store_question = [22228, 11119] # Sum is 33347\n",
        "alter_question = [12342, 56563] # Sum is 68905\n",
        "a_run_intervention(\"Override D0/D0'. Expect A0 digit impact. Expect 68907\", 16, 0, [0,2], store_question, alter_question)\n",
        "\n",
        "# Confirmed that P16.L0.H0 and H2 both impact A0, and together sum D0 and D0'"
      ],
      "metadata": {
        "id": "8gM8TpWeyysU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5c9a67-6f1d-4fef-d748-5cc422d012c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test claim that P16.L0.H0 and H2 performs D0.BA = (D0 + D0’) % 10 impacting A0 accuracy\n",
            "\n",
            "Override D0/D0'. Expect A0 digit impact. Expect 68909\n",
            "Intervening on P16.L0.H0,2,   ModelPredicts: 068909   DigitsImpacted: A0   Loss: 3.4497020070222266\n",
            "\n",
            "Override D0/D0'. Expect A0 digit impact. Expect 68907\n",
            "Intervening on P16.L0.H0,2,   ModelPredicts: 068907   DigitsImpacted: A0   Loss: 5.963881366152209\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 22: MLP Visualisation (incomplete, on-hold)"
      ],
      "metadata": {
        "id": "lTs6Cu55jB7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import einops\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "# number of questions in batch that generated sample_cache\n",
        "num_questions = 239\n",
        "\n",
        "\n",
        "def get_mlp_data(data_set_name):\n",
        "\n",
        "  data_set = sample_cache[data_set_name]\n",
        "  # print( data_set_name + \" shape\", data_set.shape) # 239, 18, 2040 = num_questions, n_ctx, d_mlp\n",
        "\n",
        "  raw_data = data_set[:,-3]\n",
        "  # print( \"raw_data shape\", raw_data.shape) # 239, 2040 = num_questions, d_mlp\n",
        "\n",
        "  answer = einops.rearrange(raw_data, \"(x y) d_mlp -> x y d_mlp\", x=num_questions).cpu().numpy()\n",
        "  # print( \"answer shape\", answer.shape) # 239, 1, 2040 = num_questions, ??, d_mlp\n",
        "\n",
        "  return answer\n",
        "\n",
        "\n",
        "l0_mlp_hook_pre_sq = get_mlp_data('blocks.0.mlp.hook_pre')\n",
        "l0_mlp_hook_post_sq = get_mlp_data('blocks.0.mlp.hook_post')\n",
        "l1_mlp_hook_pre_sq = get_mlp_data('blocks.0.mlp.hook_pre')\n",
        "l1_mlp_hook_post_sq = get_mlp_data('blocks.1.mlp.hook_post')\n",
        "\n",
        "\n",
        "def plot_mlp_neuron_activation(pos: int):\n",
        "    clear_output()\n",
        "\n",
        "    l0_mlp_pre_data = l0_mlp_hook_pre_sq[:,:,pos]\n",
        "    l0_mlp_post_data = l0_mlp_hook_post_sq[:,:,pos]\n",
        "    l1_mlp_pre_data = l1_mlp_hook_pre_sq[:,:,pos]\n",
        "    l1_mlp_post_data = l1_mlp_hook_post_sq[:,:,pos]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "    plot = axs[0].imshow(l1_mlp_pre_data, cmap='magma', vmin=0, vmax=1)\n",
        "    cbar = plt.colorbar(plot, fraction=0.1)\n",
        "    cbar.set_label(r'l0_mlp_pre_data {}'.format(pos))\n",
        "    #axs[0].set_ylim(-0.5, 99.5)\n",
        "    #axs[0].set_yticks(range(100), labels=range(100), size=5.5);\n",
        "    #axs[0].set_xticks(range(100), labels=range(100), size=5.5, rotation='vertical');\n",
        "\n",
        "    plot = axs[1].imshow(l1_mlp_post_data, cmap='magma', vmin=0, vmax=1)\n",
        "    cbar = plt.colorbar(plot, fraction=0.1)\n",
        "    cbar.set_label(r'l0_mlp_post_data {}'.format(pos))\n",
        "    #axs[0].set_ylim(-0.5, 99.5)\n",
        "    #axs[0].set_yticks(range(100), labels=range(100), size=5.5);\n",
        "    #axs[0].set_xticks(range(100), labels=range(100), size=5.5, rotation='vertical');\n",
        "\n",
        "\n",
        "interact(plot_mlp_neuron_activation, pos=widgets.IntText(value=0, description='Index:'))"
      ],
      "metadata": {
        "id": "YviVHnBRjDcs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a05adae7540b4ae08fe29010fa14effe",
            "381872007ed84ec6ba5e0a1930cc7809",
            "d49633e87154435bb553b535600fa12c",
            "ed557d2afee94529a2d0a24417e71f69",
            "4e5a703eaf8f43dcb7a115d0ba55565c",
            "61f4b98c2897473cb421c7876c86de25",
            "d445185a976e4082a6555df9440180df"
          ]
        },
        "outputId": "102876d1-bb31-409f-e384-40a3c25d1cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(IntText(value=0, description='Index:'), Output()), _dom_classes=('widget-interact',))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a05adae7540b4ae08fe29010fa14effe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.plot_mlp_neuron_activation(pos: int)>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ldGPkaokJQM5",
        "pTd3nmsMJV5T",
        "P8RfHXneJw6n",
        "ZHiJhch4KCej",
        "-KJhCxFtNKfm",
        "fGxoBWHNKRf0",
        "Lag8C3d_KkeQ",
        "FyK7QeUjLLFm",
        "ZmsGWUbILYin",
        "904WBkTOLg_5",
        "99LIC1l4MD32",
        "pKZF_B6OMIq3",
        "VpkyhHRoMOSw",
        "mN1S8LoDMXEC",
        "fHSY3blNMe7I",
        "Rbiau9foMp3h",
        "3611cpuEFhoW",
        "jIu3Pr9CMx3l",
        "XO3kH-aQyM3U",
        "sgCog0mYkYPV",
        "lTs6Cu55jB7y"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a05adae7540b4ae08fe29010fa14effe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_381872007ed84ec6ba5e0a1930cc7809",
              "IPY_MODEL_d49633e87154435bb553b535600fa12c"
            ],
            "layout": "IPY_MODEL_ed557d2afee94529a2d0a24417e71f69"
          }
        },
        "381872007ed84ec6ba5e0a1930cc7809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntTextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntTextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntTextView",
            "continuous_update": false,
            "description": "Index:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4e5a703eaf8f43dcb7a115d0ba55565c",
            "step": 1,
            "style": "IPY_MODEL_61f4b98c2897473cb421c7876c86de25",
            "value": 0
          }
        },
        "d49633e87154435bb553b535600fa12c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d445185a976e4082a6555df9440180df",
            "msg_id": "",
            "outputs": []
          }
        },
        "ed557d2afee94529a2d0a24417e71f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e5a703eaf8f43dcb7a115d0ba55565c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f4b98c2897473cb421c7876c86de25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d445185a976e4082a6555df9440180df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}