{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3OoV2Pd-w7g"
      },
      "source": [
        "# Accurate Integer Multiplication in Transformers - Train the Model\n",
        "\n",
        "This CoLab defines, trains and analyses a Transformer model that performs integer addition, subtraction and multiplication e.g. 33357+82243=115600, 012345-034567=-012323 and 00345*00123=007935. Each digit is a separate token. For 5 digit questions, the model is given 12 \"question\" (input) tokens, and must then predict the corresponding 6 \"answer\" (output) tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtVn4muC-5p1"
      },
      "source": [
        "This CoLab trains the model, storing the results to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWXJvUUb-6in"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rl41loM_Apt"
      },
      "source": [
        "# Part 1: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z1TVBObuOKF"
      },
      "outputs": [],
      "source": [
        "# Tokens used in vocab. (Token indexes 0 to 9 represent digits 0 to 9)\n",
        "PLUS_INDEX = 10\n",
        "MINUS_INDEX = 11\n",
        "EQUALS_INDEX = 12\n",
        "MULT_INDEX = 13\n",
        "DIV_INDEX = 14\n",
        "MAX_INDEX = DIV_INDEX\n",
        "\n",
        "class Config():\n",
        "  #@markdown Model\n",
        "  n_layers: int = 3 #@param\n",
        "  n_heads: int = 4 #@param\n",
        "\n",
        "  d_vocab: int = MAX_INDEX+1\n",
        "  d_model: int = ( 512 // n_heads ) * n_heads # About 512, and divisible by n_heads\n",
        "  d_mlp: int = 4 * d_model\n",
        "  d_head: int = d_model // n_heads  # About 170 when n_heads == 3\n",
        "  seed: int = 129000 #@param\n",
        "\n",
        "  #@markdown Data\n",
        "  n_digits: int = 5 #@param\n",
        "  n_ctx: int = 3 * n_digits + 3\n",
        "  act_fn: str = 'relu'\n",
        "  batch_size: int = 64 #@param\n",
        "\n",
        "  #@markdown Optimizer\n",
        "  n_training_steps: int = 40000 #@param\n",
        "  lr: float = 0.00008 #@param\n",
        "  weight_decay: int = 0.1 #@param\n",
        "\n",
        "  # Save graphs to CoLab temp files as PDF and HTML. Can manually export files for re-use in papers.\n",
        "  save_graph_to_file: bool = True\n",
        "\n",
        "  #@markdown Actions\n",
        "\n",
        "  # Percent of questions that are multiplication and subtraction. Rest are addition.\n",
        "  perc_mult: int = 100 #@param\n",
        "  perc_sub: int = 0 #@param\n",
        "\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5K2kA0L_FHc"
      },
      "source": [
        "# Part 2: Import libraries\n",
        "Imports standard libraries. Will ask for access to your Google to write model weightings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS7C7n1bubkW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViLbdGWRudlt",
        "outputId": "fe2942d0-52e8-4704-be00-d8594db77745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "model will save to /content/drive/MyDrive/AI/CoLabOutput/mult_D5_L3_H4_dmodel512_dhead128_ctx18_seed129000_train40K.pt\n"
          ]
        }
      ],
      "source": [
        "# Training saves the trained model weights to a file in your Google Drive. You will need to give permission for this CoLab to access your Google Drive.\n",
        "# Loading loads the model from your Google Drive. Avoids the say 10mins spent on training the model.\n",
        "\n",
        "GLOBAL=True\n",
        "if GLOBAL:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    rootdir=Path('/content/drive/MyDrive/AI/CoLabOutput/')\n",
        "else:\n",
        "    rootdir=Path('./')\n",
        "\n",
        "if cfg.perc_mult == 100:\n",
        "  full_fname = 'mult'\n",
        "else:\n",
        "  if cfg.perc_sub == 100:\n",
        "    full_fname = 'sub'\n",
        "  else:\n",
        "    if cfg.perc_mult == 0 and cfg.perc_sub == 0:\n",
        "      full_fname = 'add'\n",
        "    else:\n",
        "      full_fname = 'mix'\n",
        "\n",
        "epochs = str(cfg.n_training_steps//1000) + \"K\"\n",
        "full_fname += '_D{}_L{}_H{}_dmodel{}_dhead{}_ctx{}_seed{}_train{}.pt'.format(cfg.n_digits, cfg.n_layers, cfg.n_heads, cfg.d_model, cfg.d_head, cfg.n_ctx, cfg.seed, epochs)\n",
        "model_save_location = rootdir/f'{full_fname}'\n",
        "\n",
        "print('model will save to {}'.format(str(model_save_location)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBOAqd0ZuhAV",
        "outputId": "8a055a1e-df45-44f8-dc0b-aac9e6f17cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.25.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.15.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.24)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.0)\n",
            "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.35.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.5.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.16.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.9.1)\n",
            "Requirement already satisfied: typeguard<3,>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (0.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.40)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.39.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.10/dist-packages (1.43.2)\n",
            "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (1.26.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.1.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->circuitsvis) (12.3.101)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.1.0->circuitsvis) (3.13.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->circuitsvis) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->circuitsvis) (1.3.0)\n",
            "Requirement already satisfied: torchtyping in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.1.1)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from torchtyping) (2.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->torchtyping) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->torchtyping) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    %pip install kaleido\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis\n",
        "    %pip install torchtyping\n",
        "    %pip install transformers\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "295MWK2owFNa",
        "outputId": "5ac48c09-24e2-46e6-e3bd-8136c593bfc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using renderer: colab\n"
          ]
        }
      ],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h1MGZnAwHZz"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJpHPxqhwMeQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import circuitsvis as cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS-PO-BPwo8f"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tExv4rk_L0C"
      },
      "source": [
        "# Part 3: Create model\n",
        "This section defines the token embedding / unembedding and creates the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqGmiutQwp22"
      },
      "outputs": [],
      "source": [
        "# Embedding / Unembedding\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    tokens = utils.to_numpy(tokens)\n",
        "    x = \"\".join([str(i) for i in tokens[:cfg.n_digits]])\n",
        "    y = \"\".join([str(i) for i in tokens[cfg.n_digits+1:cfg.n_digits*2+1]])\n",
        "    z = \"\".join([str(i) for i in tokens[cfg.n_ctx-cfg.n_digits-1:]])\n",
        "    equals = \"=\"\n",
        "    operator = \"+\"\n",
        "    return f\"{x}{operator}{y}{equals}{z}\"\n",
        "\n",
        "def string_to_tokens(string, batch: bool=False):\n",
        "    lookup = {str(i):i for i in range(10)}\n",
        "    lookup['+']=PLUS_INDEX\n",
        "    lookup['-']=MINUS_INDEX\n",
        "    lookup['=']=EQUALS_INDEX\n",
        "    lookup['*']=MULT_INDEX\n",
        "    lookup['\\\\']=DIV_INDEX\n",
        "\n",
        "    tokens = [lookup[i] for i in string if i not in '\\n ']\n",
        "    if batch:\n",
        "        return torch.tensor(tokens)[None, :]\n",
        "    else:\n",
        "        return torch.tensor(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg6uwYzew4u5"
      },
      "outputs": [],
      "source": [
        "# Transformer creation\n",
        "\n",
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = HookedTransformerConfig(\n",
        "    n_layers = cfg.n_layers,\n",
        "    n_heads = cfg.n_heads,\n",
        "    d_model = cfg.d_model,\n",
        "    d_head = cfg.d_head,\n",
        "    d_mlp = cfg.d_mlp,\n",
        "    act_fn = cfg.act_fn,\n",
        "    normalization_type = 'LN',\n",
        "    d_vocab = cfg.d_vocab,\n",
        "    d_vocab_out = cfg.d_vocab,\n",
        "    n_ctx = cfg.n_ctx,\n",
        "    init_weights = True,\n",
        "    device = \"cuda\",\n",
        "    seed = cfg.seed,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(ht_cfg)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(),\n",
        "                        lr = cfg.lr,\n",
        "                        weight_decay = cfg.weight_decay,\n",
        "                        betas = (0.9, 0.98))\n",
        "\n",
        "max_iter = cfg.n_training_steps\n",
        "warmup_iter = max_iter // 5\n",
        "scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=int(warmup_iter))\n",
        "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(np.ceil((max_iter-warmup_iter))))\n",
        "scheduler  = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[int(warmup_iter)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg267eav_QSl"
      },
      "source": [
        "# Part 4: Data Generator\n",
        "This section defines the loss function and the training/testing data generator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIVxDpq0w7WY"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "\n",
        "# Calculate the per-token probability by comparing a batch of prediction \"logits\" to answer \"tokens\"\n",
        "def logits_to_tokens_loss(logits, tokens):\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the addition answer probabilities\n",
        "  ans_logits = logits[:, -(cfg.n_digits+2):-1]\n",
        "\n",
        "  # Convert raw score (logits) vector into a probability distribution.\n",
        "  # Emphasize the largest scores and suppress the smaller ones, to make them more distinguishable.\n",
        "  ans_probs = F.log_softmax(ans_logits.to(torch.float64), dim=-1)\n",
        "\n",
        "  max_indices = torch.argmax(ans_probs, dim=-1)\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the model’s answer.\n",
        "  ans_tokens = tokens[:, -(cfg.n_digits+1):]\n",
        "\n",
        "  # Extract values from the ans_probs tensor, based on indices from the ans_tokens tensor\n",
        "  ans_loss = torch.gather(ans_probs, -1, ans_tokens[:, :, None])[..., 0]\n",
        "\n",
        "  return ans_loss, max_indices\n",
        "\n",
        "# Calculate loss as negative of average per-token mean probability\n",
        "def loss_fn(ans_loss):\n",
        "  return -ans_loss.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dps1g8spw-U4"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" data generator function. Invoked using next().\n",
        "# \"Addition\" batch entries are formated XXXXX+YYYYY=ZZZZZZ e.g. 55003+80002=135005\n",
        "# \"Subtraction\" batch entries are formated XXXXX-YYYYY=ZZZZZZ e.g. 55003-80002=-24999, 80002-55003=024999\n",
        "# \"Multiplication\" batch entries are formated 00XXX*00YYY=ZZZZZZ e.g. 00123*00784=096432\n",
        "def data_generator( ):\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    while True:\n",
        "        #generate a batch of questions (answers calculated below)\n",
        "        batch = torch.zeros((cfg.batch_size, cfg.n_ctx)).to(torch.int64)\n",
        "        x = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "        y = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "\n",
        "        batch_rand = random.randint(1, 100)\n",
        "        batch_op = MULT_INDEX if batch_rand <= cfg.perc_mult else MINUS_INDEX if batch_rand <= cfg.perc_mult + cfg.perc_sub else PLUS_INDEX\n",
        "\n",
        "        if batch_op == MULT_INDEX:\n",
        "          # Convert from NNNNN*NNNNN= to 00NNN*00NNN=\n",
        "          x[:, 0] = 0\n",
        "          x[:, 1] = 0\n",
        "          y[:, 0] = 0\n",
        "          y[:, 1] = 0\n",
        "\n",
        "        batch[:, :cfg.n_digits] = x\n",
        "        batch[:, cfg.n_digits] = batch_op\n",
        "        batch[:, 1+cfg.n_digits:1+cfg.n_digits*2] = y\n",
        "        batch[:, 1+cfg.n_digits*2] = EQUALS_INDEX\n",
        "\n",
        "        # Convert each row into a 5-digit number\n",
        "        x_values = x[:, 0] * 10000 + x[:, 1] * 1000 + x[:, 2] * 100 + x[:, 3] * 10 + x[:, 4]\n",
        "        y_values = y[:, 0] * 10000 + y[:, 1] * 1000 + y[:, 2] * 100 + y[:, 3] * 10 + y[:, 4]\n",
        "\n",
        "        # Elementwise operations giving 1D tensor\n",
        "        if batch_op == MULT_INDEX:\n",
        "          answers = x_values * y_values\n",
        "        else:\n",
        "          if batch_op == MINUS_INDEX:\n",
        "            answers = x_values - y_values\n",
        "          else:\n",
        "            answers = x_values + y_values\n",
        "\n",
        "        for i in range(cfg.batch_size):\n",
        "          answer = answers[i]\n",
        "\n",
        "          if answer < 0:\n",
        "            batch[:, 2+cfg.n_digits*2] = MINUS_INDEX\n",
        "            answer = - answer\n",
        "\n",
        "          for j in range(cfg.n_digits+1):\n",
        "            batch[i, cfg.n_ctx-j-1] = answer % 10\n",
        "            answer = answer // 10\n",
        "            if answer == 0:\n",
        "                break\n",
        "\n",
        "        yield batch.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH_rXfA2xAIG",
        "outputId": "7d419055-3c00-476c-b56e-650aa37aee81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  0,  4,  4,  4, 13,  0,  0,  5,  7,  1, 12,  2,  5,  3,  5,  2,  4],\n",
            "        [ 0,  0,  7,  8,  3, 13,  0,  0,  8,  8,  7, 12,  6,  9,  4,  5,  2,  1],\n",
            "        [ 0,  0,  9,  9,  9, 13,  0,  0,  2,  2,  8, 12,  2,  2,  7,  7,  7,  2],\n",
            "        [ 0,  0,  4,  9,  8, 13,  0,  0,  2,  6,  3, 12,  1,  3,  0,  9,  7,  4],\n",
            "        [ 0,  0,  8,  3,  4, 13,  0,  0,  2,  8,  0, 12,  2,  3,  3,  5,  2,  0]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "ds = data_generator()\n",
        "\n",
        "tokens = next(ds)\n",
        "\n",
        "print(tokens[:5,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JDY_Fqa_Wfb"
      },
      "source": [
        "# Part 5: Train multiplication model with Infinite Data\n",
        "Train model for n_training_steps, storing train_losses per epoch.\n",
        "\n",
        "Each training step (of n_training_steps) new training data (a batch of batch_size tokens) is generated and the model is trained and loss calculated on it. No separate \"testing\" data is needed, as the training data is unique each step. Memorisation of past training data by the model (if any) is minimally beneficial. For 5 digit addition there are 10 billion possible questions, and model training is on ~2 million questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csyjY8yExGr4"
      },
      "outputs": [],
      "source": [
        "def print_config():\n",
        "  print(\"%Mult=\", cfg.perc_mult, \"%Sub=\", cfg.perc_sub, \"digits=\", cfg.n_digits, \"heads=\", cfg.n_heads, \"layers=\", cfg.n_layers, \"ctx=\", cfg.n_ctx, \"seed=\", cfg.seed, \"training_steps=\", cfg.n_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the data generator\n",
        "ds = data_generator()"
      ],
      "metadata": {
        "id": "BH4NyGeenvna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0a7674624f9e465c86a11d3ee3423ac0",
            "d19f6555ea7346ac824ef0bc2aa7e050",
            "5b9394862a484f178abab6e71b87ac4f",
            "83327ca4a9434bb481090a3d90516e71",
            "a63db403cef64c5e97f4b08626780692",
            "91076e7c1aa5429a9ea306c9bdba340e",
            "2f6acad83dfc4938beec6bd6579c7ed4",
            "28968bd77bdf4e8daf437b33e107380c",
            "e97e3130a0ce4cefaf474f553dfd29ff",
            "b53d987665664a25a2b391ea7b3f7509",
            "c6f7e216b4084cbe9d31c9d67922ca5b"
          ]
        },
        "id": "UJ61_nfKxI9c",
        "outputId": "c2fa16c4-5375-4d81-f70b-285b4b0be6b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/40000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a7674624f9e465c86a11d3ee3423ac0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.7543175178955934\n",
            "100 2.3049452856905663\n",
            "200 2.1979681145449157\n",
            "300 2.1967886171802795\n",
            "400 2.138759447254304\n",
            "500 2.0609740325318753\n",
            "600 1.9511289317281186\n",
            "700 1.8498127050717497\n",
            "800 1.735269290898528\n",
            "900 1.6297449144281768\n",
            "1000 1.5749387613188681\n",
            "1100 1.552785613323952\n",
            "1200 1.540126557594336\n",
            "1300 1.4857083509574693\n",
            "1400 1.4404442771071877\n",
            "1500 1.4835853444005953\n",
            "1600 1.4319663233426811\n",
            "1700 1.4252510732850872\n",
            "1800 1.407832119758263\n",
            "1900 1.4085540245295096\n",
            "2000 1.3605676959473947\n",
            "2100 1.3913708242387024\n",
            "2200 1.3451829431017348\n",
            "2300 1.3294406100821443\n",
            "2400 1.3716071842401356\n",
            "2500 1.3597743599780094\n",
            "2600 1.2566654792113074\n",
            "2700 1.3522836419213573\n",
            "2800 1.3529823052027494\n",
            "2900 1.2778078765775644\n",
            "3000 1.2881200801630868\n",
            "3100 1.2944332553691984\n",
            "3200 1.2646078416896942\n",
            "3300 1.2717859478430804\n",
            "3400 1.2408022278616695\n",
            "3500 1.223415434440483\n",
            "3600 1.2674888689560606\n",
            "3700 1.2738858564401467\n",
            "3800 1.207969567860267\n",
            "3900 1.30014140115737\n",
            "4000 1.2543828101359016\n",
            "4100 1.1741731942198244\n",
            "4200 1.1740943540981688\n",
            "4300 1.2229779535174496\n",
            "4400 1.1597599040384274\n",
            "4500 1.1576599972994113\n",
            "4600 1.161103431009439\n",
            "4700 1.2406847141432693\n",
            "4800 1.1082946324440794\n",
            "4900 1.1575541412211452\n",
            "5000 1.16479150649897\n",
            "5100 1.0927917661816449\n",
            "5200 1.1284299072713067\n",
            "5300 1.117021215655121\n",
            "5400 1.0477094811794294\n",
            "5500 1.1026975384896294\n",
            "5600 1.062900416925032\n",
            "5700 1.1021927956133488\n",
            "5800 1.0722496738963143\n",
            "5900 1.064689222890026\n",
            "6000 1.0056673967687673\n",
            "6100 1.0202248010927586\n",
            "6200 0.9907960714322734\n",
            "6300 1.0297238230676569\n",
            "6400 1.0725418203288648\n",
            "6500 1.0077038353900232\n",
            "6600 1.0086082381194905\n",
            "6700 0.982702487525472\n",
            "6800 0.9814562623920775\n",
            "6900 0.9577374760146309\n",
            "7000 0.9848546183462004\n",
            "7100 0.9718326753262958\n",
            "7200 0.9424490865673427\n",
            "7300 0.9759040814956712\n",
            "7400 0.9533380552299313\n",
            "7500 0.9243780303264854\n",
            "7600 0.9210015917276767\n",
            "7700 0.9514810696806789\n",
            "7800 0.9779198413244823\n",
            "7900 0.925749951689613\n",
            "8000 0.9384998050348315\n",
            "8100 0.9085260837827317\n",
            "8200 0.869505762541619\n",
            "8300 0.9136085885270605\n",
            "8400 0.9122929322440126\n",
            "8500 0.8785968433753153\n",
            "8600 0.8602123617754123\n",
            "8700 0.8696497667612029\n",
            "8800 0.7935228864370539\n",
            "8900 0.84834778714731\n",
            "9000 0.8605668441542216\n",
            "9100 0.8795293660368144\n",
            "9200 0.833210039286495\n",
            "9300 0.7679330129807376\n",
            "9400 0.7743475517095467\n",
            "9500 0.8314867311132579\n",
            "9600 0.7714801583560729\n",
            "9700 0.7514586232678118\n",
            "9800 0.7287890800660409\n",
            "9900 0.7536336875220626\n",
            "10000 0.7862547663539751\n",
            "10100 0.7255715385886884\n",
            "10200 0.7244951590270953\n",
            "10300 0.7334092043670506\n",
            "10400 0.7287065768461459\n",
            "10500 0.7816261565754798\n",
            "10600 0.7044215777772784\n",
            "10700 0.6975679186542455\n",
            "10800 0.7056278824154458\n",
            "10900 0.7215674894330468\n",
            "11000 0.6995075111890321\n",
            "11100 0.6291535584142005\n",
            "11200 0.6864858628187435\n",
            "11300 0.6888001312735064\n",
            "11400 0.662031298140482\n",
            "11500 0.679227780033614\n",
            "11600 0.6641229126657567\n",
            "11700 0.6934521059128109\n",
            "11800 0.6958999289165956\n",
            "11900 0.6255779607626253\n",
            "12000 0.6829487455291959\n",
            "12100 0.6699111875139856\n",
            "12200 0.6753534592700042\n",
            "12300 0.6503794460759946\n",
            "12400 0.6248333399752547\n",
            "12500 0.6600787977847322\n",
            "12600 0.6299574165224446\n",
            "12700 0.6558282863698548\n",
            "12800 0.6278850679800344\n",
            "12900 0.6097674573604475\n",
            "13000 0.6103764097655983\n",
            "13100 0.6830346471467292\n",
            "13200 0.66167632102274\n",
            "13300 0.6457312617155008\n",
            "13400 0.6796343234446761\n",
            "13500 0.6594489054483537\n",
            "13600 0.6508501753046764\n",
            "13700 0.6260117047344235\n",
            "13800 0.6481939673341734\n",
            "13900 0.6114035377446755\n",
            "14000 0.6238214623632367\n",
            "14100 0.6337467200269016\n",
            "14200 0.6196013176108169\n",
            "14300 0.6504635948395907\n",
            "14400 0.6249251454023336\n",
            "14500 0.6459019250364493\n",
            "14600 0.5906482657568071\n",
            "14700 0.6214529211664503\n",
            "14800 0.7000741094208442\n",
            "14900 0.5794190663405578\n",
            "15000 0.6683293465699319\n",
            "15100 0.6475861579981643\n",
            "15200 0.6431802801315281\n",
            "15300 0.6127721919974337\n",
            "15400 0.5818616554638092\n",
            "15500 0.603499437142201\n",
            "15600 0.6144283684692258\n",
            "15700 0.619835307158511\n",
            "15800 0.5516342043115614\n",
            "15900 0.6190215160374368\n",
            "16000 0.5695264136165238\n",
            "16100 0.6282812126240651\n",
            "16200 0.6224153777011\n",
            "16300 0.56300505942849\n",
            "16400 0.622290277198735\n",
            "16500 0.6591703634445574\n",
            "16600 0.6034142384222724\n",
            "16700 0.6097931718529017\n",
            "16800 0.6333663249461986\n",
            "16900 0.5780475368919396\n",
            "17000 0.5959178573872579\n",
            "17100 0.5821978578384075\n",
            "17200 0.5795050250154711\n",
            "17300 0.5947446347193001\n",
            "17400 0.6137992482911709\n",
            "17500 0.6201206091936806\n",
            "17600 0.5512494733720974\n",
            "17700 0.5939784605457268\n",
            "17800 0.5312334909463547\n",
            "17900 0.5812928338431584\n",
            "18000 0.5564778363726347\n",
            "18100 0.5635709042452448\n",
            "18200 0.5860912139762692\n",
            "18300 0.6078207447427735\n",
            "18400 0.5657610856212512\n",
            "18500 0.5259115873123608\n",
            "18600 0.5834245538253026\n",
            "18700 0.5853856791369846\n",
            "18800 0.5434566261740718\n",
            "18900 0.5587245312303668\n",
            "19000 0.6079942025136618\n",
            "19100 0.5524644302059306\n",
            "19200 0.5451858747426759\n",
            "19300 0.6090159542615399\n",
            "19400 0.5677879622979064\n",
            "19500 0.5471655016363511\n",
            "19600 0.5357139268285571\n",
            "19700 0.6132291086583194\n",
            "19800 0.534069833868994\n",
            "19900 0.5102775868514575\n",
            "20000 0.5314659724576186\n",
            "20100 0.5330191240958253\n",
            "20200 0.5314357871692152\n",
            "20300 0.5311355020257152\n",
            "20400 0.5456990530477311\n",
            "20500 0.5317064150933024\n",
            "20600 0.5356780360069886\n",
            "20700 0.5314243034207495\n",
            "20800 0.5281587567559336\n",
            "20900 0.539174432637834\n",
            "21000 0.5792797543828391\n",
            "21100 0.5581150204446623\n",
            "21200 0.46508629909693094\n",
            "21300 0.4936368571229741\n",
            "21400 0.5334825068654899\n",
            "21500 0.5321200875302315\n",
            "21600 0.5005665755429621\n",
            "21700 0.5021959394763895\n",
            "21800 0.5268902746337848\n",
            "21900 0.5416766579385084\n",
            "22000 0.4664232125495978\n",
            "22100 0.5105945899030793\n",
            "22200 0.5462906754820697\n",
            "22300 0.49682371632409195\n",
            "22400 0.502929903246836\n",
            "22500 0.4987690952880761\n",
            "22600 0.5404664938230028\n",
            "22700 0.506844112540489\n",
            "22800 0.49673570351312735\n",
            "22900 0.4917596525711306\n",
            "23000 0.5406956244786756\n",
            "23100 0.4890696751160519\n",
            "23200 0.5219856817524887\n",
            "23300 0.485561991894125\n",
            "23400 0.4797821843328441\n",
            "23500 0.5137250504958926\n",
            "23600 0.48154224047667593\n",
            "23700 0.4570786982101267\n",
            "23800 0.48001626509724715\n",
            "23900 0.4562023189656922\n",
            "24000 0.5428221549442923\n",
            "24100 0.4659578019470787\n",
            "24200 0.5010953843370458\n",
            "24300 0.5285486060001232\n",
            "24400 0.5372646803977821\n",
            "24500 0.469974687836009\n",
            "24600 0.5040305399833518\n",
            "24700 0.46379848177541305\n",
            "24800 0.47705326690407357\n",
            "24900 0.5014025567045729\n",
            "25000 0.4721337247894736\n",
            "25100 0.49354499072317287\n",
            "25200 0.5001086961273102\n",
            "25300 0.4860736439477065\n",
            "25400 0.5157456096373496\n",
            "25500 0.4862288919328154\n",
            "25600 0.481662272901911\n",
            "25700 0.4264083338471699\n",
            "25800 0.4978485340712452\n",
            "25900 0.5100092878249769\n",
            "26000 0.47695024564844\n",
            "26100 0.4443913485612657\n",
            "26200 0.45252470884401297\n",
            "26300 0.47851466838469353\n",
            "26400 0.45686151687214105\n",
            "26500 0.43949323860427036\n",
            "26600 0.43845564357934613\n",
            "26700 0.45406191323389355\n",
            "26800 0.4469187942414856\n",
            "26900 0.44738522016364235\n",
            "27000 0.4503696426198703\n",
            "27100 0.4351273485642563\n",
            "27200 0.48237678103371084\n",
            "27300 0.41960055749446634\n",
            "27400 0.4543890590670121\n",
            "27500 0.4554961489822992\n",
            "27600 0.4512570027404624\n",
            "27700 0.472826969153168\n",
            "27800 0.4581197225445802\n",
            "27900 0.4599493735086336\n",
            "28000 0.44464612183372326\n",
            "28100 0.4160173944422151\n",
            "28200 0.42568010866253414\n",
            "28300 0.43995677312721737\n",
            "28400 0.44062912964777856\n",
            "28500 0.4175196621125661\n",
            "28600 0.44032826786662926\n",
            "28700 0.43271535010163775\n",
            "28800 0.4065443513448473\n",
            "28900 0.439960305512598\n",
            "29000 0.4384094149808836\n",
            "29100 0.4189494324471759\n",
            "29200 0.4066019974203743\n",
            "29300 0.40712661707886044\n",
            "29400 0.3899889264101038\n",
            "29500 0.4063085298687713\n",
            "29600 0.3965513813924081\n",
            "29700 0.39658795063057406\n",
            "29800 0.38358631482349914\n",
            "29900 0.37723897370382886\n",
            "30000 0.40355902241337094\n",
            "30100 0.40154048154469635\n",
            "30200 0.4151013183946355\n",
            "30300 0.43686169181062307\n",
            "30400 0.38262619401174525\n",
            "30500 0.40188124326815916\n",
            "30600 0.40096094860895926\n",
            "30700 0.4180189507840275\n",
            "30800 0.3910491838634207\n",
            "30900 0.38642930918944274\n",
            "31000 0.39556520693112185\n",
            "31100 0.3713610159527364\n",
            "31200 0.3807563897404146\n",
            "31300 0.36679625557508255\n",
            "31400 0.38256894751569104\n",
            "31500 0.40580646820709815\n",
            "31600 0.37278032997071336\n",
            "31700 0.3884178899537929\n",
            "31800 0.4226851640364079\n",
            "31900 0.4121464040565852\n",
            "32000 0.38115124402970296\n",
            "32100 0.4069765017851461\n",
            "32200 0.36809996779405907\n",
            "32300 0.3729479381635994\n",
            "32400 0.39903710380852286\n",
            "32500 0.3665428420051245\n",
            "32600 0.3849772119358608\n",
            "32700 0.3545067240891375\n",
            "32800 0.34871050467758546\n",
            "32900 0.38108166950382555\n",
            "33000 0.36835027687477273\n",
            "33100 0.3739969580289564\n",
            "33200 0.3122265829266392\n",
            "33300 0.40183704894441724\n",
            "33400 0.3544356951593862\n",
            "33500 0.32464858122396845\n",
            "33600 0.337420999551283\n",
            "33700 0.35150545322797716\n",
            "33800 0.37809663081427525\n",
            "33900 0.33775429228928155\n",
            "34000 0.3726483254176054\n",
            "34100 0.3330418204506794\n",
            "34200 0.3972813741237433\n",
            "34300 0.37955422116716925\n",
            "34400 0.3778564948310773\n",
            "34500 0.36359325051836017\n",
            "34600 0.342961436607028\n",
            "34700 0.36093249442307096\n",
            "34800 0.37408830786192954\n",
            "34900 0.3202628140051294\n",
            "35000 0.3570398199944763\n",
            "35100 0.3540089169671814\n",
            "35200 0.3557605991295072\n",
            "35300 0.33284187702204515\n",
            "35400 0.32215440692289743\n",
            "35500 0.3474284615544835\n",
            "35600 0.3751071580622982\n",
            "35700 0.33788166343015363\n",
            "35800 0.411160053570819\n",
            "35900 0.3546731680889606\n",
            "36000 0.3666786584738425\n",
            "36100 0.34137405236035084\n",
            "36200 0.3271271041134084\n",
            "36300 0.3416576555847518\n",
            "36400 0.33339126395643326\n",
            "36500 0.3596759784380337\n",
            "36600 0.3302284057331287\n",
            "36700 0.3660710566236119\n",
            "36800 0.33835220827549684\n",
            "36900 0.2991662455289439\n",
            "37000 0.35140335595114214\n",
            "37100 0.37071300026338194\n",
            "37200 0.3748071520383143\n",
            "37300 0.3032678195433528\n",
            "37400 0.34723730402244923\n",
            "37500 0.31505863934836953\n",
            "37600 0.33302500866710627\n",
            "37700 0.2945521590038102\n",
            "37800 0.34752056909696327\n",
            "37900 0.3313064468750753\n",
            "38000 0.3315274712131311\n",
            "38100 0.3431598382436055\n",
            "38200 0.3333906060508769\n",
            "38300 0.32731291519439654\n",
            "38400 0.31724306994453255\n",
            "38500 0.3381586160998119\n",
            "38600 0.3560838982311588\n",
            "38700 0.29211774345465447\n",
            "38800 0.3347472124358626\n",
            "38900 0.34447586399399743\n",
            "39000 0.32589862823416915\n",
            "39100 0.3366237155593899\n",
            "39200 0.2690470004020122\n",
            "39300 0.3238782892182077\n",
            "39400 0.3137999518295416\n",
            "39500 0.3459309447065811\n",
            "39600 0.31833844178967163\n",
            "39700 0.3627461645813724\n",
            "39800 0.3167906646538329\n",
            "39900 0.3211004080469134\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train_losses_list = []\n",
        "per_token_train_losses_list = []\n",
        "\n",
        "for epoch in tqdm.tqdm(range(cfg.n_training_steps)):\n",
        "\n",
        "  tokens = next(ds)\n",
        "  logits = model(tokens)\n",
        "\n",
        "  per_token_train_losses_raw, _ = logits_to_tokens_loss(logits, tokens)\n",
        "  per_token_train_losses = loss_fn(per_token_train_losses_raw)\n",
        "  per_token_train_losses_list.append(utils.to_numpy(per_token_train_losses))\n",
        "\n",
        "  train_loss = per_token_train_losses.mean()\n",
        "  train_loss.backward()\n",
        "  train_losses_list.append(train_loss.item())\n",
        "\n",
        "  optimizer.step()\n",
        "  scheduler.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    the_loss = train_loss.item()\n",
        "    print(epoch, the_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgzjamSn2y4j",
        "outputId": "7f6a538b-79b8-4fdc-ff52-a5206d340b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to Google drive /content/drive/MyDrive/AI/CoLabOutput/mult_D5_L3_H4_dmodel512_dhead128_ctx18_seed129000_train40K.pt\n"
          ]
        }
      ],
      "source": [
        "print(\"Saving model to Google drive\", model_save_location)\n",
        "torch.save(model.state_dict(), model_save_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc4pQgE__cDp"
      },
      "source": [
        "# Part 6: Final Training Loss\n",
        "\n",
        "- add_D5_L2_H3_dmodel510_dhead170_ctx18_seed129000_train30K has loss of 1.38e-07\n",
        "- sub_D5_L2_H3_dmodel510_dhead170_ctx18_seed129000_train30K has loss of 2.501e-06\n",
        "- mult_D5_L2_H3_dmodel510_dhead170_ctx18_seed129000_train30K has loss of 0.594356712\n",
        "- mult_D5_L3_H3_dmodel510_dhead170_ctx18_seed129000_train30K has loss of 0.408103767\n",
        "- mult_D5_L3_H4_dmodel512_dhead128_ctx18_seed129000_train30K has loss of 0.377608407\n",
        "-mult_D5_L3_H4_dmodel512_dhead128_ctx18_seed129000_train40K has loss of 0.34133034\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwJBrnSJ23eb",
        "outputId": "0aaac503-1956-4e59-f0d0-675044fa0fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%Mult= 100 %Sub= 0 digits= 5 heads= 4 layers= 3 ctx= 18 seed= 129000 training_steps= 40000\n",
            "Final training loss 0.34133034\n"
          ]
        }
      ],
      "source": [
        "print_config()\n",
        "\n",
        "final_training_loss = round((train_losses_list[-5]+train_losses_list[-4]+train_losses_list[-3]+train_losses_list[-2]+train_losses_list[-1])/5,9)\n",
        "print( \"Final training loss\", final_training_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiOdvQFD_fxZ"
      },
      "source": [
        "# Part 7: Line Graphs\n",
        "\n",
        "This section analyses the training loss by graphing it at a high level.\n",
        "\n",
        "The loss curve for all digits show visible inflection points (bumps), but is too high level to help understand the algorithm.\n",
        "\n",
        "When this graph is decomposed into 'per digit' graphs, the interesting distinct 'per digit' curves appear, showing each digit is being refined semi-independently, with the model algorithm refining each digit separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmyT2TFd29at"
      },
      "outputs": [],
      "source": [
        "epochs_to_graph=1200\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "# Helper function to plot multiple lines\n",
        "def lines(raw_lines_list, x=None, mode='lines', labels=None, xaxis='Epoch', yaxis='Loss', title = '', log_y=False, hover=None, all_epochs=True, **kwargs):\n",
        "\n",
        "    lines_list = raw_lines_list if all_epochs==False else [row[:epochs_to_graph] for row in raw_lines_list]\n",
        "    log_suffix = '' if log_y==False else ' (Log)'\n",
        "    epoch_suffix = '' if all_epochs==False else ' (' + str(epochs_to_graph) + ' steps)'\n",
        "    full_title = title + log_suffix + epoch_suffix\n",
        "\n",
        "    if type(lines_list)==torch.Tensor:\n",
        "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
        "    if x is None:\n",
        "        x=np.arange(len(lines_list[0]))\n",
        "    if cfg.save_graph_to_file :\n",
        "      fig = go.Figure(layout={})\n",
        "      print(full_title)\n",
        "    else:\n",
        "      fig = go.Figure(layout={'title':full_title})\n",
        "\n",
        "    fig.update_xaxes(title=xaxis)\n",
        "    fig.update_yaxes(title=yaxis + log_suffix)\n",
        "    for c, line in enumerate(lines_list):\n",
        "        if type(line)==torch.Tensor:\n",
        "            line = utils.to_numpy(line)\n",
        "        if labels is not None:\n",
        "            label = labels[c]\n",
        "        else:\n",
        "            label = c\n",
        "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
        "    if log_y:\n",
        "        fig.update_layout(yaxis_type=\"log\")\n",
        "    if cfg.save_graph_to_file:\n",
        "        fig.update_layout(margin=dict(l=10, r=10, t=10, b=10),width=1200,height=300)\n",
        "\n",
        "    fig.show(bbox_inches=\"tight\")\n",
        "\n",
        "    if cfg.save_graph_to_file:\n",
        "        filename = full_title.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"&\", \"\").replace(\",\", \"\").replace(\"%\", \"\")   +'.pdf'\n",
        "        pio.write_image(fig, filename)\n",
        "\n",
        "\n",
        "\n",
        "title_suffix = ' Loss Curves for ' + str(cfg.n_digits) + ' digit addition'\n",
        "per_token_losses = np.stack(per_token_train_losses_list, axis=0)\n",
        "\n",
        "line(train_losses_list,\n",
        "    title=title_suffix)\n",
        "\n",
        "all_epochs = True;\n",
        "for i in range(2):\n",
        "  lines([per_token_losses[:, i] for i in range(1+cfg.n_digits)]+[train_losses_list],\n",
        "        labels = [f'digit {i}' for i in range(1+cfg.n_digits)]+['all_digits'],\n",
        "        title='Per digit'+title_suffix,\n",
        "        all_epochs=all_epochs)\n",
        "\n",
        "  all_epochs = False;\n",
        "\n",
        "for i in range(1+cfg.n_digits):\n",
        "  print('Final Loss for digit ' + str(i) + ' is ', per_token_losses[-1, i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgk8m02A_lxB"
      },
      "source": [
        "# Part 8: Questions Set Up\n",
        "\n",
        "Create sets of sample questions (by task) to ask the model to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjM6eWDm3Ap7"
      },
      "outputs": [],
      "source": [
        "# Insert a number into the question\n",
        "def insert_question_number(the_question, index, first_digit_index, the_digits, n):\n",
        "\n",
        "  last_digit_index = first_digit_index + the_digits - 1\n",
        "\n",
        "  for j in range(the_digits):\n",
        "    the_question[index, last_digit_index-j] = n % 10\n",
        "    n = n // 10\n",
        "\n",
        "\n",
        "# Create a single question\n",
        "def make_a_question(the_question, index, q1, q2):\n",
        "  a = q1 + q2\n",
        "\n",
        "  insert_question_number(the_question, index, 0, cfg.n_digits, q1)\n",
        "\n",
        "  the_question[index, cfg.n_digits] = PLUS_INDEX\n",
        "\n",
        "  insert_question_number( the_question, index, cfg.n_digits+1, cfg.n_digits, q2)\n",
        "\n",
        "  the_question[index, 2*cfg.n_digits+1] = EQUALS_INDEX\n",
        "  offset = 2\n",
        "\n",
        "  insert_question_number(the_question, index, 2*cfg.n_digits + offset, cfg.n_digits+1, q1+q2)\n",
        "\n",
        "\n",
        "# Create a batch of questions from a 2D matrix of ints\n",
        "def make_questions(q_matrix):\n",
        "  length = len(q_matrix)\n",
        "\n",
        "  questions = torch.zeros((length, cfg.n_ctx)).to(torch.int64)\n",
        "\n",
        "  limit = 10 ** cfg.n_digits\n",
        "  for i in range(length):\n",
        "    if (q_matrix[i][0] < limit) and (q_matrix[i][1] < limit) :\n",
        "      make_a_question(questions, i, q_matrix[i][0], q_matrix[i][1])\n",
        "\n",
        "  return questions\n",
        "\n",
        "\n",
        "def prediction_to_string(max_indices):\n",
        "  answer = \"\".join([str(i) for i in utils.to_numpy(max_indices)[0]])\n",
        "  return answer;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7tHBz7V3DL_"
      },
      "outputs": [],
      "source": [
        "# Analyse the question and return the use case as BA, MC, SimpleUS9 or CascadeUS9\n",
        "def get_question_case(q):\n",
        "  qa = utils.to_numpy(q)\n",
        "  qn = qa[:2*cfg.n_digits+2]\n",
        "\n",
        "  # Locate the MC and MS digits (if any)\n",
        "  mc = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  ms = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if qn[dn] + qn[dn + cfg.n_digits + 1] == 9:\n",
        "      ms[cfg.n_digits-1-dn] = 1\n",
        "    if qn[dn] + qn[dn + cfg.n_digits +1] > 9:\n",
        "      mc[cfg.n_digits-1-dn] = 1\n",
        "\n",
        "  # Calculate the use case of a question\n",
        "  if torch.sum(mc) == 0:\n",
        "    return \"BA\"\n",
        "\n",
        "  if torch.sum(ms) == 0:\n",
        "    return \"MC1\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-2 and mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1:\n",
        "      return \"CascadeUS9\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-1 and mc[dn] == 1 and ms[dn+1] == 1:\n",
        "      return \"SimpleUS9\"\n",
        "\n",
        "  return \"MC1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BpOcko53GrI"
      },
      "outputs": [],
      "source": [
        "# Manually create some questions that strongly test one use case\n",
        "\n",
        "\n",
        "def make_ba_questions():\n",
        "    return make_questions(\n",
        "      [[12345, 33333],\n",
        "      [33333, 12345],\n",
        "      [45762, 33113],\n",
        "      [888, 11111],\n",
        "      [2362, 23123],\n",
        "      [15, 81],\n",
        "      [1000, 4440],\n",
        "      [4440, 1000],\n",
        "      [24033, 25133],\n",
        "      [23533, 21133],\n",
        "      [32500, 1],\n",
        "      [31500, 1111],\n",
        "      [5500, 12323],\n",
        "      [4500, 2209],\n",
        "      [ 33345, 66643], # =099988\n",
        "      [ 66643, 33345], # =099988\n",
        "      [10990, 44000],\n",
        "      [60000, 30000],\n",
        "      [10000, 20000]])\n",
        "\n",
        "\n",
        "def make_uc1_questions():\n",
        "    return make_questions(\n",
        "      [[ 15, 45],\n",
        "      [ 25, 55],\n",
        "      [ 35, 59],\n",
        "      [ 40035, 40049],\n",
        "      [ 5025, 5059],\n",
        "      [ 15, 65],\n",
        "      [ 44000, 46000],\n",
        "      [ 70000, 40000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 1500, 4500],\n",
        "      [ 2500, 5500],\n",
        "      [ 3500, 5900],\n",
        "      [ 15020, 45091],\n",
        "      [ 25002, 55019],\n",
        "      [ 35002, 59019]])\n",
        "\n",
        "\n",
        "def make_simple_us9_questions():\n",
        "    return make_questions(\n",
        "      [[ 55, 45],\n",
        "      [ 45, 55],\n",
        "      [ 45, 59],\n",
        "      [ 35, 69],\n",
        "      [ 25, 79],\n",
        "      [ 15, 85],\n",
        "      [ 15, 88],\n",
        "      [ 15508, 14500],\n",
        "      [ 14508, 15500],\n",
        "      [ 24533, 25933],\n",
        "      [ 23533, 26933],\n",
        "      [ 32500, 7900],\n",
        "      [ 31500, 8500],\n",
        "      [ 550, 450],\n",
        "      [ 450, 550],\n",
        "      [ 10880, 41127],\n",
        "      [ 41127, 10880],\n",
        "      [ 12386, 82623]])\n",
        "\n",
        "\n",
        "def make_cascade_us9_questions(clean = True):\n",
        "    return make_questions(\n",
        "      # These are two level UseSum9 cascades\n",
        "      [[ 555, 445],\n",
        "      [ 3340, 6660],\n",
        "      [ 8880, 1120],\n",
        "      [ 1120, 8880],\n",
        "      [ 123, 877],\n",
        "      [ 877, 123],\n",
        "      [ 321, 679],\n",
        "      [ 679, 321],\n",
        "      [ 1283, 88786],\n",
        "      # These are three level UseSum9 cascades\n",
        "      [ 5555, 4445],\n",
        "      [ 55550, 44450],\n",
        "      [ 334, 666],\n",
        "      [ 3340, 6660],\n",
        "      [ 33400, 66600],\n",
        "      [ 888, 112],\n",
        "      [ 8880, 1120],\n",
        "      [ 88800, 11200],\n",
        "      [ 1234, 8766],\n",
        "      [ 4321, 5679],\n",
        "      # These are four level UseSum9 cascades\n",
        "      [ 44445, 55555],\n",
        "      [ 33334, 66666],\n",
        "      [ 88888, 11112],\n",
        "      [ 12345, 87655],\n",
        "      [ 54321, 45679],\n",
        "      [ 45545, 54455],\n",
        "      [ 36634, 63366],\n",
        "      [ 81818, 18182],\n",
        "      [ 87345, 12655],\n",
        "      [ 55379, 44621]])\n",
        "\n",
        "\n",
        "# These questions focus mainly on 1 digit at a time\n",
        "# (We're assuming that the 0 + 0 digit additions are trivial bigrams)\n",
        "def make_answerdigit_questions():\n",
        "    return make_questions(\n",
        "      [[ 1, 0],\n",
        "      [ 4, 3],\n",
        "      [ 5, 5],\n",
        "      [ 8, 1],\n",
        "      [ 40, 30],\n",
        "      [ 44, 46],\n",
        "      [ 400, 300],\n",
        "      [ 440, 460],\n",
        "      [ 800, 100],\n",
        "      [ 270, 470],\n",
        "      [ 600, 300],\n",
        "      [ 4000, 3000],\n",
        "      [ 4400, 4600],\n",
        "      [ 6000, 3000],\n",
        "      [ 7000, 4000],\n",
        "      [ 40000, 30000],\n",
        "      [ 44000, 46000],\n",
        "      [ 60000, 30000],\n",
        "      [ 70000, 40000],\n",
        "      [ 10000, 20000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 76000, 99000]])\n",
        "\n",
        "\n",
        "# Returns 128 random and ~100 manually-chosen questions\n",
        "def make_varied_questions():\n",
        "  q0, _, _, _, _, _ = next(ds)\n",
        "  q1 = make_ba_questions()\n",
        "  q2 = make_uc1_questions()\n",
        "  q3 = make_simple_us9_questions()\n",
        "  q4 = make_cascade_us9_questions()\n",
        "  q5 = make_answerdigit_questions()\n",
        "  q6, _, _, _, _, _ = next(ds)\n",
        "\n",
        "  questions = torch.vstack((q0.cuda(), q1.cuda(), q2.cuda(), q3.cuda(), q4.cuda(), q5.cuda(), q6.cuda()))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1cLLNpU3I0j"
      },
      "outputs": [],
      "source": [
        "# Test that the get_question_case code works as expected\n",
        "def unit_test_get_question_case_core(correct_case, questions):\n",
        "  num_questions = questions.shape[0]\n",
        "  print( correct_case, \"#Questions=\", num_questions)\n",
        "  for i in range(num_questions):\n",
        "    question_case = get_question_case(questions[i])\n",
        "    if question_case != correct_case:\n",
        "      print( \"Case mismatch:\", correct_case, question_case, questions[i])\n",
        "\n",
        "def unit_test_get_question_case():\n",
        "  unit_test_get_question_case_core( \"BA\", make_ba_questions())\n",
        "  unit_test_get_question_case_core( \"MC1\", make_uc1_questions())\n",
        "  unit_test_get_question_case_core( \"SimpleUS9\", make_simple_us9_questions())\n",
        "  unit_test_get_question_case_core( \"CascadeUS9\", make_cascade_us9_questions())\n",
        "\n",
        "unit_test_get_question_case()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4bqJ5C23M-A"
      },
      "outputs": [],
      "source": [
        "verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJayaDiR3PgJ"
      },
      "outputs": [],
      "source": [
        "# Build a test batch of 64 random and ~100 manually-chosen questions\n",
        "varied_questions = make_varied_questions();\n",
        "\n",
        "\n",
        "# Run the sample batch, gather the cache\n",
        "model.reset_hooks()\n",
        "model.set_use_attn_result(True)\n",
        "sample_logits, sample_cache = model.run_with_cache(varied_questions.cuda())\n",
        "print(sample_cache) # Gives names of datasets in the cache\n",
        "sample_losses_raw, sample_max_indices = logits_to_tokens_loss(sample_logits, varied_questions.cuda())\n",
        "sample_loss_mean = utils.to_numpy(loss_fn(sample_losses_raw).mean())\n",
        "print(\"Sample Mean Loss\", sample_loss_mean) # Loss < 0.04 is good\n",
        "\n",
        "\n",
        "# attn.hook_z is the \"attention head output\" hook point name (at a specified layer)\n",
        "# Used in h_set_attn_hook_z and t_*_hook functions\n",
        "l_attn_hook_z_name = ['blocks.0.attn.hook_z','blocks.1.attn.hook_z']\n",
        "sample_attn_z = sample_cache[l_attn_hook_z_name[0]]\n",
        "print(\"Sample\", l_attn_hook_z_name[0], sample_attn_z.shape) # gives [239, 18, 3, 170] = ???, cfg.n_ctx, n_heads, d_head\n",
        "mean_attn_z = torch.mean(sample_attn_z, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_attn_hook_z_name[0], mean_attn_z.shape) # gives [1, 18, 3, 170] = 1, cfg.n_ctx, n_heads, d_head\n",
        "\n",
        "\n",
        "# hook_resid_pre is the \"pre residual memory update\" hook point name (at a specified layer)\n",
        "# Used in o_*_hook functions\n",
        "l_hook_resid_pre_name = ['blocks.0.hook_resid_pre','blocks.1.hook_resid_pre']\n",
        "\n",
        "\n",
        "# hook_resid_post is the \"post residual memory update\" hook point name (at a specified layer)\n",
        "# Used in c_*_hook and t_*_hook functions\n",
        "l_hook_resid_post_name = ['blocks.0.hook_resid_post','blocks.1.hook_resid_post']\n",
        "sample_resid_post = sample_cache[l_hook_resid_post_name[0]]\n",
        "print(\"Sample\", l_hook_resid_post_name[0], sample_resid_post.shape) # gives [239, 18, 510] = ???, cfg.n_ctx, d_model\n",
        "mean_resid_post = torch.mean(sample_resid_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_hook_resid_post_name[0], mean_resid_post.shape) # gives [1, 18, 510] = 1, cfg.n_ctx, d_model\n",
        "\n",
        "\n",
        "# mlp.hook_post is the \"MLP layer\" hook point name (at a specified layer)\n",
        "# Used in m_*_hook functions\n",
        "l_mlp_hook_post_name = ['blocks.0.mlp.hook_post','blocks.1.mlp.hook_post']\n",
        "sample_mlp_hook_post = sample_cache[l_mlp_hook_post_name[0]]\n",
        "print(\"Sample\", l_mlp_hook_post_name[0], sample_mlp_hook_post.shape) # gives [239, 18, 2040] = ???, cfg.n_ctx, d_model*4\n",
        "mean_mlp_hook_post = torch.mean(sample_mlp_hook_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_mlp_hook_post_name[0], mean_mlp_hook_post.shape) # gives [1, 18, 2040] = 1, cfg.n_ctx, d_model*4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deGhK1OC_1mD"
      },
      "source": [
        "# Part 9: Attention Patterns\n",
        "Attention patterns show which token(s) the model's attention heads are paying attention to in each token position of the prediction calculation.\n",
        "\n",
        "For the default CoLab set up, the  model has 3 attention heads, and performs 5 digit addition. The attention pattern is 18 by 18 squares (as 54321+77779=132100 is 18 tokens). Time proceeds vertically downwards, with one additional token being revealed horizontally at each token position, giving the overall triangle shape. This visualisation provided insights. After the question is fully revealed (at token position 11), each head starts attending to pairs of question digits from left to right (i.e. high-value digits before lower-value digits) giving the “double staircase\" shape. The three heads attend to a given digit pair in three different token position, giving a time ordering of heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiVCSBSE3Xpk"
      },
      "outputs": [],
      "source": [
        "def show_token_attention_patterns(index, layer, token_at_index, use_case):\n",
        "\n",
        "  the_tokens = [str(token) for token in token_at_index.tolist()]\n",
        "  if layer == 0:\n",
        "    tokens_str = tokens_to_string(the_tokens)\n",
        "    print(\"Attention patterns for\", tokens_str)\n",
        "\n",
        "  attention_pattern=sample_cache[\"pattern\", layer, \"attn\"][index]\n",
        "  display(cv.attention.attention_patterns(\n",
        "      tokens=the_tokens,\n",
        "      attention=attention_pattern,\n",
        "      #attention_head_names=[f\"L{layer}H{i}\" for i in range(cfg.n_heads)],\n",
        "  ))\n",
        "\n",
        "\n",
        "sample_size = 3\n",
        "\n",
        "# Show attention patterns for some randomly chosen tokens\n",
        "for i in range(sample_size):\n",
        "  for layer in range(cfg.n_layers):\n",
        "    show_token_attention_patterns(i, layer, tokens[i], \"Misc\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUCXw1k93a20"
      },
      "outputs": [],
      "source": [
        "if cfg.save_graph_to_file:\n",
        "\n",
        "  tokens_str = []\n",
        "  for i in range(cfg.n_heads):\n",
        "    one_token_str = []\n",
        "    for j in tokens[i]:\n",
        "      one_token_str.append(str(utils.to_numpy(j)))\n",
        "    tokens_str.append(one_token_str)\n",
        "\n",
        "  # Refer https://github.com/callummcdougall/CircuitsVis/blob/main/python/circuitsvis/circuitsvis_demo.ipynb\n",
        "\n",
        "  # html_object = cv.attention.from_cache(\n",
        "  #    cache = sample_cache,\n",
        "  #    tokens = tokens_str, # list of list of strings\n",
        "  #    return_mode = \"html\",\n",
        "  #)\n",
        "\n",
        "  # Create a CoLab file containing the attention pattern(s) in HTML\n",
        "  #filename = \"AttentionPattern\" + str(cfg.n_digits) + \"Digits\" + str(cfg.n_heads) + \"Heads.html\"\n",
        "  #with open(filename, \"w\") as f:\n",
        "  #    f.write(html_object.data)\n",
        "\n",
        "  # Manually download the CoLab \"html\" file and open in your local browser.\n",
        "  # Install and use the Edge extension \"FireShot\" to save a portion of the HTML page as a PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1vfpD1Z_6vK"
      },
      "source": [
        "# Part 10: Run a prediction question\n",
        "\n",
        "Create way to get model to predict sample question answers and analysis/show results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2tcTkbi3l5M"
      },
      "outputs": [],
      "source": [
        "def predict_experiment_question(questions, the_hook, the_threshold):\n",
        "\n",
        "  c_loss_mean = 0\n",
        "\n",
        "  for question_num in range(questions.shape[0]):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    model.reset_hooks()\n",
        "    model.set_use_attn_result(True)\n",
        "    exp_logits = model.run_with_hooks(q.cuda(), return_type=\"logits\", fwd_hooks=the_hook)\n",
        "\n",
        "    q_2d = q.unsqueeze(0)\n",
        "    exp_losses_raw, exp_max_indices = logits_to_tokens_loss(exp_logits, q_2d.cuda())\n",
        "    c_loss_mean = utils.to_numpy(loss_fn(exp_losses_raw).mean())\n",
        "\n",
        "    # Only show the question if the loss exceeds the threshold (because of the ablated token position)\n",
        "    if c_loss_mean > the_threshold:\n",
        "      print(tokens_to_string(q), \"ModelAnswer:\", exp_answer_str, \"Matches:\", match_str, \"Loss:\", c_loss_mean, \"Case:\", the_case )\n",
        "\n",
        "  return c_loss_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFTPgtnGA2U3"
      },
      "source": [
        "# Part 11 : Is the model 100% accurate?\n",
        "\n",
        "This is hard to prove. If it does 1M predictions without error, then we assume it is 100%.\n",
        "\n",
        "This part takes ~90 mins to run for 5-digit 2-layer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia1CqJ5n4zXD"
      },
      "outputs": [],
      "source": [
        "def null_hook(value, hook):\n",
        "  global verbose\n",
        "\n",
        "  verbose = False\n",
        "\n",
        "\n",
        "def one_million_questions():\n",
        "  global verbose\n",
        "  global exp9_threshold\n",
        "  global exp9_successes\n",
        "  global exp9_fails\n",
        "\n",
        "  verbose = False\n",
        "  exp9_threshold = 0.12\n",
        "  exp9_successes = 0\n",
        "  exp9_fails = 0\n",
        "\n",
        "  num_batches = 1000000//cfg.batch_size\n",
        "  for epoch in range(num_batches):\n",
        "      tokens, _, _, _, _, _ = next(ds)\n",
        "\n",
        "      the_hook = [(l_attn_hook_z_name[0], null_hook)]\n",
        "      predict_experiment_question(tokens, the_hook, exp9_threshold)\n",
        "\n",
        "      exp9_fails = total_case_fails()\n",
        "      if exp9_fails > 0:\n",
        "        break\n",
        "\n",
        "      exp9_successes = exp9_successes + len(hcfg.questions)\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "          print(\"Batch\", epoch, \"of\", num_batches, \"#Successes=\", exp9_successes)\n",
        "\n",
        "  print(\"successes\", exp9_successes, \"num_fails\", exp9_fails)\n",
        "\n",
        "\n",
        "# Commented out as it takes ~90 minutes to run with cfg.n_layers=2, n_digits=5, train=30K\n",
        "# one_million_questions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2kvnJV1f9m-"
      },
      "source": [
        "#Part 12: Model Transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHXof7Q1gCZQ"
      },
      "outputs": [],
      "source": [
        "# Transfer the model small weights to the model large\n",
        "def transfer_model(small_model, large_model, start_layer, end_layer, transfer_ln=True, transfer_embeds=True):\n",
        "    \"\"\"Args:\n",
        "    small_model: The model to transfer weights from\n",
        "    large_model: The model to transfer weights to\n",
        "    start_layer: The first layer to transfer weights to\n",
        "    end_layer: The last layer to transfer weights to (Note that this is end-inclusive!)\n",
        "    \"\"\"\n",
        "    small_cfg = {k: v for k,v in small_model.cfg.__dict__.items() if k in [\"d_head\", \"d_mlp\", \"d_model\", \"n_heads\", \"n_layers\"]}\n",
        "    large_cfg = {k: v for k,v in large_model.cfg.__dict__.items() if k in [\"d_head\", \"d_mlp\", \"d_model\", \"n_heads\", \"n_layers\"]}\n",
        "\n",
        "    # Sanity checks for large model > small model\n",
        "    assert small_cfg[\"d_model\"] == large_cfg[\"d_model\"]\n",
        "    assert small_cfg[\"d_head\"] == large_cfg[\"d_head\"]\n",
        "    assert small_cfg[\"n_layers\"] <= large_cfg[\"n_layers\"]\n",
        "    assert small_cfg[\"n_heads\"] <= large_cfg[\"n_heads\"]\n",
        "    assert small_cfg[\"d_mlp\"] <= large_cfg[\"d_mlp\"]\n",
        "\n",
        "    assert 0 <= start_layer < end_layer <= large_cfg[\"n_layers\"] # Make sure start_layer and end_layer are valid\n",
        "    assert end_layer - start_layer + 1 == small_cfg[\"n_layers\"] # Make sure the number of layers to transfer is correct\n",
        "\n",
        "    # Transfer heads and MLPs\n",
        "    for small_layer_no, large_layer_no in enumerate(range(start_layer, end_layer+1)):\n",
        "        # Transfer Heads\n",
        "        large_model.blocks[large_layer_no].attn.W_Q.data[:small_cfg[\"n_heads\"]] = small_model.blocks[small_layer_no].attn.W_Q.clone().data\n",
        "        large_model.blocks[large_layer_no].attn.W_K.data[:small_cfg[\"n_heads\"]] = small_model.blocks[small_layer_no].attn.W_K.clone().data\n",
        "        large_model.blocks[large_layer_no].attn.W_V.data[:small_cfg[\"n_heads\"]] = small_model.blocks[small_layer_no].attn.W_V.clone().data\n",
        "\n",
        "        large_model.blocks[large_layer_no].attn.b_Q.data[:small_cfg[\"n_heads\"]] = small_model.blocks[small_layer_no].attn.b_Q.clone().data\n",
        "        large_model.blocks[large_layer_no].attn.b_K.data[:small_cfg[\"n_heads\"]] = small_model.blocks[small_layer_no].attn.b_K.clone().data\n",
        "        large_model.blocks[large_layer_no].attn.b_V.data[:small_cfg[\"n_heads\"]] = small_model.blocks[small_layer_no].attn.b_V.clone().data\n",
        "\n",
        "        # Transfer MLPs\n",
        "        large_model.blocks[large_layer_no].mlp.W_in.data[:, :small_cfg[\"d_mlp\"]] = small_model.blocks[small_layer_no].mlp.W_in.clone().data\n",
        "        large_model.blocks[large_layer_no].mlp.b_in.data[:small_cfg[\"d_mlp\"]] = small_model.blocks[small_layer_no].mlp.b_in.clone().data\n",
        "        large_model.blocks[large_layer_no].mlp.W_out.data[:small_cfg[\"d_mlp\"],] = small_model.blocks[small_layer_no].mlp.W_out.clone().data\n",
        "        large_model.blocks[large_layer_no].mlp.b_out.data = small_model.blocks[small_layer_no].mlp.b_out.clone().data\n",
        "\n",
        "    if transfer_ln:\n",
        "        for small_layer_no, large_layer_no in enumerate(range(start_layer, end_layer+1)):\n",
        "            large_model.blocks[large_layer_no].ln1.w.data = small_model.blocks[small_layer_no].ln1.w.clone().data\n",
        "            large_model.blocks[large_layer_no].ln1.b.data = small_model.blocks[small_layer_no].ln1.b.clone().data\n",
        "\n",
        "        large_model.ln_final.w.data = small_model.ln_final.w.clone().data\n",
        "\n",
        "    if transfer_embeds:\n",
        "        large_model.embed.W_E.data = small_model.embed.W_E.clone().data\n",
        "        large_model.pos_embed.W_pos.data = small_model.pos_embed.W_pos.clone().data\n",
        "        large_model.unembed.W_U.data = small_model.unembed.W_U.clone().data\n",
        "\n",
        "#transfer_model(small_model=model_small, large_model=model_large, start_layer=1, end_layer=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "O3OoV2Pd-w7g",
        "F5K2kA0L_FHc",
        "3tExv4rk_L0C",
        "Pg267eav_QSl",
        "LiOdvQFD_fxZ",
        "pgk8m02A_lxB",
        "deGhK1OC_1mD",
        "j1vfpD1Z_6vK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a7674624f9e465c86a11d3ee3423ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d19f6555ea7346ac824ef0bc2aa7e050",
              "IPY_MODEL_5b9394862a484f178abab6e71b87ac4f",
              "IPY_MODEL_83327ca4a9434bb481090a3d90516e71"
            ],
            "layout": "IPY_MODEL_a63db403cef64c5e97f4b08626780692"
          }
        },
        "d19f6555ea7346ac824ef0bc2aa7e050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91076e7c1aa5429a9ea306c9bdba340e",
            "placeholder": "​",
            "style": "IPY_MODEL_2f6acad83dfc4938beec6bd6579c7ed4",
            "value": "100%"
          }
        },
        "5b9394862a484f178abab6e71b87ac4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28968bd77bdf4e8daf437b33e107380c",
            "max": 40000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e97e3130a0ce4cefaf474f553dfd29ff",
            "value": 40000
          }
        },
        "83327ca4a9434bb481090a3d90516e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b53d987665664a25a2b391ea7b3f7509",
            "placeholder": "​",
            "style": "IPY_MODEL_c6f7e216b4084cbe9d31c9d67922ca5b",
            "value": " 40000/40000 [46:12&lt;00:00, 15.05it/s]"
          }
        },
        "a63db403cef64c5e97f4b08626780692": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91076e7c1aa5429a9ea306c9bdba340e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f6acad83dfc4938beec6bd6579c7ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28968bd77bdf4e8daf437b33e107380c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e97e3130a0ce4cefaf474f553dfd29ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b53d987665664a25a2b391ea7b3f7509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f7e216b4084cbe9d31c9d67922ca5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}