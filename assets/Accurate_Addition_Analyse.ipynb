{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG2gZSoSJD5C"
      },
      "source": [
        "# Accurate Integer Addition in Transformers - Analyse the Model\n",
        "\n",
        "This CoLab analyses a Transformer model that performs integer addition e.g. 33357+82243=115600. Each digit is a separate token. For 5 digit addition, the model is given 12 \"question\" (input) tokens, and must then predict the corresponding 6 \"answer\" (output) tokens.\n",
        "\n",
        "The model weightings created by the sister CoLab [Accurate_Addition_Train](https://github.com/PhilipQuirke/transformer-maths/blob/main/assets/Accurate_Addition_Train.ipynb) are loaded from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzkGrSqHJKqN"
      },
      "source": [
        "## Tips for using the Colab\n",
        " * You can run and alter the code in this CoLab notebook yourself in Google CoLab ( https://colab.research.google.com/ ).\n",
        " * To run the notebook, in Google CoLab, **you will need to** go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.\n",
        " * Some graphs are interactive!\n",
        " * Use the table of contents pane in the sidebar to navigate.\n",
        " * Collapse irrelevant sections with the dropdown arrows.\n",
        " * Search the page using the search in the sidebar, not CTRL+F."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGPkaokJQM5"
      },
      "source": [
        "# Part 1: Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "HmjGdFcdJat3"
      },
      "outputs": [],
      "source": [
        "# Tokens used in vocab. (Token indexes 0 to 9 represent digits 0 to 9)\n",
        "PLUS_INDEX = 10\n",
        "MINUS_INDEX = 11\n",
        "EQUALS_INDEX = 12\n",
        "\n",
        "class Config():\n",
        "  #@markdown Model\n",
        "  n_layers: int = 2 #@param\n",
        "  n_heads: int = 3 #@param\n",
        "\n",
        "  d_vocab: int = EQUALS_INDEX+1\n",
        "  d_model: int = ( 512 // n_heads ) * n_heads # About 512, and divisible by n_heads\n",
        "  d_mlp: int = 4 * d_model\n",
        "  d_head: int = d_model // n_heads  # About 170 when n_heads == 3\n",
        "  seed: int = 129000 #@param\n",
        "\n",
        "  #@markdown Data\n",
        "  n_digits: int = 5 #@param\n",
        "  n_ctx: int = 3 * n_digits + 3\n",
        "  act_fn: str = 'relu'\n",
        "  batch_size: int = 64 #@param\n",
        "\n",
        "  #@markdown Optimizer\n",
        "  n_training_steps: int = 30000 #@param\n",
        "  lr: float = 0.00008 #@param\n",
        "  weight_decay: int = 0.1 #@param\n",
        "\n",
        "  # Save graphs to CoLab temp files as PDF and HTML. Can manually export files for re-use in papers.\n",
        "  save_graph_to_file: bool = True\n",
        "\n",
        "  # The format to output prettytable in. Options are text|html|json|csv|latex\n",
        "  # Use Text for this CoLab, latex for Overleaf output, and html for GitHub blog output\n",
        "  table_out_format: str = \"text\"\n",
        "\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTd3nmsMJV5T"
      },
      "source": [
        "# Part 2: Import libraries\n",
        "Imports standard libraries. Don't bother reading.\n",
        "You will need to give permission for this CoLab to access your Google Drive to load the model weights (created by the \"Accurate Addition - Train\" CoLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "czb-vRg_I1c_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp63uecjJel7",
        "outputId": "88944aeb-9bf2-4e54-efaa-8d0d5f923631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "rootdir is /content/drive/MyDrive/AI/CoLabOutput\n",
            "model will save to /content/drive/MyDrive/AI/CoLabOutput/add_digits5_layer2_heads3_dmodel510_dhead170_ctx18_seed129000_train30000.pt\n"
          ]
        }
      ],
      "source": [
        "GLOBAL=True\n",
        "if GLOBAL:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    rootdir=Path('/content/drive/MyDrive/AI/CoLabOutput/')\n",
        "else:\n",
        "    rootdir=Path('./')\n",
        "\n",
        "base_fname = '_digits{}_layer{}_heads{}_dmodel{}_dhead{}_ctx{}_seed{}_train{}.pt'.format(cfg.n_digits, cfg.n_layers, cfg.n_heads, cfg.d_model, cfg.d_head, cfg.n_ctx, cfg.seed, cfg.n_training_steps)\n",
        "\n",
        "add_fname = 'add' + base_fname\n",
        "model_save_location = rootdir/f'{add_fname}'\n",
        "\n",
        "print(f'rootdir is {rootdir}')\n",
        "print('model will save to {}'.format(str(model_save_location)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCdmr6-_Jkzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71bbce64-2631-43f5-d766-5ec44c9b5635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n"
          ]
        }
      ],
      "source": [
        "DEVELOPMENT_MODE = True\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    %pip install --upgrade numpy\n",
        "    %pip install matplotlib\n",
        "    %pip install prettytable\n",
        "    %pip install seaborn\n",
        "\n",
        "    %pip install kaleido\n",
        "    %pip install transformer_lens\n",
        "    %pip install torchtyping\n",
        "    %pip install transformers\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up2QLAZLJnG9"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import kaleido\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve-TndERJoaJ"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6zOEFryJqGN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from prettytable import PrettyTable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Use seaborn library to display heatmaps\n",
        "use_sns= True\n",
        "try:\n",
        "  import seaborn as sns\n",
        "except Exception as e:\n",
        "  print(\"sns import exception\", e)\n",
        "  use_sns = False\n",
        "\n",
        "# Use Principal Component Analysis (PCA) library\n",
        "use_pca = True\n",
        "try:\n",
        "  from sklearn.decomposition import PCA\n",
        "except Exception as e:\n",
        "  print(\"pca import exception\", e)\n",
        "  use_pca = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8VQ4e0QJsIB"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8RfHXneJw6n"
      },
      "source": [
        "# Part 3: Create model\n",
        "This section defines the token embedding / unembedding and creates the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QYFZIalJ3tK"
      },
      "outputs": [],
      "source": [
        "# Embedding / Unembedding\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    tokens = utils.to_numpy(tokens)\n",
        "    x = \"\".join([str(i) for i in tokens[:cfg.n_digits]])\n",
        "    y = \"\".join([str(i) for i in tokens[cfg.n_digits+1:cfg.n_digits*2+1]])\n",
        "    z = \"\".join([str(i) for i in tokens[cfg.n_ctx-cfg.n_digits-1:]])\n",
        "    equals = \"=\"\n",
        "    operator = \"+\"\n",
        "    return f\"{x}{operator}{y}{equals}{z}\"\n",
        "\n",
        "def string_to_tokens(string, batch: bool=False):\n",
        "    lookup = {str(i):i for i in range(10)}\n",
        "    lookup['+']=PLUS_INDEX\n",
        "    lookup['-']=MINUS_INDEX\n",
        "    lookup['=']=EQUALS_INDEX\n",
        "\n",
        "    tokens = [lookup[i] for i in string if i not in '\\n ']\n",
        "    if batch:\n",
        "        return torch.tensor(tokens)[None, :]\n",
        "    else:\n",
        "        return torch.tensor(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA16Nb2PJ7MB"
      },
      "outputs": [],
      "source": [
        "# Transformer creation\n",
        "\n",
        "# Structure is documented at https://neelnanda-io.github.io/TransformerLens/transformer_lens.html#transformer_lens.HookedTransformerConfig.HookedTransformerConfig\n",
        "ht_cfg = HookedTransformerConfig(\n",
        "    n_layers = cfg.n_layers,\n",
        "    n_heads = cfg.n_heads,\n",
        "    d_model = cfg.d_model,\n",
        "    d_head = cfg.d_head,\n",
        "    d_mlp = cfg.d_mlp,\n",
        "    act_fn = cfg.act_fn,\n",
        "    normalization_type = 'LN',\n",
        "    d_vocab = cfg.d_vocab,\n",
        "    d_vocab_out = cfg.d_vocab,\n",
        "    n_ctx = cfg.n_ctx,\n",
        "    init_weights = True,\n",
        "    device = \"cuda\",\n",
        "    seed = cfg.seed,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(ht_cfg)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(),\n",
        "                        lr = cfg.lr,\n",
        "                        weight_decay = cfg.weight_decay,\n",
        "                        betas = (0.9, 0.98))\n",
        "\n",
        "# Gives 2e-9 loss (but changes the cells used by the model)\n",
        "# max_iter = cfg.n_training_steps\n",
        "# warmup_iter = max_iter // 5\n",
        "# scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=int(warmup_iter))\n",
        "# scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(np.ceil((max_iter-warmup_iter))))\n",
        "# scheduler  = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[int(warmup_iter)])\n",
        "\n",
        "# Gives 3e-8 loss\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiJhch4KCej"
      },
      "source": [
        "# Part 4: Data Generator. Addition sub-task categorisation\n",
        "This section defines the loss function and the training/tesing data generator.\n",
        "\n",
        "It also defines functions to categorise the training data by the addition sub-task defined in the paper. The addition sub tasks are abbreviated as:\n",
        "- BA is Base Add. Calculates the sum of two digits Dn and Dn' modulo 10, ignoring any carry over from previous columns.\n",
        "- MC1 is Make Carry 1. Evaluates to true if adding digits Dn and Dn' results in a carry over of 1 to the next column.\n",
        "- MS9 is Make Sum 9. Evaluates to true if adding digits Dn and Dn' gives exactly 9.\n",
        "- UC1 is Use Carry 1. Takes the previous column's carry output and adds it to the sum of the current digit pair.\n",
        "- US9 is Use Sum 9. Propagates (aka cascades) a carry over of 1 to the next column if the current column sums to 9 and the previous column generated a carry over. US9 is the most complex task as it spans three digits. For some rare questions (e.g. 00555 + 00445 = 01000) US9 applies to up to four sequential digits, causing a chain effect, with the MC1 cascading through multiple digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ2iNO-nKDBW"
      },
      "outputs": [],
      "source": [
        "# Loss functions\n",
        "\n",
        "# Calculate the per-token probability by comparing a batch of prediction \"logits\" to answer \"tokens\"\n",
        "def logits_to_tokens_loss(logits, tokens):\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the addition answer probabilities\n",
        "  ans_logits = logits[:, -(cfg.n_digits+2):-1]\n",
        "\n",
        "  # Convert raw score (logits) vector into a probability distribution.\n",
        "  # Emphasize the largest scores and suppress the smaller ones, to make them more distinguishable.\n",
        "  ans_probs = F.log_softmax(ans_logits.to(torch.float64), dim=-1)\n",
        "\n",
        "  max_indices = torch.argmax(ans_probs, dim=-1)\n",
        "\n",
        "  # The last \"n_digit+1\" tokens are the modelâ€™s answer.\n",
        "  ans_tokens = tokens[:, -(cfg.n_digits+1):]\n",
        "\n",
        "  # Extract values from the ans_probs tensor, based on indices from the ans_tokens tensor\n",
        "  ans_loss = torch.gather(ans_probs, -1, ans_tokens[:, :, None])[..., 0]\n",
        "\n",
        "  return ans_loss, max_indices\n",
        "\n",
        "# Calculate loss as negative of average per-token mean probability\n",
        "def loss_fn(ans_loss):\n",
        "  return -ans_loss.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSp8pS1eKHf6"
      },
      "outputs": [],
      "source": [
        "# Define \"iterator\" data generator function. Invoked using next().\n",
        "# \"Addition\" batch entries are formated XXXXX+YYYYY=ZZZZZZ e.g. 55003+80002=135005\n",
        "# \"Subtraction\" batch entries are formated XXXXX-YYYYY=ZZZZZZ e.g. 55003-80002=-24999, 80002-55003=024999\n",
        "# Note that answer has one more digit than the question\n",
        "# Returns characteristics of each batch entry to aid later analysis\n",
        "def data_generator():\n",
        "    torch.manual_seed(cfg.seed)\n",
        "    while True:\n",
        "        #generate a batch of questions (answers calculated below)\n",
        "        batch = torch.zeros((cfg.batch_size, cfg.n_ctx)).to(torch.int64)\n",
        "        x = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "        y = torch.randint(0, 10, (cfg.batch_size, cfg.n_digits))\n",
        "\n",
        "\n",
        "        # The UseSum9 task is compound and rare and so hard to learn.\n",
        "        # For some batches, we increase the MakeSum9 case frequency\n",
        "        # UseSum9 also relies on MakeCarry1 (50%) from previous column.\n",
        "        # So UseSum9 frequency is increased by 60% * 40% * 50% = 12%\n",
        "        if random.randint(1, 5) < 3: # 60%\n",
        "          # Flatten x and y to 1D tensors\n",
        "          x_flat = x.view(-1)\n",
        "          y_flat = y.view(-1)\n",
        "\n",
        "          num_elements_to_modify = int(0.40 * x.numel()) # 40%\n",
        "          indices_to_modify = torch.randperm(x_flat.numel())[:num_elements_to_modify]\n",
        "          if random.randint(1, 2) == 1:\n",
        "            x_flat[indices_to_modify] = 9 - y_flat[indices_to_modify]\n",
        "          else:\n",
        "            y_flat[indices_to_modify] = 9 - x_flat[indices_to_modify]\n",
        "\n",
        "          # Reshape x and y back to its original shape\n",
        "          x = x_flat.view(x.shape)\n",
        "          y = y_flat.view(x.shape)\n",
        "\n",
        "\n",
        "        batch[:, :cfg.n_digits] = x\n",
        "        batch[:, cfg.n_digits] = PLUS_INDEX\n",
        "        batch[:, 1+cfg.n_digits:1+cfg.n_digits*2] = y\n",
        "        batch[:, 1+cfg.n_digits*2] = EQUALS_INDEX\n",
        "\n",
        "        # These attributes are used for testing addition\n",
        "        base_adds = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        make_carry1s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        sum9s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        use_carry1s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "        use_sum9s = torch.zeros((cfg.batch_size,cfg.n_digits)).to(torch.int64)\n",
        "\n",
        "        # generate the addition question answers & other info for testing\n",
        "        for i in range(cfg.n_digits):\n",
        "            # the column in the test attributes being updated\n",
        "            test_col = cfg.n_digits-1-i\n",
        "\n",
        "            base_add = batch[:, cfg.n_digits-1-i] + batch[:, 2*cfg.n_digits-i]\n",
        "            base_adds[:, test_col] = base_add % 10\n",
        "\n",
        "            sum9 = (base_add == 9)\n",
        "            sum9s[:, test_col] = sum9\n",
        "\n",
        "            if i>0:\n",
        "              use_carry1s[:, test_col] = make_carry1s[:, test_col+1]\n",
        "            use_carry = use_carry1s[:, test_col]\n",
        "\n",
        "            use_sum9s[:, test_col] = sum9 & use_carry;\n",
        "\n",
        "            digit_sum = base_add + use_carry1s[:, test_col]\n",
        "\n",
        "            make_carry = (digit_sum >= 10)\n",
        "            make_carry1s[:, test_col] = make_carry\n",
        "\n",
        "            batch[:, -1-i] = (digit_sum % 10)\n",
        "\n",
        "        # Final (possible) carry to highest digit of the sum\n",
        "        batch[:, -1-cfg.n_digits] = make_carry1s[:, 0]\n",
        "\n",
        "        yield batch.cuda(), base_adds.cuda(), make_carry1s.cuda(), sum9s.cuda(), use_carry1s.cuda(), use_sum9s.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqzljhQ4KJU5"
      },
      "outputs": [],
      "source": [
        "ds = data_generator()\n",
        "\n",
        "tokens, base_adds, make_carry1s, sum9s, use_carry1s, use_sum9s = next(ds)\n",
        "\n",
        "print(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KJhCxFtNKfm"
      },
      "source": [
        "# Part 5 Load Model from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2EgOq5wNPWu"
      },
      "outputs": [],
      "source": [
        "def print_config():\n",
        "  print(\"n_digits=\", cfg.n_digits, \"n_heads=\", cfg.n_heads, \"n_layers=\", cfg.n_layers, \"n_ctx=\", cfg.n_ctx, \"seed=\", cfg.seed, \"n_training_steps=\", cfg.n_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMkB_8GNRc0"
      },
      "outputs": [],
      "source": [
        "print(\"Loading model from file\", model_save_location)\n",
        "model.load_state_dict(torch.load(model_save_location))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxoBWHNKRf0"
      },
      "source": [
        "# Part 8: Sample Questions Set Up\n",
        "\n",
        "Create sets of sample questions (by task) to ask the model to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj_xSuSSKR9t"
      },
      "outputs": [],
      "source": [
        "# Insert a number into the question\n",
        "def insert_question_number(the_question, index, first_digit_index, the_digits, n):\n",
        "\n",
        "  last_digit_index = first_digit_index + the_digits - 1\n",
        "\n",
        "  for j in range(the_digits):\n",
        "    the_question[index, last_digit_index-j] = n % 10\n",
        "    n = n // 10\n",
        "\n",
        "\n",
        "# Create a single question\n",
        "def make_a_question(the_question, index, q1, q2):\n",
        "  a = q1 + q2\n",
        "\n",
        "  insert_question_number(the_question, index, 0, cfg.n_digits, q1)\n",
        "\n",
        "  the_question[index, cfg.n_digits] = PLUS_INDEX\n",
        "\n",
        "  insert_question_number( the_question, index, cfg.n_digits+1, cfg.n_digits, q2)\n",
        "\n",
        "  the_question[index, 2*cfg.n_digits+1] = EQUALS_INDEX\n",
        "  offset = 2\n",
        "\n",
        "  insert_question_number(the_question, index, 2*cfg.n_digits + offset, cfg.n_digits+1, q1+q2)\n",
        "\n",
        "\n",
        "# Create a batch of questions from a 2D matrix of ints\n",
        "def make_questions(q_matrix):\n",
        "  length = len(q_matrix)\n",
        "\n",
        "  questions = torch.zeros((length, cfg.n_ctx)).to(torch.int64)\n",
        "\n",
        "  limit = 10 ** cfg.n_digits\n",
        "  for i in range(length):\n",
        "    if (q_matrix[i][0] < limit) and (q_matrix[i][1] < limit) :\n",
        "      make_a_question(questions, i, q_matrix[i][0], q_matrix[i][1])\n",
        "\n",
        "  return questions\n",
        "\n",
        "\n",
        "def prediction_to_string(max_indices):\n",
        "  answer = \"\".join([str(i) for i in utils.to_numpy(max_indices)[0]])\n",
        "  return answer;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xiOHRfGKW-W"
      },
      "outputs": [],
      "source": [
        "# Analyse the question and return the use case as BA, MC, SimpleUS9 or CascadeUS9\n",
        "def get_question_case(q):\n",
        "  qa = utils.to_numpy(q)\n",
        "  qn = qa[:2*cfg.n_digits+2]\n",
        "\n",
        "  # Locate the MC and MS digits (if any)\n",
        "  mc = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  ms = torch.zeros( cfg.n_digits).to(torch.int64)\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if qn[dn] + qn[dn + cfg.n_digits + 1] == 9:\n",
        "      ms[cfg.n_digits-1-dn] = 1\n",
        "    if qn[dn] + qn[dn + cfg.n_digits +1] > 9:\n",
        "      mc[cfg.n_digits-1-dn] = 1\n",
        "\n",
        "  # Calculate the use case of a question\n",
        "  if torch.sum(mc) == 0:\n",
        "    return \"BA\"\n",
        "\n",
        "  if torch.sum(ms) == 0:\n",
        "    return \"MC1\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-2 and mc[dn] == 1 and ms[dn+1] == 1 and ms[dn+2] == 1:\n",
        "      return \"CascadeUS9\"\n",
        "\n",
        "  for dn in range(cfg.n_digits):\n",
        "    if dn < cfg.n_digits-1 and mc[dn] == 1 and ms[dn+1] == 1:\n",
        "      return \"SimpleUS9\"\n",
        "\n",
        "  return \"MC1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRCPyETTKaEs"
      },
      "outputs": [],
      "source": [
        "# Manually create some questions that strongly test one use case\n",
        "\n",
        "\n",
        "def make_ba_questions():\n",
        "    return make_questions(\n",
        "      [[12345, 33333],\n",
        "      [33333, 12345],\n",
        "      [45762, 33113],\n",
        "      [888, 11111],\n",
        "      [2362, 23123],\n",
        "      [15, 81],\n",
        "      [1000, 4440],\n",
        "      [4440, 1000],\n",
        "      [24033, 25133],\n",
        "      [23533, 21133],\n",
        "      [32500, 1],\n",
        "      [31500, 1111],\n",
        "      [5500, 12323],\n",
        "      [4500, 2209],\n",
        "      [ 33345, 66643], # =099988\n",
        "      [ 66643, 33345], # =099988\n",
        "      [10990, 44000],\n",
        "      [60000, 30000],\n",
        "      [10000, 20000]])\n",
        "\n",
        "\n",
        "def make_uc1_questions():\n",
        "    return make_questions(\n",
        "      [[ 15, 45],\n",
        "      [ 25, 55],\n",
        "      [ 35, 59],\n",
        "      [ 40035, 40049],\n",
        "      [ 5025, 5059],\n",
        "      [ 15, 65],\n",
        "      [ 44000, 46000],\n",
        "      [ 70000, 40000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 1500, 4500],\n",
        "      [ 2500, 5500],\n",
        "      [ 3500, 5900],\n",
        "      [ 15020, 45091],\n",
        "      [ 25002, 55019],\n",
        "      [ 35002, 59019]])\n",
        "\n",
        "\n",
        "def make_simple_us9_questions():\n",
        "    return make_questions(\n",
        "      [[ 55, 45],\n",
        "      [ 45, 55],\n",
        "      [ 45, 59],\n",
        "      [ 35, 69],\n",
        "      [ 25, 79],\n",
        "      [ 15, 85],\n",
        "      [ 15, 88],\n",
        "      [ 15508, 14500],\n",
        "      [ 14508, 15500],\n",
        "      [ 24533, 25933],\n",
        "      [ 23533, 26933],\n",
        "      [ 32500, 7900],\n",
        "      [ 31500, 8500],\n",
        "      [ 550, 450],\n",
        "      [ 450, 550],\n",
        "      [ 10880, 41127],\n",
        "      [ 41127, 10880],\n",
        "      [ 12386, 82623]])\n",
        "\n",
        "\n",
        "def make_cascade_us9_questions(clean = True):\n",
        "    return make_questions(\n",
        "      # These are two level UseSum9 cascades\n",
        "      [[ 555, 445],\n",
        "      [ 3340, 6660],\n",
        "      [ 8880, 1120],\n",
        "      [ 1120, 8880],\n",
        "      [ 123, 877],\n",
        "      [ 877, 123],\n",
        "      [ 321, 679],\n",
        "      [ 679, 321],\n",
        "      [ 1283, 88786],\n",
        "      # These are three level UseSum9 cascades\n",
        "      [ 5555, 4445],\n",
        "      [ 55550, 44450],\n",
        "      [ 334, 666],\n",
        "      [ 3340, 6660],\n",
        "      [ 33400, 66600],\n",
        "      [ 888, 112],\n",
        "      [ 8880, 1120],\n",
        "      [ 88800, 11200],\n",
        "      [ 1234, 8766],\n",
        "      [ 4321, 5679],\n",
        "      # These are four level UseSum9 cascades\n",
        "      [ 44445, 55555],\n",
        "      [ 33334, 66666],\n",
        "      [ 88888, 11112],\n",
        "      [ 12345, 87655],\n",
        "      [ 54321, 45679],\n",
        "      [ 45545, 54455],\n",
        "      [ 36634, 63366],\n",
        "      [ 81818, 18182],\n",
        "      [ 87345, 12655],\n",
        "      [ 55379, 44621]])\n",
        "\n",
        "\n",
        "# These questions focus mainly on 1 digit at a time\n",
        "# (We're assuming that the 0 + 0 digit additions are trivial bigrams)\n",
        "def make_answerdigit_questions():\n",
        "    return make_questions(\n",
        "      [[ 1, 0],\n",
        "      [ 4, 3],\n",
        "      [ 5, 5],\n",
        "      [ 8, 1],\n",
        "      [ 40, 30],\n",
        "      [ 44, 46],\n",
        "      [ 400, 300],\n",
        "      [ 440, 460],\n",
        "      [ 800, 100],\n",
        "      [ 270, 470],\n",
        "      [ 600, 300],\n",
        "      [ 4000, 3000],\n",
        "      [ 4400, 4600],\n",
        "      [ 6000, 3000],\n",
        "      [ 7000, 4000],\n",
        "      [ 40000, 30000],\n",
        "      [ 44000, 46000],\n",
        "      [ 60000, 30000],\n",
        "      [ 70000, 40000],\n",
        "      [ 10000, 20000],\n",
        "      [ 15000, 25000],\n",
        "      [ 35000, 35000],\n",
        "      [ 45000, 85000],\n",
        "      [ 67000, 85000],\n",
        "      [ 99000, 76000],\n",
        "      [ 76000, 99000]])\n",
        "\n",
        "\n",
        "# Returns 128 random and ~100 manually-chosen questions\n",
        "def make_varied_questions():\n",
        "  q0, _, _, _, _, _ = next(ds)\n",
        "  q1 = make_ba_questions()\n",
        "  q2 = make_uc1_questions()\n",
        "  q3 = make_simple_us9_questions()\n",
        "  q4 = make_cascade_us9_questions()\n",
        "  q5 = make_answerdigit_questions()\n",
        "  q6, _, _, _, _, _ = next(ds)\n",
        "\n",
        "  questions = torch.vstack((q0.cuda(), q1.cuda(), q2.cuda(), q3.cuda(), q4.cuda(), q5.cuda(), q6.cuda()))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFzN_OxmKcEx"
      },
      "outputs": [],
      "source": [
        "# Test that the get_question_case code works as expected\n",
        "def unit_test_get_question_case_core(correct_case, questions):\n",
        "  num_questions = questions.shape[0]\n",
        "  print( correct_case, \"#Questions=\", num_questions)\n",
        "  for i in range(num_questions):\n",
        "    question_case = get_question_case(questions[i])\n",
        "    if question_case != correct_case:\n",
        "      print( \"Case mismatch:\", correct_case, question_case, questions[i])\n",
        "\n",
        "def unit_test_get_question_case():\n",
        "  unit_test_get_question_case_core( \"BA\", make_ba_questions())\n",
        "  unit_test_get_question_case_core( \"MC1\", make_uc1_questions())\n",
        "  unit_test_get_question_case_core( \"SimpleUS9\", make_simple_us9_questions())\n",
        "  unit_test_get_question_case_core( \"CascadeUS9\", make_cascade_us9_questions())\n",
        "\n",
        "unit_test_get_question_case()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E56siA_QKe0W"
      },
      "outputs": [],
      "source": [
        "num_questions = 0;\n",
        "correct_answers = 0;\n",
        "verbose = True\n",
        "total_mean_loss = 0.0\n",
        "\n",
        "\n",
        "# Clear the question summary results\n",
        "def clear_questions_results(title):\n",
        "  global num_questions\n",
        "  global correct_answers\n",
        "  global verbose\n",
        "  global total_mean_loss\n",
        "\n",
        "  num_questions = 0\n",
        "  correct_answers = 0\n",
        "  total_mean_loss = 0\n",
        "\n",
        "  if verbose:\n",
        "    print(title)\n",
        "\n",
        "\n",
        "# Ask model to predict answer for each question & collect results\n",
        "def do_questions(questions):\n",
        "    global num_questions\n",
        "    global correct_answers\n",
        "    global verbose\n",
        "    global total_mean_loss\n",
        "\n",
        "    num_questions = questions.shape[0]\n",
        "    for question_num in range(num_questions):\n",
        "      q = questions[question_num]\n",
        "\n",
        "      # Run with no hook\n",
        "      the_logits = model(q.cuda())\n",
        "\n",
        "      q_2d = q.unsqueeze(0)\n",
        "      losses_raw, max_indices = logits_to_tokens_loss(the_logits, q_2d.cuda())\n",
        "      losses = loss_fn(losses_raw)\n",
        "      mean_loss = utils.to_numpy(losses.mean())\n",
        "      total_mean_loss = total_mean_loss + mean_loss\n",
        "\n",
        "      model_answer_str = prediction_to_string(max_indices)\n",
        "      model_answer_num = int(model_answer_str)\n",
        "\n",
        "      i = cfg.n_digits*2 + 2\n",
        "\n",
        "      a = 0\n",
        "      # 5 digit addition yields a 6 digit answer. Hence cfg.n_digits+1\n",
        "      for j in range(cfg.n_digits+1):\n",
        "        a = a * 10 + q[i+j]\n",
        "\n",
        "      correct = (model_answer_num == a)\n",
        "      if correct :\n",
        "        correct_answers += 1\n",
        "\n",
        "      if verbose:\n",
        "        print(tokens_to_string(q), \"ModelAnswer:\", model_answer_str, \"Correct:\", correct, \"Loss:\", mean_loss )\n",
        "\n",
        "\n",
        "# Print the question summary results\n",
        "def print_questions_results(prefix, output_table):\n",
        "  global num_questions\n",
        "  global correct_answers\n",
        "  global total_mean_loss\n",
        "\n",
        "  output_table.add_row([prefix, num_questions, str(correct_answers), 100*correct_answers/num_questions, total_mean_loss/num_questions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RHrsjKqKhR4"
      },
      "outputs": [],
      "source": [
        "# Build a test batch of 64 random and ~100 manually-chosen questions\n",
        "varied_questions = make_varied_questions();\n",
        "\n",
        "\n",
        "# Run the sample batch, gather the cache\n",
        "model.reset_hooks()\n",
        "model.set_use_attn_result(True)\n",
        "sample_logits, sample_cache = model.run_with_cache(varied_questions.cuda())\n",
        "print(sample_cache) # Gives names of datasets in the cache\n",
        "sample_losses_raw, sample_max_indices = logits_to_tokens_loss(sample_logits, varied_questions.cuda())\n",
        "sample_loss_mean = utils.to_numpy(loss_fn(sample_losses_raw).mean())\n",
        "print(\"Sample Mean Loss\", sample_loss_mean) # Loss < 0.04 is good\n",
        "\n",
        "\n",
        "# attn.hook_z is the \"attention head output\" hook point name (at a specified layer)\n",
        "# Used in h_set_attn_hook_z and t_*_hook functions\n",
        "l_attn_hook_z_name = ['blocks.0.attn.hook_z','blocks.1.attn.hook_z']\n",
        "sample_attn_z = sample_cache[l_attn_hook_z_name[0]]\n",
        "print(\"Sample\", l_attn_hook_z_name[0], sample_attn_z.shape) # gives [239, 18, 3, 170] = #questions, cfg.n_ctx, n_heads, d_head\n",
        "mean_attn_z = torch.mean(sample_attn_z, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_attn_hook_z_name[0], mean_attn_z.shape) # gives [1, 18, 3, 170] = 1, cfg.n_ctx, n_heads, d_head\n",
        "\n",
        "\n",
        "# hook_resid_pre is the \"pre residual memory update\" hook point name (at a specified layer)\n",
        "# Used in o_*_hook functions\n",
        "l_hook_resid_pre_name = ['blocks.0.hook_resid_pre','blocks.1.hook_resid_pre']\n",
        "\n",
        "\n",
        "# hook_resid_post is the \"post residual memory update\" hook point name (at a specified layer)\n",
        "# Used in c_*_hook and t_*_hook functions\n",
        "l_hook_resid_post_name = ['blocks.0.hook_resid_post','blocks.1.hook_resid_post']\n",
        "sample_resid_post = sample_cache[l_hook_resid_post_name[0]]\n",
        "print(\"Sample\", l_hook_resid_post_name[0], sample_resid_post.shape) # gives [239, 18, 510] = #questions, cfg.n_ctx, d_model\n",
        "mean_resid_post = torch.mean(sample_resid_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_hook_resid_post_name[0], mean_resid_post.shape) # gives [1, 18, 510] = 1, cfg.n_ctx, d_model\n",
        "\n",
        "\n",
        "# mlp.hook_post is the \"MLP layer\" hook point name (at a specified layer)\n",
        "# Used in m_*_hook functions\n",
        "l_mlp_hook_post_name = ['blocks.0.mlp.hook_post','blocks.1.mlp.hook_post']\n",
        "sample_mlp_hook_post = sample_cache[l_mlp_hook_post_name[0]]\n",
        "print(\"Sample\", l_mlp_hook_post_name[0], sample_mlp_hook_post.shape) # gives [239, 18, 2040] = #questions, cfg.n_ctx, d_model*4\n",
        "mean_mlp_hook_post = torch.mean(sample_mlp_hook_post, dim=0, keepdim=True)\n",
        "print(\"Mean\", l_mlp_hook_post_name[0], mean_mlp_hook_post.shape) # gives [1, 18, 2040] = 1, cfg.n_ctx, d_model*4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lag8C3d_KkeQ"
      },
      "source": [
        "# Part 9: Prediction Analysis By Use Case\n",
        "This section sets up BA, UC1 and US9 test cases that will be re-used in later experiments to show the impact of ablating heads or token positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z6RdolRKnnR"
      },
      "outputs": [],
      "source": [
        "exp0_output = PrettyTable()\n",
        "exp0_output.field_names = [\"Case\", \"#Questions\", \"#Correct\", \"%Correct\", \"Mean loss\"]\n",
        "verbose = False\n",
        "\n",
        "clear_questions_results(\"Simple BaseAdd cases\")\n",
        "do_questions(make_ba_questions())\n",
        "print_questions_results(\"BaseAdd\", exp0_output)\n",
        "sum_total_mean_loss = total_mean_loss\n",
        "sum_num_questions = num_questions\n",
        "\n",
        "clear_questions_results(\"These are Use Carry 1 (UC1) examples (not UseSum9 examples)\")\n",
        "do_questions(make_uc1_questions())\n",
        "print_questions_results(\"UseCarry1\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "clear_questions_results(\"These are simple (one level) UseSum9 exampless\")\n",
        "do_questions(make_simple_us9_questions())\n",
        "print_questions_results(\"SimpleUS9\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "clear_questions_results(\"These are UseSum9 two, three and four level cascades\")\n",
        "do_questions(make_cascade_us9_questions())\n",
        "print_questions_results(\"CascadeUS9\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "clear_questions_results(\"These questions focus on different answer digits\")\n",
        "do_questions(make_answerdigit_questions())\n",
        "print_questions_results(\"AnswerDigits\", exp0_output)\n",
        "sum_total_mean_loss = sum_total_mean_loss + total_mean_loss\n",
        "sum_num_questions = sum_num_questions + num_questions\n",
        "\n",
        "exp0_output.add_row([\"OVERALL\", sum_num_questions, \"\", \"\", sum_total_mean_loss])\n",
        "\n",
        "print_config()\n",
        "print()\n",
        "print(exp0_output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyK7QeUjLLFm"
      },
      "source": [
        "# Part 11: Set Up \"Count\" Framework\n",
        "\n",
        "Create way to get model to predict sample question answers and analysis/show results. Use prefix \"c_\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYPI4tw0LNhV"
      },
      "outputs": [],
      "source": [
        "# Build up a list of success/failure by case (BA, MC1, US9) found, and the frequency of each case\n",
        "c_case_counts = {}\n",
        "\n",
        "\n",
        "def count_question_cases(questions):\n",
        "  global c_case_counts\n",
        "\n",
        "  c_case_counts = {}\n",
        "\n",
        "  for i in range(questions.shape[0]):\n",
        "    q_case = get_question_case(questions[i])\n",
        "\n",
        "    if q_case in c_case_counts:\n",
        "      # If the key is already in the dictionary, increment its count\n",
        "      c_case_counts[q_case] += 1\n",
        "    else:\n",
        "      # If the key is not in the dictionary, add it with a count of 1\n",
        "      c_case_counts[q_case] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZXEfGBMLPFW"
      },
      "outputs": [],
      "source": [
        "# Compare each digit in the answer. Returns a A45 pattern where 4 each digit means a failed digit\n",
        "def get_digit_accuracy_impact(a_int, answer_str):\n",
        "  a_str = str(a_int.cpu().numpy()).zfill(cfg.n_digits+1)\n",
        "  match_str = \"A\"\n",
        "  for i in range(cfg.n_digits+1):\n",
        "    match_str += \"\" if answer_str[i] == a_str[i] else str(cfg.n_digits-i)\n",
        "\n",
        "  return \"\" if match_str == \"A\" else match_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liT_UbPOLRhO"
      },
      "outputs": [],
      "source": [
        "# Build up a list of success/failure digit-patterns found, and the frequency of each pattern\n",
        "c_pattern_fails = {}\n",
        "\n",
        "\n",
        "def clear_pattern_fails():\n",
        "  global c_pattern_fails\n",
        "\n",
        "  c_pattern_fails = {}\n",
        "\n",
        "\n",
        "def add_pattern_fail(match_str):\n",
        "  global c_pattern_fails\n",
        "\n",
        "  if match_str in c_pattern_fails:\n",
        "    # If the key is already in the dictionary, increment its count\n",
        "    c_pattern_fails[match_str] += 1\n",
        "  else:\n",
        "    # If the key is not in the dictionary, add it with a count of 1\n",
        "    c_pattern_fails[match_str] = 1\n",
        "\n",
        "\n",
        "def get_pattern_fails():\n",
        "  global c_pattern_fails\n",
        "\n",
        "  results = \"\"\n",
        "  top_result = \"\"\n",
        "  if len(c_pattern_fails) > 0 :\n",
        "    sorted_fails = dict(sorted(c_pattern_fails.items(), key=lambda item: item[1], reverse=True))\n",
        "    for key, value in sorted_fails.items():\n",
        "      this_cell = key + \"=\" + str(value)\n",
        "\n",
        "      results = results + this_cell + \" \"\n",
        "\n",
        "      if top_result == \"\":\n",
        "        top_result = this_cell\n",
        "      else:\n",
        "        top_result = top_result + \", \" + this_cell\n",
        "\n",
        "  return results, top_result\n",
        "\n",
        "\n",
        "def get_pattern_fails_total():\n",
        "  global c_pattern_fails\n",
        "\n",
        "  if len(c_pattern_fails) == 0:\n",
        "    return 0\n",
        "\n",
        "  total_sum = 0\n",
        "  for key, value in c_pattern_fails.items():\n",
        "      if isinstance(value, int):\n",
        "          total_sum += value\n",
        "  return total_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TudM4_8PLTh1"
      },
      "outputs": [],
      "source": [
        "# Build up a count of failure cases\n",
        "c_case_fails = {}\n",
        "\n",
        "\n",
        "def clear_case_fails():\n",
        "  global c_case_fails\n",
        "\n",
        "  c_case_fails = {}\n",
        "\n",
        "\n",
        "def add_case_fail(case_key):\n",
        "  global c_case_fails\n",
        "\n",
        "  if case_key in c_case_fails:\n",
        "    # If the key is already in the dictionary, increment its count\n",
        "    c_case_fails[case_key] += 1\n",
        "  else:\n",
        "    # If the key is not in the dictionary, add it with a count of 1\n",
        "    c_case_fails[case_key] = 1\n",
        "\n",
        "\n",
        "def total_case_fails():\n",
        "  global c_case_fails\n",
        "\n",
        "  answer = 0\n",
        "  for _, value in c_case_fails.items():\n",
        "    answer = answer + value\n",
        "  return answer\n",
        "\n",
        "\n",
        "def get_case_fails():\n",
        "  global c_case_fails\n",
        "  global c_case_counts\n",
        "\n",
        "  results = \"\"\n",
        "  num_results = len(c_case_fails)\n",
        "  if num_results > 0:\n",
        "    sorted_fails = dict(sorted(c_case_fails.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    for key, value in sorted_fails.items():\n",
        "      percent = round(100 * value / c_case_counts[key])\n",
        "      results = results + \"%\" + key + \"=\" + str(percent)+ \" \"\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqmNRgCgLWV1"
      },
      "outputs": [],
      "source": [
        "def predict_experiment_question(questions, the_hook, the_threshold):\n",
        "\n",
        "  c_loss_mean = 0\n",
        "\n",
        "  clear_case_fails()\n",
        "  clear_pattern_fails()\n",
        "  count_question_cases(questions)\n",
        "\n",
        "  for question_num in range(questions.shape[0]):\n",
        "    q = questions[question_num]\n",
        "\n",
        "    model.reset_hooks()\n",
        "    model.set_use_attn_result(True)\n",
        "    exp_logits = model.run_with_hooks(q.cuda(), return_type=\"logits\", fwd_hooks=the_hook)\n",
        "\n",
        "    q_2d = q.unsqueeze(0)\n",
        "    exp_losses_raw, exp_max_indices = logits_to_tokens_loss(exp_logits, q_2d.cuda())\n",
        "    c_loss_mean = utils.to_numpy(loss_fn(exp_losses_raw).mean())\n",
        "\n",
        "    # Only show the question if the loss exceeds the threshold (because of the ablated token position)\n",
        "    if c_loss_mean > the_threshold:\n",
        "      exp_answer_str = prediction_to_string(exp_max_indices)\n",
        "\n",
        "      i = 12\n",
        "      a = q[i+0] * 100000 + q[i+1] * 10000 + q[i+2] * 1000 + q[i+3] * 100 + q[i+4] * 10 + q[i+5] * 1;\n",
        "\n",
        "      match_str = get_digit_accuracy_impact( a, exp_answer_str )\n",
        "      # Only count the question if the model got the question wrong\n",
        "      if 'A' in match_str:\n",
        "        the_case = get_question_case(q)\n",
        "        add_case_fail(the_case)\n",
        "        add_pattern_fail(match_str)\n",
        "        if verbose:\n",
        "          print(tokens_to_string(q), \"ModelAnswer:\", exp_answer_str, \"Matches:\", match_str, \"Loss:\", c_loss_mean, \"Case:\", the_case )\n",
        "\n",
        "  return c_loss_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmsGWUbILYin"
      },
      "source": [
        "# Part 12: Ablate ALL Heads in EACH token position. What is the impact on Loss?\n",
        "\n",
        "Here we ablate all heads in each token position (overriding the model memory aka residual stream) and see if loss increases. If loss increases the token position is used by the algorithm. Unused token positions can be excluded from further analysis. Use \"C_\" prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx7LwECBLajQ"
      },
      "outputs": [],
      "source": [
        "class C_Config():\n",
        "  position : int = 0  # zero-based token position to ablate\n",
        "  threshold : float = 0.01\n",
        "  questions = varied_questions\n",
        "  output = PrettyTable()\n",
        "  perc_list = []\n",
        "  hook_calls : int = 0\n",
        "\n",
        "  min_useful_position : int = -1 # Minimum useful position where loss increases on ablation\n",
        "  max_useful_position : int = -1 # Maximum useful position where loss increases on ablation\n",
        "\n",
        "\n",
        "ccfg = C_Config()\n",
        "ccfg.output.field_names = [\"Position\", \"Fails\", \"% Fails by Case\", \"# Fails by Patterns\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGL57MRBLdUh"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "\n",
        "\n",
        "def c_set_resid_post_hook(value, hook):\n",
        "  global ccfg\n",
        "\n",
        "  #print( \"In hook\", l_hook_resid_post_name[ccfg.layer], ccfg.ablate, ccfg.position, value.shape) # Get [64, 18, 510] = cfg.batch_size, num_tokens, d_model\n",
        "\n",
        "  # Copy the mean resid post values in position N to all the batch questions\n",
        "  value[:,ccfg.position,:] = mean_resid_post[0,ccfg.position,:].clone()\n",
        "\n",
        "\n",
        "num_questions = 0\n",
        "if cfg.n_digits >= 5 :\n",
        "  c_fwd_hooks = [(l_hook_resid_post_name[0], c_set_resid_post_hook)] if cfg.n_layers == 1 else [(l_hook_resid_post_name[0], c_set_resid_post_hook),(l_hook_resid_post_name[1], c_set_resid_post_hook)]\n",
        "\n",
        "  num_questions = ccfg.questions.shape[0]\n",
        "\n",
        "  for ccfg.position in range(cfg.n_ctx):\n",
        "    loss_mean = predict_experiment_question(ccfg.questions, c_fwd_hooks, ccfg.threshold)\n",
        "\n",
        "    num_fails = total_case_fails()\n",
        "    perc_fails = 0\n",
        "    if num_fails > 0:\n",
        "      perc_fails = round(100 * num_fails / num_questions)\n",
        "\n",
        "      if ccfg.min_useful_position == -1:\n",
        "        ccfg.min_useful_position = ccfg.position\n",
        "      ccfg.max_useful_position = ccfg.position\n",
        "\n",
        "    ccfg.perc_list = ccfg.perc_list + [perc_fails]\n",
        "\n",
        "    (pattern_results, top_pattern) = get_pattern_fails()\n",
        "    ccfg.output.add_row([str(ccfg.position), str(perc_fails)+\"%\", get_case_fails(), pattern_results])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpRp5YMmLe1y"
      },
      "outputs": [],
      "source": [
        "print_config()\n",
        "print(\"num_questions=\", num_questions, \"min_useful_position=\", ccfg.min_useful_position, \"max_useful_position=\", ccfg.max_useful_position )\n",
        "print()\n",
        "\n",
        "plt.hist(ccfg.perc_list, cfg.n_ctx, facecolor='blue', alpha=0.5)\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(r'Histogram of IQ: $\\mu=100$, $\\sigma=15$')\n",
        "# Tweak spacing to prevent clipping of ylabel\n",
        "plt.subplots_adjust(left=0.15)\n",
        "plt.show()\n",
        "\n",
        "print(ccfg.output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904WBkTOLg_5"
      },
      "source": [
        "# Part 13: Setup: Cell matrix\n",
        "\n",
        "Uses \"u_\" prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qgSo4acLj5D"
      },
      "outputs": [],
      "source": [
        "class UsefulCell():\n",
        "  # Is this cell an attention head? If not, it must be an MLP layer\n",
        "  is_head: bool = True\n",
        "\n",
        "  position: int = 0  # token-position\n",
        "  layer: int = 0\n",
        "  head: int = 0\n",
        "\n",
        "\n",
        "# We (once) calculate the list of cells (attention head and MLP layers per position) that are useful to the model.\n",
        "calc_useful_cells = True\n",
        "# Once this list of useful cells is calculated (available) it is used to speed up functions.\n",
        "useful_cells = []\n",
        "\n",
        "\n",
        "def add_useful_cell(the_is_head, the_position, the_layer, the_head):\n",
        "  global calc_useful_cells\n",
        "  global useful_cells\n",
        "\n",
        "  if calc_useful_cells:\n",
        "    useful_cell = UsefulCell()\n",
        "    useful_cell.is_head = the_is_head\n",
        "    useful_cell.position = the_position\n",
        "    useful_cell.layer = the_layer\n",
        "    useful_cell.head = the_head\n",
        "\n",
        "    useful_cells = useful_cells + [useful_cell]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTaSo-UzLlZ_"
      },
      "outputs": [],
      "source": [
        "class U_Config():\n",
        "  # This is a head+MLP (row) by token (column) matrix of percent of failure percentages with associated notes\n",
        "  fail_percs = [[]]\n",
        "  fail_notes = [[]]\n",
        "  num_heads = 0\n",
        "  num_mlps = 0\n",
        "\n",
        "\n",
        "ucfg = U_Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wSoqTt1LofE"
      },
      "outputs": [],
      "source": [
        "def ucfg_reset():\n",
        "  global ucfg\n",
        "\n",
        "  ucfg.fail_percs = [[0 for _ in range(cfg.n_ctx)] for _ in range((cfg.n_heads + 1) * cfg.n_layers)]\n",
        "  ucfg.fail_notes = [[\"\" for _ in range(cfg.n_ctx)] for _ in range((cfg.n_heads + 1) * cfg.n_layers)]\n",
        "  ucfg.num_heads = 0\n",
        "  ucfg.num_mlps = 0\n",
        "\n",
        "\n",
        "ucfg_reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ28dx5YLs0P"
      },
      "outputs": [],
      "source": [
        "def add_u_fail_perc( the_position, the_layer, the_head, perc_fails, notes ):\n",
        "  global ucfg\n",
        "\n",
        "  the_row = the_layer * (cfg.n_heads+1) + the_head\n",
        "\n",
        "  if ucfg.fail_percs[the_row][the_position] == 0 :\n",
        "    ucfg.fail_percs[the_row][the_position] = perc_fails\n",
        "\n",
        "    add_useful_cell(the_head != cfg.n_heads, the_position, the_layer, the_head)\n",
        "\n",
        "  else:\n",
        "    print( \"add_u_fail_perc: Bad index\", the_row, the_position)\n",
        "\n",
        "  ucfg.fail_percs[the_row][the_position] = perc_fails\n",
        "  ucfg.fail_notes[the_row][the_position] = notes\n",
        "\n",
        "\n",
        "def add_head_fail_perc( the_position, the_layer, the_head, perc_fails, notes ):\n",
        "  global ucfg\n",
        "\n",
        "  add_u_fail_perc( the_position, the_layer, the_head, perc_fails, notes )\n",
        "  ucfg.num_heads += 1\n",
        "\n",
        "\n",
        "def add_mlp_fail_perc( the_position, the_layer, perc_fails, notes ):\n",
        "  global ucfg\n",
        "\n",
        "  add_u_fail_perc( the_position, the_layer, cfg.n_heads, perc_fails, notes )\n",
        "  ucfg.num_mlps += 1\n",
        "\n",
        "\n",
        "def get_column_headings():\n",
        "  datums = [\"Position\"]\n",
        "  for i in range(ccfg.min_useful_position, ccfg.max_useful_position+1):\n",
        "    datums = datums + [\"P\"+str(i)]\n",
        "  return datums\n",
        "\n",
        "\n",
        "def get_row_heading(i):\n",
        "  head = i % (cfg.n_heads + 1)\n",
        "  layer = i // (cfg.n_heads + 1)\n",
        "  return ( \"L\" + str(layer) + \"H\" + str(head) ) if head < cfg.n_heads else \"MLP \"\n",
        "\n",
        "\n",
        "# Print a 2 by 2 matrix of the percentage failures.\n",
        "def print_u_fail_percs():\n",
        "  global ucfg\n",
        "  global mcfg\n",
        "\n",
        "  print(\"The % failure rate when each head or MLP in each position is ablated, # failed heads =\", ucfg.num_heads, \", # failed mlps =\", ucfg.num_mlps )\n",
        "\n",
        "  cell_output = PrettyTable()\n",
        "\n",
        "  col_headings = get_column_headings()\n",
        "  cell_output.field_names = col_headings\n",
        "\n",
        "  num_rows = (cfg.n_heads + 1) * cfg.n_layers\n",
        "  num_cols = ccfg.max_useful_position - ccfg.min_useful_position + 1\n",
        "  percs_matrix = torch.zeros((num_rows, num_cols)).to(torch.int64)\n",
        "  mask_matrix = torch.zeros((num_rows, num_cols)).to(torch.int64)\n",
        "  row_headings = []\n",
        "\n",
        "  for i in range(num_rows):\n",
        "    row_heading = get_row_heading(i)\n",
        "    row_headings = row_headings + [row_heading]\n",
        "\n",
        "    datums = [row_heading]\n",
        "    for j in range(num_cols):\n",
        "      value = ucfg.fail_percs[i][ccfg.min_useful_position + j]\n",
        "      datums = datums + [\"\" if value == 0 else str(value)+\"%\"]\n",
        "      percs_matrix[i,j] = value\n",
        "      mask_matrix[i,j] = ( value == 0 )\n",
        "\n",
        "    cell_output.add_row(datums)\n",
        "\n",
        "  # Display a 2D heat map of the percentages\n",
        "  if use_sns == True:\n",
        "\n",
        "    sns.set_theme(rc={\"figure.dpi\": 96}) # use higher resolution\n",
        "    # %config InlineBackend.figure_format = \"svg\"\n",
        "    sns.set(font_scale=0.8)\n",
        "    sns.heatmap(utils.to_numpy(percs_matrix), annot=True, mask=utils.to_numpy(mask_matrix), cmap=\"YlGnBu\", xticklabels=col_headings[1:], yticklabels=row_headings)\n",
        "    plt.show()\n",
        "  else:\n",
        "    # Display a \"pretty\" table in html for use in blog\n",
        "    print(cell_output.get_formatted_string(out_format=cfg.table_out_format))\n",
        "\n",
        "\n",
        "# Print a 2 by 2 matrix of notes.\n",
        "def print_u_fail_notes():\n",
        "  global ucfg\n",
        "  global mcfg\n",
        "\n",
        "  print(\"The most common failure pattern (with associated failure #) when each head or MLP in each position is ablated\")\n",
        "\n",
        "  cell_output = PrettyTable()\n",
        "  cell_output.field_names = get_column_headings()\n",
        "\n",
        "  for i in range((cfg.n_heads + 1) * cfg.n_layers):\n",
        "    datums = [get_row_heading(i)]\n",
        "    for j in range(ccfg.min_useful_position, ccfg.max_useful_position+1):\n",
        "      datums = datums + [ucfg.fail_notes[i][j]]\n",
        "    cell_output.add_row(datums)\n",
        "\n",
        "  print(cell_output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99LIC1l4MD32"
      },
      "source": [
        "# Part 14: Setup: Ablate each MLP in EACH position. Impact on Loss?\n",
        "Ablating the MLP in each layer in each position and seeing if the loss increases shows which head+layer+MLP are used by the algorithm. Use \"m_\" prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOxNc9SAMGRH"
      },
      "outputs": [],
      "source": [
        "class M_Config():\n",
        "  position : int = 0  # zero-based token-position to ablate\n",
        "  layer : int = 0 # zero-based layer to ablate. 0 to 1\n",
        "  threshold : float = 0.12\n",
        "  questions = varied_questions\n",
        "  output = PrettyTable()\n",
        "  hook_calls : int = 0\n",
        "\n",
        "\n",
        "mcfg = M_Config()\n",
        "\n",
        "\n",
        "def m_reset():\n",
        "  global mcfg\n",
        "\n",
        "  mcfg.output = PrettyTable()\n",
        "  mcfg.output.field_names = [\"Position\", \"MLP Layer\", \"% Fails\", \"% Fails by Case\", \"# Fails by Patterns\"]\n",
        "  mcfg.hook_calls = 0\n",
        "\n",
        "\n",
        "def m_mlp_hook_post(value, hook):\n",
        "  global mcfg\n",
        "\n",
        "  mcfg.hook_calls += 1\n",
        "  #print( \"In m_mlp_hook_post\", value.shape) # Get [1, 18, 2040] = ???, cfg.n_ctx, ???\n",
        "\n",
        "  # Mean ablate. Copy the mean resid post values in position N to the MLP\n",
        "  value[:,mcfg.position,:] =  mean_mlp_hook_post[:,mcfg.position,:].clone()\n",
        "\n",
        "\n",
        "def m_perform_core(show_all = False):\n",
        "  global mcfg\n",
        "\n",
        "  the_hook = [(l_mlp_hook_post_name[mcfg.layer], m_mlp_hook_post)]\n",
        "  loss_mean = predict_experiment_question(mcfg.questions, the_hook, mcfg.threshold)\n",
        "\n",
        "  num_fails = total_case_fails()\n",
        "  if show_all or (num_fails > 0):\n",
        "    perc_fails = round(100 * num_fails / mcfg.questions.shape[0])\n",
        "    (pattern_results, top_pattern) = get_pattern_fails()\n",
        "\n",
        "    mcfg.output.add_row([str(mcfg.position), str(mcfg.layer), perc_fails, get_case_fails(), pattern_results])\n",
        "\n",
        "    add_mlp_fail_perc( mcfg.position, mcfg.layer, perc_fails, top_pattern )\n",
        "\n",
        "\n",
        "def m_perform(all_cells):\n",
        "  global mcfg\n",
        "\n",
        "  if cfg.n_digits >= 5 :\n",
        "    ucfg_reset()\n",
        "    m_reset()\n",
        "    if all_cells:\n",
        "      for mcfg.position in range(cfg.n_ctx):\n",
        "        for mcfg.layer in range(cfg.n_layers):\n",
        "          m_perform_core()\n",
        "    else:\n",
        "      for useful_cell in useful_cells:\n",
        "        if not useful_cell.is_head:\n",
        "          mcfg.position = useful_cell.position\n",
        "          mcfg.layer = useful_cell.layer\n",
        "          m_perform_core()\n",
        "\n",
        "\n",
        "def m_print_results(title):\n",
        "    global mcfg\n",
        "\n",
        "    print_config()\n",
        "    print()\n",
        "    print(title, mcfg.questions.shape[0])\n",
        "    print(mcfg.output.get_formatted_string(out_format=cfg.table_out_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKZF_B6OMIq3"
      },
      "source": [
        "# Part 15: Setup: Ablate EACH head in EACH position. Impact on Digit & Task Loss?\n",
        "Ablating each head in each layer in each position and seeing if the loss increases shows which position+layer+head are used by the algorithm. Use \"h_\" prefix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2EnisO6MMGQ"
      },
      "outputs": [],
      "source": [
        "class H_Config():\n",
        "  position : int = 0 # zero-based token position to ablate. 0 to say 17\n",
        "  layer : int = 0 # zero-based layer to ablate. 0 to 1\n",
        "  head : int = 0 # zero-based head to ablate. 0 to 2\n",
        "  threshold : float = 0.12\n",
        "  questions = varied_questions\n",
        "  output = PrettyTable()\n",
        "  hook_calls: int = 0\n",
        "\n",
        "\n",
        "hcfg = H_Config()\n",
        "\n",
        "\n",
        "def h_reset():\n",
        "  global hcfg\n",
        "\n",
        "  hcfg.output = PrettyTable()\n",
        "  hcfg.output.field_names = [\"Position\", \"Layer\", \"Head\", \"% Fails\", \"% Fails by Case\", \"# Fails by Impact\"]\n",
        "  hcfg.hook_calls = 0\n",
        "\n",
        "\n",
        "def h_set_attn_hook_z(value, hook):\n",
        "  global hcfg\n",
        "\n",
        "  hcfg.hook_calls += 1\n",
        "  # print( \"In h_set_attn_hook_z\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "\n",
        "  # Mean ablate. Copy the mean resid post values in position N to all the batch questions\n",
        "  value[:,hcfg.position,hcfg.head,:] = mean_attn_z[:,hcfg.position,hcfg.head,:].clone()\n",
        "\n",
        "\n",
        "def h_perform_core(show_all = False):\n",
        "  global hcfg\n",
        "\n",
        "  the_hook = [(l_attn_hook_z_name[hcfg.layer], h_set_attn_hook_z)]\n",
        "  loss_mean = predict_experiment_question(hcfg.questions, the_hook, hcfg.threshold)\n",
        "\n",
        "  num_fails = total_case_fails()\n",
        "  if show_all or (num_fails > 0):\n",
        "    perc_fails = round(100 * num_fails / hcfg.questions.shape[0])\n",
        "    (pattern_results, top_pattern) = get_pattern_fails()\n",
        "\n",
        "    hcfg.output.add_row([str(hcfg.position), str(hcfg.layer), str(hcfg.head), perc_fails, get_case_fails(), pattern_results])\n",
        "\n",
        "    add_head_fail_perc( hcfg.position, hcfg.layer, hcfg.head, perc_fails, top_pattern)\n",
        "\n",
        "  return num_fails\n",
        "\n",
        "\n",
        "def h_perform(all_cells):\n",
        "  global hcfg\n",
        "\n",
        "  if cfg.n_digits >= 5 :\n",
        "    h_reset()\n",
        "    if all_cells:\n",
        "      for hcfg.position in range(ccfg.min_useful_position, ccfg.max_useful_position+1):\n",
        "        for hcfg.layer in range(cfg.n_layers):\n",
        "          for hcfg.head in range(cfg.n_heads):\n",
        "            h_perform_core()\n",
        "    else:\n",
        "      for useful_cell in useful_cells:\n",
        "        if useful_cell.is_head:\n",
        "          hcfg.position = useful_cell.position\n",
        "          hcfg.layer = useful_cell.layer\n",
        "          hcfg.head = useful_cell.head\n",
        "          h_perform_core()\n",
        "\n",
        "\n",
        "def h_print_results(title, the_format=\"\"):\n",
        "  global hcfg\n",
        "\n",
        "  print_config()\n",
        "  print()\n",
        "  print(title, hcfg.questions.shape[0], \", #hook_calls=\", hcfg.hook_calls)\n",
        "\n",
        "  if the_format == \"\":\n",
        "    the_format = cfg.table_out_format\n",
        "  print(hcfg.output.get_formatted_string(out_format=the_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpkyhHRoMOSw"
      },
      "source": [
        "# Part 16: Calculate show cell matrixes\n",
        "\n",
        "Show the percentage failure rate (incorrect prediction) when individual Attention Heads and MLPs are ablated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Uluv96KMQJS"
      },
      "outputs": [],
      "source": [
        "def calc_cell_matrices(title, questions, all_cells):\n",
        "  global mcfg\n",
        "  global hcfg\n",
        "  global verbose\n",
        "\n",
        "  mcfg.questions = questions\n",
        "  m_perform(all_cells)\n",
        "  if verbose:\n",
        "    m_print_results(title)\n",
        "\n",
        "  hcfg.questions = questions\n",
        "  h_perform(all_cells)\n",
        "  if verbose:\n",
        "    h_print_results(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE64pXy1MRsw"
      },
      "outputs": [],
      "source": [
        "def print_cell_matrices():\n",
        "  global verbose\n",
        "\n",
        "  verbose = False\n",
        "\n",
        "  print_config()\n",
        "  print()\n",
        "  print_u_fail_percs()\n",
        "  print()\n",
        "  print_u_fail_notes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as9ot9RMMTAi"
      },
      "outputs": [],
      "source": [
        "def run_cell_matrices():\n",
        "  global calc_useful_cells\n",
        "\n",
        "  calc_useful_cells = True\n",
        "  calc_cell_matrices(\"Varied questions\", varied_questions, True)\n",
        "  calc_useful_cells = False\n",
        "\n",
        "  print_cell_matrices()\n",
        "\n",
        "\n",
        "run_cell_matrices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN1S8LoDMXEC"
      },
      "source": [
        "# Part 17 - Case Analysis\n",
        "Just processing BA, MC, US9 questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utqnefc2MYxl"
      },
      "outputs": [],
      "source": [
        "calc_cell_matrices(\"BA questions\", make_ba_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biUdemo5MZAz"
      },
      "outputs": [],
      "source": [
        "calc_cell_matrices(\"UC1 questions\", make_uc1_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV1zYsCeMarq"
      },
      "outputs": [],
      "source": [
        "calc_cell_matrices(\"Simple US9 questions\", make_simple_us9_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjLzcox9McGN"
      },
      "outputs": [],
      "source": [
        "calc_cell_matrices(\"Cascade US9 questions\", make_cascade_us9_questions(), False)\n",
        "\n",
        "print_cell_matrices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHSY3blNMe7I"
      },
      "source": [
        "#Part 18: SetUp: Calc and graph PCA decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCiBsiQAMhS_"
      },
      "outputs": [],
      "source": [
        "tn_questions = 100\n",
        "\n",
        "# These are n_digit addition questions where the first test_digits add up from 0 to 8\n",
        "# Randomise the last test_digits-1 digits of both numbers\n",
        "def make_t8_questions(test_digit):\n",
        "    limit = 10 ** test_digit\n",
        "    questions = []\n",
        "    for i in range(tn_questions):\n",
        "        x = random.randint(0, 8)\n",
        "        y = random.randint(0, 8-x)\n",
        "        x = x * limit + random.randint(0, limit-1)\n",
        "        y = y * limit + random.randint(0, limit-1)\n",
        "        questions.append([x, y])\n",
        "    return make_questions(questions)\n",
        "\n",
        "\n",
        "# These are n_digit addition questions where the first test_digits add up to 9\n",
        "# Randomise the last test_digits-1 digits of both numbers\n",
        "def make_t9_questions(test_digit):\n",
        "    limit = 10 ** test_digit\n",
        "    questions = []\n",
        "    for i in range(tn_questions):\n",
        "        x = random.randint(0, 9)\n",
        "        y = 9 - x\n",
        "        x = x * limit + random.randint(0, limit-1)\n",
        "        y = y * limit + random.randint(0, limit-1)\n",
        "        questions.append([x, y])\n",
        "    return make_questions(questions)\n",
        "\n",
        "\n",
        "# These are n_digit addition questions where the first test_digits add up to 10 to 18\n",
        "# Randomise the last test_digits-1 digits of both numbers\n",
        "def make_t10_questions(test_digit):\n",
        "    limit = 10 ** test_digit\n",
        "    questions = []\n",
        "    for i in range(tn_questions):\n",
        "        x = random.randint(1, 9)\n",
        "        y = random.randint(10-x, 9)\n",
        "        x = x * limit + random.randint(0, limit-1)\n",
        "        y = y * limit + random.randint(0, limit-1)\n",
        "        questions.append([x, y])\n",
        "    return make_questions(questions)\n",
        "\n",
        "\n",
        "def make_tricase_questions(test_digit):\n",
        "  q1 = make_t8_questions(test_digit)\n",
        "  q2 = make_t9_questions(test_digit)\n",
        "  q3 = make_t10_questions(test_digit)\n",
        "\n",
        "  questions = torch.vstack((q1, q2, q3))\n",
        "\n",
        "  return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWCPgoQZMjgc"
      },
      "outputs": [],
      "source": [
        "# Do one Principal Component Analysis\n",
        "def calc_tricase_pca(t_position, t_layer, t_head, t_digit):\n",
        "  global tn_questions\n",
        "\n",
        "  t_questions = make_tricase_questions(t_digit)\n",
        "  #print('Sample t8 question:', t_questions[0].tolist())\n",
        "  #print('Sample t9 question:', t_questions[tn_questions].tolist())\n",
        "  #print('Sample t10 question:', t_questions[2*tn_questions].tolist())\n",
        "\n",
        "  t_logits, t_cache = model.run_with_cache(t_questions)\n",
        "\n",
        "  # Gather attention patterns for all the (randomly chosen) questions\n",
        "  attention_outputs = []\n",
        "  for i in range(len(t_questions)):\n",
        "\n",
        "    # Output of individual heads, without final bias\n",
        "    attention_cache=t_cache[\"result\", t_layer, \"attn\"] # Output of individual heads, without final bias\n",
        "    attention_output=attention_cache[i]  # Shape [n_ctx, n_head, d_model]\n",
        "    attention_outputs.append(attention_output[t_position, t_head, :])\n",
        "\n",
        "  attn_outputs = torch.stack(attention_outputs, dim=0).cpu()\n",
        "\n",
        "  pca = PCA(n_components=6)\n",
        "  pca.fit(attn_outputs)\n",
        "  pca_attn_outputs = pca.transform(attn_outputs)\n",
        "\n",
        "  title = 'P' + str(t_position) + '.L' + str(t_layer) + '.H'+str(t_head) + ', A'+str(t_digit)\n",
        "\n",
        "  return (pca, pca_attn_outputs, title)\n",
        "\n",
        "\n",
        "# Plot one PCA scatter graph\n",
        "def graph_pca(pca, pca_attn_outputs, ax, title):\n",
        "  global tn_questions\n",
        "\n",
        "  ax.scatter(pca_attn_outputs[:tn_questions, 0], pca_attn_outputs[:tn_questions, 1], color='red', label='0-8') # t8 questions\n",
        "  ax.scatter(pca_attn_outputs[tn_questions:2*tn_questions, 0], pca_attn_outputs[tn_questions:2*tn_questions, 1], color='green', label='9') # t9 questions\n",
        "  ax.scatter(pca_attn_outputs[2*tn_questions:, 0], pca_attn_outputs[2*tn_questions:, 1], color='blue', label='10-18') # t10 questionsset\n",
        "\n",
        "  if title != \"\" :\n",
        "    ax.set_title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk2K3y7pMlQr"
      },
      "outputs": [],
      "source": [
        "# Graph the PCA of Sn.Ln.Hn's attention pattern, using T8, T9, T10 questions that differ in the An digit\n",
        "def add_one_pca_subplot(ax, t_position, t_layer, t_head, t_digit):\n",
        "  pca, pca_attn_outputs, title = calc_tricase_pca(t_position, t_layer, t_head, t_digit)\n",
        "  graph_pca( pca, pca_attn_outputs, ax, title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEF7MQqCMngv"
      },
      "outputs": [],
      "source": [
        "def save_plt_to_file( full_title ):\n",
        "  if cfg.save_graph_to_file:\n",
        "    filename = full_title.replace(\" \", \"_\").replace(\",\", \"\").replace(\":\", \"_\")  + '.png'\n",
        "    plt.savefig(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbiau9foMp3h"
      },
      "source": [
        "#Part 19: PCA decomposition tri-state results\n",
        "\n",
        "Plot attention heads in the positions 8 to 16 with a clear \"tri-state\" response to (exactly) one An."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ5fS3XNMs8e"
      },
      "outputs": [],
      "source": [
        "if cfg.n_digits == 5 and cfg.n_layers == 2 and use_pca :\n",
        "\n",
        "  # graph all useful early cells\n",
        "  fig, axs = plt.subplots(4, 2)\n",
        "  fig.set_figheight(8)\n",
        "  fig.set_figwidth(5)\n",
        "\n",
        "  # Plot all useful attention heads in the positions 8 to 12 with the clearest An selected\n",
        "  add_one_pca_subplot(axs[0, 0], 8, 0, 1, 2)    # S8.L0.H1 is clear only for A2\n",
        "  add_one_pca_subplot(axs[0, 1], 9, 0, 1, 1)    # S9.L0.H1 is clear only for A1\n",
        "  add_one_pca_subplot(axs[1, 0], 11, 0, 1, 3)   # S11.L0.H1 is clear only for A3\n",
        "  add_one_pca_subplot(axs[1, 1], 11, 0, 2, 4)   # S11.L0.H2 is clear only for A4\n",
        "  add_one_pca_subplot(axs[2, 0], 12, 0, 1, 3)   # S12.L0.H1 is clear only for A3\n",
        "  add_one_pca_subplot(axs[2, 1], 13, 0, 1, 2)   # S13.L0.H1 is clear only for A2\n",
        "  add_one_pca_subplot(axs[3, 0], 14, 0, 1, 1)   # S14.L0.H1 is clear only for A1\n",
        "\n",
        "  lines_labels = [axs[0,0].get_legend_handles_labels()]\n",
        "  lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "  fig.legend(lines, labels, loc='lower center', ncol=4)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file('PCA_Trigrams')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3611cpuEFhoW"
      },
      "source": [
        "#Part 19B: PCA decomposition bi-state results\n",
        "\n",
        "Plot attention heads in the positions 8 to 16 with a clear \"bi-state\" response to (exactly) one An."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAT8Z54oMuur"
      },
      "outputs": [],
      "source": [
        "if cfg.n_digits == 5 and cfg.n_layers == 2 and use_pca :\n",
        "\n",
        "  # graph all useful early cells\n",
        "  fig, axs = plt.subplots(1, 2)\n",
        "  fig.set_figheight(2)\n",
        "  fig.set_figwidth(5)\n",
        "\n",
        "  # Plot all useful attention heads in the positions 8 to 12 with the clearest An selected\n",
        "  add_one_pca_subplot(axs[0], 10, 0, 1, 0)   # S10.L0.H1 is clear only for A0\n",
        "  add_one_pca_subplot(axs[1], 15, 0, 1, 0)   # S15.L0.H1 is clear only for A0\n",
        "\n",
        "  lines_labels = [axs[0].get_legend_handles_labels()]\n",
        "  lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
        "  fig.legend(lines, labels, loc='lower center', ncol=4)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file('PCA_Bigrams')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQhbJTFTMwU_"
      },
      "outputs": [],
      "source": [
        "# Do one Principal Component Analysis and graph it\n",
        "def run_one_tricase_pca(t_position, t_layer, t_head, t_digit):\n",
        "\n",
        "  pca, pca_attn_outputs, title = calc_tricase_pca(t_position, t_layer, t_head, t_digit)\n",
        "\n",
        "  # Plot the PCA results\n",
        "  fig, ax = plt.subplots()\n",
        "  graph_pca(pca, pca_attn_outputs, ax, \"\")\n",
        "\n",
        "  full_title = 'PCA of attention: n_digits=' + str(cfg.n_digits) + ', ' + title\n",
        "  plt.title(full_title + ', EVR[0]=' + str(round(pca.explained_variance_ratio_[0],3)) )\n",
        "\n",
        "  plt.tight_layout()\n",
        "  save_plt_to_file(full_title)\n",
        "  plt.show()\n",
        "\n",
        "  print( \"First few principal components explain variance of:\", pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIu3Pr9CMx3l"
      },
      "source": [
        "#Part 19C: PCA decomposition of useful cells with digits 0 to 4\n",
        "\n",
        "Parts 19A and 19B are selective. This part is not. Use it to find (verify) the interesting parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdfpkXmAMzg4"
      },
      "outputs": [],
      "source": [
        "def graph_all_pca_results():\n",
        "\n",
        "  for useful_cell in useful_cells:\n",
        "      if useful_cell.is_head:\n",
        "        position = useful_cell.position\n",
        "        layer = useful_cell.layer\n",
        "        head = useful_cell.head\n",
        "        print( \"PCA: position=\", position, \"layer=\", layer, \"head=\", head)\n",
        "\n",
        "        fig, axs = plt.subplots(3, 2)\n",
        "\n",
        "        add_one_pca_subplot(axs[0, 0], position, layer, head, 0)\n",
        "        add_one_pca_subplot(axs[0, 1], position, layer, head, 1)\n",
        "        add_one_pca_subplot(axs[1, 0], position, layer, head, 2)\n",
        "        add_one_pca_subplot(axs[1, 1], position, layer, head, 3)\n",
        "        add_one_pca_subplot(axs[2, 0], position, layer, head, 4)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if use_pca :\n",
        "  graph_all_pca_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 20: Implement Mathematical framework"
      ],
      "metadata": {
        "id": "XO3kH-aQyM3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tri_case(dn1,dn2):\n",
        "  s = dn1 + dn2\n",
        "  if s >= 10:\n",
        "    return 10\n",
        "  if s == 9:\n",
        "    return 9\n",
        "  return 8\n",
        "\n",
        "\n",
        "def tri_add(dn_cx, dm_cy):\n",
        "  if dn_cx == 10 or (dn_cx == 9 and dm_cy == 10):\n",
        "    return 10\n",
        "  if dn_cx == 8 and dm_cy == 10:\n",
        "    return 9\n",
        "  return 8\n",
        "\n",
        "\n",
        "def addition_psuedo_code( d4, d3, d2, d1, d0, d4d, d3d, d2d, d1d, d0d ):\n",
        "  # D2.C1 | TriCase(D2, D2â€™) | P8.L0.H1 and P8.L0.MLP\n",
        "  d2_c1 = tri_case(d2, d2d)\n",
        "\n",
        "  # D1.C1 | TriCase(D1, D1') | P9.L0.H1 and P9.L0.MLP\n",
        "  d1_c1 = tri_case(d1, d1d)\n",
        "\n",
        "  # D1.C2 | TriAdd(D1.C1, TriCase(D0, D0â€™)) | P10.L0.H1 and P10.L0.MLP\n",
        "  d1_c2 =  tri_add( d1_c1, tri_case(d0, d0d) )\n",
        "\n",
        "  # D3.C4 | TriAdd(TriCase(D3, D3â€™), TriAdd(D2.C1,D1.C2)) | P11.L0.H1\n",
        "  d3_c4 = tri_add( tri_case(d3, d3d), tri_add(d2_c1,d1_c2) )\n",
        "\n",
        "  # D4.C1 | TriCase(D4, D4â€™) | P11.L0.H2\n",
        "  d4_c1 = tri_case(d4, d4d)\n",
        "\n",
        "  # D4.C5 | TriAdd(D4.C1, D3.C4) | P11.L0.MLP\n",
        "  d4_c5 = tri_add(d4_c1, d3_c4)\n",
        "\n",
        "  # A5 | (D4.C5 == 10) | P11.L1.MLP\n",
        "  a5 = 1 if d4_c5 == 10 else 0\n",
        "\n",
        "  # D4.BA | (D4 + D4') % 10 | P12.L0.H0 + H2\n",
        "  d4_ba = (d4 + d4d) % 10\n",
        "\n",
        "  # D3.C4 | TriAdd(TriCase(D3, D3â€™), TriAdd(D2.C1,D1.C2)) | P12.L0.H1\n",
        "  d3_c4 = tri_add( tri_case(d3, d3d), tri_add(d2_c1, d1_c2) )\n",
        "\n",
        "  # A4 | (D4.BA + D3.C4 % 10) % 10 | P12.L0.MLP and P12.L1.MLP\n",
        "  a4 = (d4_ba + (d3_c4 % 10)) % 10\n",
        "\n",
        "  #D3.BA | (D3 + D3') % 10 | P13.L0.H0 + H2\n",
        "  d3_ba = (d3 + d3d) % 10\n",
        "\n",
        "  # D2.C3 | TriAdd(D2.C1,D1.C2) | P13.L0.H1\n",
        "  d2_c3 = tri_add(d2_c1, d1_c2)\n",
        "\n",
        "  # A3 | (D3.BA + D2.C3 % 10) % 10 | P13.L0.MLP and P13.L1.MLP\n",
        "  a3 = (d3_ba + (d2_c3 % 10)) % 10\n",
        "\n",
        "  # D2.BA | (D2 + D2') % 10 | P14.L0.H0 + H2\n",
        "  d2_ba = (d2 + d2d) % 10\n",
        "\n",
        "  # D1.C2 | Copy from P10 | P14.L0.H1\n",
        "  # skip\n",
        "\n",
        "  # A2 | (D2.BA + D1.C2 % 10) % 10 | P14.L0.MLP and P14.L1.MLP\n",
        "  a2 = (d2_ba + (d1_c2 % 10)) % 10\n",
        "\n",
        "  # D1.BA | (D1 + D1') % 10 | P15.L0.H0 + H2\n",
        "  d1_ba = (d1 + d1d) % 10\n",
        "\n",
        "  # D0.MC | (D0 + D0') // 10 | P15.L0.H1\n",
        "  d0_mc = (d0 + d0d) // 10\n",
        "\n",
        "  # A1 | (D1.BA + D0.MC) % 10 | P15.L0.MLP and P15.L1.MLP\n",
        "  a1 = (d1_ba + d0_mc) % 10\n",
        "\n",
        "  # A0 | (D0 + D0') % 10 | P16.L0.H0 + H2 P16.L0.MLP and P16.L1.MLP\n",
        "  a0 = (d0 + d0d) % 10\n",
        "\n",
        "  return a5, a4, a3, a2, a1, a0\n",
        "\n",
        "\n",
        "def do_addition_question(question):\n",
        "  if cfg.n_digits == 5:\n",
        "    d4 = int(question[0])\n",
        "    d3 = int(question[1])\n",
        "    d2 = int(question[2])\n",
        "    d1 = int(question[3])\n",
        "    d0 = int(question[4])\n",
        "    d4d = int(question[6])\n",
        "    d3d = int(question[7])\n",
        "    d2d = int(question[8])\n",
        "    d1d = int(question[9])\n",
        "    d0d = int(question[10])\n",
        "\n",
        "    a5, a4, a3, a2, a1, a0 = addition_psuedo_code( d4, d3, d2, d1, d0, d4d, d3d, d2d, d1d, d0d)\n",
        "\n",
        "    d = d4 * 10000 + d3 * 1000 + d2 * 100 + d1 * 10 + d0\n",
        "    dd = d4d * 10000 + d3d * 1000 + d2d * 100 + d1d * 10 + d0d\n",
        "    a = a5 * 100000 + a4 * 10000 + a3 * 1000 + a2 * 100 + a1 * 10 + a0\n",
        "\n",
        "    if d + dd != a :\n",
        "      print(d4, d3, d2, d1, d0, d4d, d3d, d2d, d1d, d0d)\n",
        "      print(\"Bad addition:\", d, \"+\", dd, \"=\", a, \"Should be\", d+dd, \"Delta\", d+dd-a)\n",
        "      return 1\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "OauytHdPyRyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if cfg.n_digits == 5:\n",
        "  num_questions = varied_questions.shape[0]\n",
        "  print(\"#Questions=\", num_questions)\n",
        "\n",
        "  num_fails = 0\n",
        "  for i in range(3): #num_questions):\n",
        "    question = varied_questions[i]\n",
        "    num_fails += do_addition_question(question)\n",
        "\n",
        "  print(\"#Fails=\", num_fails)"
      ],
      "metadata": {
        "id": "nt7B0hdy-OZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 21: MLP Visualisation"
      ],
      "metadata": {
        "id": "lTs6Cu55jB7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import einops\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "#from IPython.display import clear_output\n",
        "#import circuitsvis as cv\n",
        "#import h5py\n",
        "#import palettable\n",
        "\n",
        "cache = sample_cache\n",
        "\n",
        "attn_mat = cache['pattern', 0, 'attn'][:,:,-3,:]\n",
        "neuron_acts_pre = cache['blocks.0.mlp.hook_pre'][:,-3]\n",
        "neuron_acts_post = cache['blocks.0.mlp.hook_post'][:,-3]\n",
        "print(neuron_acts_post.shape)\n",
        "neuron_acts_post_sq = einops.rearrange(neuron_acts_post, \"(x y) d_mlp -> x y d_mlp\", x=239) # num questions\n",
        "neuron_acts_pre_sq = einops.rearrange(neuron_acts_pre, \"(x y) d_mlp -> x y d_mlp\", x=239) # num questions\n",
        "#posmask = data.tok[:,-2] == 100\n",
        "#posmask = data.tok[:,-2] == 100\n",
        "#negmask = data.tok[:,-2] == 101\n",
        "\n",
        "#posmask_sq = einops.rearrange(posmask.numpy(), \"(x y) -> x y \", x=239)[:,:,None,None]\n",
        "#negmask_sq = einops.rearrange(negmask.numpy(), \"(x y) -> x y \", x=239)[:,:,None,None]\n",
        "#print(posmask_sq.shape, posmask.sum(), negmask.sum())\n",
        "\n",
        "def plot_neuronact(pos: int):\n",
        "    #pq clear_output()\n",
        "    #pq data = neuron_acts_pre_sq.detach().numpy()[:,:,pos]\n",
        "    data = neuron_acts_pre_sq.cpu().numpy()[:,:,pos]\n",
        "    ft2d = np.fft.fft2(data)\n",
        "    power = (ft2d*np.conj(ft2d)).real\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
        "    plot = axs[0].imshow(data, cmap='magma', vmin=0)#, vmax=1)\n",
        "    cbar = plt.colorbar(plot, fraction=0.1)\n",
        "    cbar.set_label(r'neuron activation strength {}'.format(pos))\n",
        "    axs[0].set_ylim(-0.5, 99.5)\n",
        "    axs[0].set_yticks(range(100), labels=range(100), size=5.5);\n",
        "    axs[0].set_xticks(range(100), labels=range(100), size=5.5, rotation='vertical');\n",
        "\n",
        "    plot = axs[1].imshow(np.log10(power), cmap='magma', vmin=0)#, vmax=1)\n",
        "    cbar = plt.colorbar(plot, fraction=0.1)\n",
        "    cbar.set_label(r'neuron log10 power {}'.format(pos))\n",
        "    #axs[1].set_ylim(-0.5, 99.5)\n",
        "    #axs[1].set_yticks(range(100));\n",
        "    #axs[1].set_yticklabels([f'{freq:.2f}' for freq in freqs], fontsize=5.5);\n",
        "    #axs[1].set_xticks(range(100));\n",
        "    #axs[1].set_xticklabels([f'{freq:.2f}' for freq in freqs], fontsize=5.5, rotation='vertical');\n",
        "\n",
        "interact(plot_neuronact, pos=widgets.IntText(value=0, description='Index:'))"
      ],
      "metadata": {
        "id": "YviVHnBRjDcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8Hqjz9M5b_"
      },
      "source": [
        "# Part 22 : What does the model do at S11 (the \"=\" token)? NOT USED\n",
        "\n",
        "NOT USED\n",
        "\n",
        "At S11, the model predicts A5 (P12), using attention heads paying attention to tokens at P0.. P4, P6 .. P10. TBC  \n",
        "\n",
        "Use \"a_\" prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVte-OEMM7Xp"
      },
      "outputs": [],
      "source": [
        "class A_Config():\n",
        "  prediction_step : int = 11 # The prediction step we want to focus on. P11 is the \"=\" token\n",
        "\n",
        "  token_position : int = 8 # The token position we want to get/set. P8 to P11 contribute to A5 calculations\n",
        "  layer : int = 0 # The layer we want to get/set\n",
        "  head : int = 1 # The head we want to get/set\n",
        "  answer_digit : int = '5'  # The answer digit we are testing\n",
        "  threshold : int = 0.01\n",
        "\n",
        "  hook_calls: int = 0\n",
        "  answer_failures : int = 0    # Failures of any digit\n",
        "  digit_failures : int = 0     # Failures of the answer_digit\n",
        "\n",
        "  questions = []\n",
        "  store = []\n",
        "\n",
        "\n",
        "acfg = A_Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmaKxx_2M9na"
      },
      "outputs": [],
      "source": [
        "# Get and put attention head values\n",
        "\n",
        "def a_null_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  acfg.hook_calls += 1\n",
        "  #print(\"In a_null_attn_z_hook\", value.shape)  # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "\n",
        "\n",
        "def a_get_l0_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 0:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In a_get_l0_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "    acfg.store = value.clone()\n",
        "\n",
        "\n",
        "def a_get_l1_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 1:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In acfg.get_l1_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, cfg.d_head\n",
        "    acfg.store = value.clone()\n",
        "\n",
        "\n",
        "def a_put_l0_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 0:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In a_l0_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "    value[:,acfg.token_position,acfg.head,:] = acfg.store[:,acfg.token_position,acfg.head,:].clone()\n",
        "\n",
        "\n",
        "\n",
        "def a_put_l1_attn_z_hook(value, hook):\n",
        "  global acfg\n",
        "\n",
        "  if acfg.layer == 1:\n",
        "    acfg.hook_calls += 1\n",
        "    # print( \"In a_put_l1_attn_z_hook\", value.shape) # Get [1, 18, 3, 170] = ???, cfg.n_ctx, cfg.n_heads, d_head\n",
        "    value[:,acfg.token_position,acfg.head,:] = acfg.store[:,acfg.token_position,acfg.head,:].clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZomVSmaM_RJ"
      },
      "outputs": [],
      "source": [
        "def a_predict_questions_with_hooks(the_hooks):\n",
        "  global acfg\n",
        "  global model\n",
        "\n",
        "  acfg.hook_calls = 0\n",
        "  acfg.answer_failures = 0\n",
        "  acfg.digit_failures = 0\n",
        "  sum_loss = 0.0\n",
        "\n",
        "  for question_num in range(acfg.questions.shape[0]):\n",
        "    q = acfg.questions[question_num]\n",
        "\n",
        "    model.reset_hooks()\n",
        "    model.set_use_attn_result(True)\n",
        "    exp_logits = model.run_with_hooks(q.cuda(), return_type=\"logits\", fwd_hooks=the_hooks)\n",
        "\n",
        "    q_2d = q.unsqueeze(0)\n",
        "    exp_losses_raw, exp_max_indices = logits_to_tokens_loss(exp_logits, q_2d.cuda())\n",
        "    loss_mean = utils.to_numpy(loss_fn(exp_losses_raw).mean())\n",
        "    sum_loss += loss_mean\n",
        "\n",
        "    # Only show the question if the loss exceeds the threshold (because of the ablated step)\n",
        "    if loss_mean > acfg.threshold:\n",
        "      acfg.answer_failures += 1\n",
        "\n",
        "      exp_answer_str = prediction_to_string(exp_max_indices)\n",
        "\n",
        "      i = 12\n",
        "      a = q[i+0] * 100000 + q[i+1] * 10000 + q[i+2] * 1000 + q[i+3] * 100 + q[i+4] * 10 + q[i+5] * 1;\n",
        "\n",
        "      match_str = get_digit_accuracy_impact( a, exp_answer_str )\n",
        "      acfg.digit_failures += 1 if acfg.answer_digit in match_str else 0\n",
        "      if verbose:\n",
        "        print(\"    \"+match_str)\n",
        "\n",
        "\n",
        "  return sum_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LcRMtKQNBVJ"
      },
      "outputs": [],
      "source": [
        "def explain_A5():\n",
        "  global acfg\n",
        "\n",
        "  if cfg.n_digits == 5 and cfg.n_layers == 2:\n",
        "\n",
        "    acfg.questions = varied_questions[:7]\n",
        "    acfg.answer_digit = '5'\n",
        "    print(\"Explain_A5: #questions=\", acfg.questions.shape[0], \"answer_digit=\", acfg.answer_digit)\n",
        "\n",
        "    # With no hook, loss should be zero\n",
        "    # How many times is the hook called? cfg.n_ctx? If so this sequence gives a_prediction_step\n",
        "    the_null_attn_z_hook = [(l_attn_hook_z_name[0], a_null_attn_z_hook)]\n",
        "    sum_loss = a_predict_questions_with_hooks(the_null_attn_z_hook)\n",
        "    print(\"Run with null hook: #Calls=\", acfg.hook_calls, \"#AnswerFails=\", acfg.answer_failures, \"#DigitFails=\", acfg.digit_failures, \"sum_loss=\", sum_loss)\n",
        "\n",
        "\n",
        "    the_get_attn_z_hook = [(l_attn_hook_z_name[0], a_get_l0_attn_z_hook),(l_attn_hook_z_name[1], a_get_l1_attn_z_hook)]\n",
        "    sum_loss = a_predict_questions_with_hooks(the_get_attn_z_hook)\n",
        "    print(\"Run with get hook: #Calls=\", acfg.hook_calls, \"#AnswerFails=\", acfg.answer_failures, \"#DigitFails=\", acfg.digit_failures, \"store.shape=\", acfg.store.shape, \"sum_loss=\", sum_loss)\n",
        "\n",
        "\n",
        "    acfg.prediction_step = 11 # Not used\n",
        "    acfg.token_position = 10\n",
        "    acfg.layer = 0\n",
        "    acfg.head = 1\n",
        "    acfg.store = mean_attn_z.clone() # Mean ablate\n",
        "\n",
        "    the_put_attn_z_hook = [(l_attn_hook_z_name[0], a_put_l0_attn_z_hook),(l_attn_hook_z_name[1], a_put_l1_attn_z_hook)]\n",
        "    #sum_loss = a_predict_questions_with_hooks(the_put_attn_z_hook)\n",
        "    #print(\"Run with put hook: #Calls=\", acfg.hook_calls, \"#AnswerFails=\", acfg.answer_failures, \"#DigitFails=\", acfg.digit_failures, \"sum_loss=\", sum_loss)\n",
        "\n",
        "\n",
        "    # Repeatedly predict these questions, mean ablating one token_position+head+layer at a time, to see the impact on A5\n",
        "    print()\n",
        "    for acfg.token_position in range(8,12):\n",
        "      for acfg.layer in range(cfg.n_layers):\n",
        "          for acfg.head in range(cfg.n_heads):\n",
        "            sum_loss = a_predict_questions_with_hooks(the_put_attn_z_hook)\n",
        "            if acfg.answer_failures > 0:\n",
        "              temp = \"#Digit\" + str(acfg.answer_digit) + \"Fails=\"\n",
        "              print(\"Run with put hook: token_position=\", acfg.token_position, \"layer=\", acfg.layer, \"head=\", acfg.head, \"#AnswerFails=\", acfg.answer_failures, temp, acfg.digit_failures, \"sum_loss=\", sum_loss)\n",
        "            #else:\n",
        "            #  print(\"Good: token_position=\", acfg.token_position, \"layer=\", acfg.layer, \"head=\", acfg.head, \"sum_loss=\", sum_loss)\n",
        "\n",
        "\n",
        "#explain_A5()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ldGPkaokJQM5",
        "P8RfHXneJw6n",
        "Lag8C3d_KkeQ",
        "FyK7QeUjLLFm",
        "ZmsGWUbILYin",
        "904WBkTOLg_5",
        "99LIC1l4MD32",
        "pKZF_B6OMIq3",
        "mN1S8LoDMXEC",
        "fHSY3blNMe7I",
        "Rbiau9foMp3h",
        "3611cpuEFhoW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}